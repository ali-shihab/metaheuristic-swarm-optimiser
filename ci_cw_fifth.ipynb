{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVv-rpppIdfY"
      },
      "source": [
        "# Computational Intelligence Coursework - Ali"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2BYuECfNGoI"
      },
      "source": [
        "## Imports, Functions, Model, Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoQTbsACfg_f"
      },
      "source": [
        "### Imports & Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wro-rS5oHtpV",
        "outputId": "d2782442-d297-42df-9290-529bfa5bfe61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# general imports\n",
        "import numpy as np\n",
        "\n",
        "# torch & data manipulation imports\n",
        "import torch\n",
        "from torch.utils.data import ConcatDataset, Subset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# model-related imports\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "\n",
        "# seed for reproducibility\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.backends.cuda.is_availale() else \"cpu\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3nmEEA8MgoD"
      },
      "source": [
        "### Define Custom Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Y_VhUWit2xa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# DATA PREPROCESSING\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, indices, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[self.indices[idx]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# calculate mean & standard deviation based on dataset\n",
        "def calc_mean_std(dataset):\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "    mean_sum = 0.\n",
        "    var_sum = 0.\n",
        "    total_images_count = 0\n",
        "    for images, _ in dataloader:\n",
        "        batch_samples = images.size(0)\n",
        "        images = images.view(batch_samples, images.size(1), -1)\n",
        "        mean_sum += images.mean(2).sum(0)\n",
        "        var_sum += images.var(2).sum(0)\n",
        "        total_images_count += batch_samples\n",
        "\n",
        "    mean = mean_sum / total_images_count\n",
        "    var = var_sum / total_images_count\n",
        "    std = np.sqrt(var)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "# define transformations for data augmentation\n",
        "def train_transform(data, mean, std):\n",
        "  data = normalize(data, mean, std)\n",
        "  transform = transforms.Compose([\n",
        "                                transforms.RandomHorizontalFlip(0.25),\n",
        "                                transforms.RandomVerticalFlip(0.25),\n",
        "                                transforms.RandomGrayscale(0.25),\n",
        "                                transforms.RandomCrop(32, padding=4)\n",
        "                                 ])\n",
        "  return transform(data)\n",
        "\n",
        "# define normalisation\n",
        "def normalize(data, mean, std):\n",
        "  transform = transforms.Compose([\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ])\n",
        "  return transform(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jGcc7gkI4Mt"
      },
      "source": [
        "### Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jWDU4lgbZuGM"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dropout_prob):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 32, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 32 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def freeze_all_but_last():\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'fc2' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Extract weights from the last layer\n",
        "    def extract_weights():\n",
        "        return [p.data.numpy() for p in self.fc2.parameters()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSuZ5c9VNjU8"
      },
      "source": [
        "### Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OyPC4tWRp-XR"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ9wVgxINsGO"
      },
      "source": [
        "### Data Loading & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L44Im7lwPnw_",
        "outputId": "a0092e19-91ee-4e51-c8d2-25a8697809fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# load CIFAR-10 dataset & convert to tensor\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True,\n",
        "                                         transform=transforms.ToTensor())\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                            download=True,\n",
        "                                        transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "RghE-KgmZ91b"
      },
      "outputs": [],
      "source": [
        "# dataset hyperparameters\n",
        "num_folds = 5\n",
        "test_size = 0.20\n",
        "\n",
        "# combine train and test datasets for stratified splitting\n",
        "combined_set = ConcatDataset([train_set, test_set])\n",
        "\n",
        "# STRATIFIED SPLIT\n",
        "# collect the labels\n",
        "labels = [y for _, y in combined_set]\n",
        "\n",
        "# stratified split subset indices\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "train_idx, test_idx = next(sss.split(np.zeros(len(labels)), labels))\n",
        "\n",
        "# subset using indices & collect associated labels\n",
        "stratified_train_set, stratified_train_labels = Subset(combined_set, train_idx).dataset, [labels[i] for i in range(len(labels)) if i in train_idx]\n",
        "stratified_test_set, stratified_test_labels = Subset(combined_set, test_idx).dataset, [labels[i] for i in range(len(labels)) if i in test_idx]\n",
        "\n",
        "# create StratifiedKFold object for train set only\n",
        "skf = StratifiedKFold(n_splits=num_folds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x-vTtxYIqXd"
      },
      "source": [
        "## Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnNyTV4bN_C0"
      },
      "source": [
        "### Gradient Based - Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "82P35TRdVpN-"
      },
      "outputs": [],
      "source": [
        "# function for training and evaluating the model\n",
        "def adam_train_and_validate(model, train_loader, test_loader, criterion, optimizer, mean, std, epochs=30):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # early stopping parameters\n",
        "    early_stopping_patience = 3  # number of epochs to wait for improvement before stopping\n",
        "    early_stopping_counter = 0    # counter for epochs without improvement\n",
        "    best_accuracy = 0             # track the best accuracy\n",
        "\n",
        "    # train\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        i = 0\n",
        "\n",
        "        for inputs, train_load_labels in train_loader:\n",
        "            inputs = train_transform(inputs, mean, std)\n",
        "            inputs, train_load_labels = inputs.to(device), train_load_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, train_load_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += train_load_labels.size(0)\n",
        "            correct += (predicted == train_load_labels).sum().item()\n",
        "            i += 1\n",
        "            if i % 20 == 0:\n",
        "              batch_accuracy = 100 * correct / total\n",
        "              print(f'{i}th Batch Loss: {loss.item():.4f} Batch Accuracy: {batch_accuracy:.4f}')\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}] Loss: {loss.item():.4f} Epoch Accuracy: {epoch_accuracy:.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # validate\n",
        "        with torch.no_grad():\n",
        "            for inputs, test_load_labels in test_loader:\n",
        "                inputs = normalize(inputs, mean, std)\n",
        "                inputs, test_load_labels = inputs.to(device), test_load_labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += test_load_labels.size(0)\n",
        "                correct += (predicted == test_load_labels).sum().item()\n",
        "\n",
        "        validation_accuracy = 100 * correct / total\n",
        "\n",
        "        # check if the current validation accuracy is better than the best recorded accuracy\n",
        "        if validation_accuracy > best_accuracy:\n",
        "            best_accuracy = validation_accuracy\n",
        "            early_stopping_counter = 0  # Reset the counter\n",
        "            # save the model checkpoint\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, filename=f\"best_model_epoch_{epoch+1}.pth.tar\")\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        print(early_stopping_counter)\n",
        "        # check if early stopping should be triggered\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "        print(f'Validation Accuracy: {validation_accuracy:.2f}%')\n",
        "    return model\n",
        "\n",
        "# function for testing the model\n",
        "def test(model, test_loader, mean, std):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, test_load_labels in test_loader:\n",
        "            inputs = normalize(inputs, mean, std)\n",
        "            inputs, test_load_labels = inputs.to(device), test_load_labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += test_load_labels.size(0)\n",
        "            correct += (predicted == test_load_labels).sum().item()\n",
        "\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8pu_QfAabSj",
        "outputId": "6de086bf-2187-4bce-dc2d-628c183c4669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 1.9541 Batch Accuracy: 17.5781\n",
            "40th Batch Loss: 2.0445 Batch Accuracy: 22.0312\n",
            "60th Batch Loss: 1.7624 Batch Accuracy: 25.0781\n",
            "80th Batch Loss: 1.6816 Batch Accuracy: 26.7578\n",
            "100th Batch Loss: 1.7538 Batch Accuracy: 28.0469\n",
            "120th Batch Loss: 1.3792 Batch Accuracy: 29.8438\n",
            "140th Batch Loss: 1.7243 Batch Accuracy: 30.8482\n",
            "160th Batch Loss: 1.6397 Batch Accuracy: 31.8262\n",
            "180th Batch Loss: 1.7269 Batch Accuracy: 32.3177\n",
            "200th Batch Loss: 1.4811 Batch Accuracy: 33.1406\n",
            "220th Batch Loss: 1.6009 Batch Accuracy: 33.8778\n",
            "240th Batch Loss: 1.5021 Batch Accuracy: 34.6810\n",
            "260th Batch Loss: 1.6279 Batch Accuracy: 35.0361\n",
            "280th Batch Loss: 1.6219 Batch Accuracy: 35.4018\n",
            "300th Batch Loss: 1.5158 Batch Accuracy: 36.1250\n",
            "320th Batch Loss: 1.9280 Batch Accuracy: 36.2207\n",
            "340th Batch Loss: 1.6841 Batch Accuracy: 36.6360\n",
            "360th Batch Loss: 1.5452 Batch Accuracy: 37.1094\n",
            "380th Batch Loss: 1.3005 Batch Accuracy: 37.5493\n",
            "400th Batch Loss: 1.3828 Batch Accuracy: 37.9961\n",
            "420th Batch Loss: 1.4601 Batch Accuracy: 38.3036\n",
            "440th Batch Loss: 1.4721 Batch Accuracy: 38.7891\n",
            "460th Batch Loss: 1.4830 Batch Accuracy: 39.2357\n",
            "480th Batch Loss: 1.1254 Batch Accuracy: 39.6842\n",
            "500th Batch Loss: 1.3119 Batch Accuracy: 40.0656\n",
            "520th Batch Loss: 1.4158 Batch Accuracy: 40.3996\n",
            "540th Batch Loss: 1.4274 Batch Accuracy: 40.8015\n",
            "560th Batch Loss: 1.4575 Batch Accuracy: 41.0631\n",
            "580th Batch Loss: 1.2431 Batch Accuracy: 41.3120\n",
            "600th Batch Loss: 1.1145 Batch Accuracy: 41.5990\n",
            "620th Batch Loss: 1.2640 Batch Accuracy: 41.9934\n",
            "640th Batch Loss: 1.5449 Batch Accuracy: 42.3755\n",
            "660th Batch Loss: 1.5691 Batch Accuracy: 42.6657\n",
            "Epoch [1/10] Loss: 1.3378 Epoch Accuracy: 42.8472\n",
            "0\n",
            "Validation Accuracy: 55.31%\n",
            "20th Batch Loss: 1.0363 Batch Accuracy: 45.3906\n",
            "40th Batch Loss: 1.5045 Batch Accuracy: 45.5078\n",
            "60th Batch Loss: 1.5605 Batch Accuracy: 45.8594\n",
            "80th Batch Loss: 1.2483 Batch Accuracy: 47.1484\n",
            "100th Batch Loss: 1.2065 Batch Accuracy: 48.0156\n",
            "120th Batch Loss: 1.1124 Batch Accuracy: 48.7891\n",
            "140th Batch Loss: 1.7438 Batch Accuracy: 49.2188\n",
            "160th Batch Loss: 1.4334 Batch Accuracy: 49.2676\n",
            "180th Batch Loss: 1.3423 Batch Accuracy: 49.9479\n",
            "200th Batch Loss: 1.2366 Batch Accuracy: 50.2344\n",
            "220th Batch Loss: 1.5881 Batch Accuracy: 50.2770\n",
            "240th Batch Loss: 1.3216 Batch Accuracy: 50.7357\n",
            "260th Batch Loss: 1.5029 Batch Accuracy: 51.0216\n",
            "280th Batch Loss: 1.3730 Batch Accuracy: 51.4062\n",
            "300th Batch Loss: 1.1625 Batch Accuracy: 51.6094\n",
            "320th Batch Loss: 1.2907 Batch Accuracy: 51.8750\n",
            "340th Batch Loss: 1.3867 Batch Accuracy: 52.3024\n",
            "360th Batch Loss: 1.2284 Batch Accuracy: 52.4219\n",
            "380th Batch Loss: 1.4001 Batch Accuracy: 52.5781\n",
            "400th Batch Loss: 1.1854 Batch Accuracy: 52.9180\n",
            "420th Batch Loss: 1.1681 Batch Accuracy: 53.1734\n",
            "440th Batch Loss: 1.2578 Batch Accuracy: 53.3558\n",
            "460th Batch Loss: 1.1769 Batch Accuracy: 53.4511\n",
            "480th Batch Loss: 1.1369 Batch Accuracy: 53.7337\n",
            "500th Batch Loss: 1.1551 Batch Accuracy: 53.8781\n",
            "520th Batch Loss: 1.2077 Batch Accuracy: 54.1106\n",
            "540th Batch Loss: 1.0178 Batch Accuracy: 54.4039\n",
            "560th Batch Loss: 0.9913 Batch Accuracy: 54.6680\n",
            "580th Batch Loss: 1.1358 Batch Accuracy: 54.7225\n",
            "600th Batch Loss: 0.9684 Batch Accuracy: 54.8516\n",
            "620th Batch Loss: 1.1183 Batch Accuracy: 54.9672\n",
            "640th Batch Loss: 1.2307 Batch Accuracy: 55.0488\n",
            "660th Batch Loss: 0.8851 Batch Accuracy: 55.1894\n",
            "Epoch [2/10] Loss: 1.1978 Epoch Accuracy: 55.3102\n",
            "0\n",
            "Validation Accuracy: 68.33%\n",
            "20th Batch Loss: 0.9245 Batch Accuracy: 61.7188\n",
            "40th Batch Loss: 1.3864 Batch Accuracy: 61.8750\n",
            "60th Batch Loss: 1.0315 Batch Accuracy: 59.9740\n",
            "80th Batch Loss: 1.0861 Batch Accuracy: 59.7461\n",
            "100th Batch Loss: 1.0709 Batch Accuracy: 60.2812\n",
            "120th Batch Loss: 1.2005 Batch Accuracy: 60.1823\n",
            "140th Batch Loss: 0.9505 Batch Accuracy: 60.0000\n",
            "160th Batch Loss: 1.0248 Batch Accuracy: 60.1660\n",
            "180th Batch Loss: 1.1012 Batch Accuracy: 60.3906\n",
            "200th Batch Loss: 1.0523 Batch Accuracy: 60.2734\n",
            "220th Batch Loss: 0.8878 Batch Accuracy: 60.2131\n",
            "240th Batch Loss: 1.2487 Batch Accuracy: 60.5208\n",
            "260th Batch Loss: 0.8492 Batch Accuracy: 60.6971\n",
            "280th Batch Loss: 1.2410 Batch Accuracy: 60.9431\n",
            "300th Batch Loss: 0.8739 Batch Accuracy: 61.1042\n",
            "320th Batch Loss: 0.7721 Batch Accuracy: 61.0791\n",
            "340th Batch Loss: 0.9530 Batch Accuracy: 61.1029\n",
            "360th Batch Loss: 1.0168 Batch Accuracy: 61.2500\n",
            "380th Batch Loss: 1.1394 Batch Accuracy: 61.2459\n",
            "400th Batch Loss: 0.9805 Batch Accuracy: 61.0742\n",
            "420th Batch Loss: 0.8432 Batch Accuracy: 61.3244\n",
            "440th Batch Loss: 1.2966 Batch Accuracy: 61.2820\n",
            "460th Batch Loss: 1.0027 Batch Accuracy: 61.3043\n",
            "480th Batch Loss: 1.1440 Batch Accuracy: 61.4160\n",
            "500th Batch Loss: 0.8968 Batch Accuracy: 61.4625\n",
            "520th Batch Loss: 1.1565 Batch Accuracy: 61.4453\n",
            "540th Batch Loss: 0.6065 Batch Accuracy: 61.5451\n",
            "560th Batch Loss: 0.8064 Batch Accuracy: 61.6071\n",
            "580th Batch Loss: 0.7978 Batch Accuracy: 61.6514\n",
            "600th Batch Loss: 0.8010 Batch Accuracy: 61.7057\n",
            "620th Batch Loss: 0.7448 Batch Accuracy: 61.7414\n",
            "640th Batch Loss: 0.9537 Batch Accuracy: 61.7798\n",
            "660th Batch Loss: 1.3556 Batch Accuracy: 61.7614\n",
            "Epoch [3/10] Loss: 1.3404 Epoch Accuracy: 61.7847\n",
            "0\n",
            "Validation Accuracy: 69.17%\n",
            "20th Batch Loss: 1.2444 Batch Accuracy: 65.1562\n",
            "40th Batch Loss: 1.0052 Batch Accuracy: 64.6875\n",
            "60th Batch Loss: 1.0845 Batch Accuracy: 64.9479\n",
            "80th Batch Loss: 0.9896 Batch Accuracy: 64.3164\n",
            "100th Batch Loss: 0.8601 Batch Accuracy: 64.8438\n",
            "120th Batch Loss: 0.8295 Batch Accuracy: 64.7656\n",
            "140th Batch Loss: 0.9865 Batch Accuracy: 64.8661\n",
            "160th Batch Loss: 0.7512 Batch Accuracy: 64.8730\n",
            "180th Batch Loss: 1.3608 Batch Accuracy: 64.8958\n",
            "200th Batch Loss: 0.6841 Batch Accuracy: 64.9609\n",
            "220th Batch Loss: 0.7447 Batch Accuracy: 65.1776\n",
            "240th Batch Loss: 1.1192 Batch Accuracy: 65.0195\n",
            "260th Batch Loss: 0.9255 Batch Accuracy: 65.0721\n",
            "280th Batch Loss: 0.9457 Batch Accuracy: 65.1842\n",
            "300th Batch Loss: 0.9957 Batch Accuracy: 65.1302\n",
            "320th Batch Loss: 1.3301 Batch Accuracy: 65.0586\n",
            "340th Batch Loss: 0.9993 Batch Accuracy: 65.0827\n",
            "360th Batch Loss: 0.7440 Batch Accuracy: 65.0477\n",
            "380th Batch Loss: 0.8716 Batch Accuracy: 64.9959\n",
            "400th Batch Loss: 0.8078 Batch Accuracy: 65.0820\n",
            "420th Batch Loss: 1.0084 Batch Accuracy: 64.9814\n",
            "440th Batch Loss: 1.2550 Batch Accuracy: 64.8544\n",
            "460th Batch Loss: 1.2671 Batch Accuracy: 64.8505\n",
            "480th Batch Loss: 1.3128 Batch Accuracy: 64.7982\n",
            "500th Batch Loss: 1.0583 Batch Accuracy: 64.8750\n",
            "520th Batch Loss: 1.3302 Batch Accuracy: 64.8558\n",
            "540th Batch Loss: 0.6848 Batch Accuracy: 64.9103\n",
            "560th Batch Loss: 1.3236 Batch Accuracy: 65.0363\n",
            "580th Batch Loss: 1.1578 Batch Accuracy: 65.1643\n",
            "600th Batch Loss: 0.8015 Batch Accuracy: 65.2943\n",
            "620th Batch Loss: 0.8326 Batch Accuracy: 65.2747\n",
            "640th Batch Loss: 0.7977 Batch Accuracy: 65.3174\n",
            "660th Batch Loss: 0.7081 Batch Accuracy: 65.3220\n",
            "Epoch [4/10] Loss: 0.7955 Epoch Accuracy: 65.3264\n",
            "0\n",
            "Validation Accuracy: 72.67%\n",
            "20th Batch Loss: 1.3803 Batch Accuracy: 64.7656\n",
            "40th Batch Loss: 1.0425 Batch Accuracy: 65.3906\n",
            "60th Batch Loss: 0.9809 Batch Accuracy: 66.6667\n",
            "80th Batch Loss: 1.1191 Batch Accuracy: 66.3086\n",
            "100th Batch Loss: 0.9006 Batch Accuracy: 65.6094\n",
            "120th Batch Loss: 1.4410 Batch Accuracy: 65.6510\n",
            "140th Batch Loss: 0.8615 Batch Accuracy: 65.7701\n",
            "160th Batch Loss: 1.0130 Batch Accuracy: 65.5469\n",
            "180th Batch Loss: 0.9242 Batch Accuracy: 65.6771\n",
            "200th Batch Loss: 0.8729 Batch Accuracy: 65.6953\n",
            "220th Batch Loss: 0.7878 Batch Accuracy: 66.1790\n",
            "240th Batch Loss: 1.0124 Batch Accuracy: 66.4062\n",
            "260th Batch Loss: 0.6944 Batch Accuracy: 66.5505\n",
            "280th Batch Loss: 0.8593 Batch Accuracy: 66.5458\n",
            "300th Batch Loss: 0.6690 Batch Accuracy: 66.5990\n",
            "320th Batch Loss: 0.9614 Batch Accuracy: 66.6064\n",
            "340th Batch Loss: 0.9761 Batch Accuracy: 66.6131\n",
            "360th Batch Loss: 0.9087 Batch Accuracy: 66.7448\n",
            "380th Batch Loss: 0.5561 Batch Accuracy: 66.8627\n",
            "400th Batch Loss: 1.4592 Batch Accuracy: 66.8867\n",
            "420th Batch Loss: 1.5508 Batch Accuracy: 66.8973\n",
            "440th Batch Loss: 1.1989 Batch Accuracy: 66.8857\n",
            "460th Batch Loss: 1.4543 Batch Accuracy: 66.9056\n",
            "480th Batch Loss: 0.6674 Batch Accuracy: 66.9466\n",
            "500th Batch Loss: 1.2258 Batch Accuracy: 67.0312\n",
            "520th Batch Loss: 0.9061 Batch Accuracy: 67.0733\n",
            "540th Batch Loss: 0.8385 Batch Accuracy: 66.9936\n",
            "560th Batch Loss: 1.0946 Batch Accuracy: 66.9810\n",
            "580th Batch Loss: 0.7167 Batch Accuracy: 67.0609\n",
            "600th Batch Loss: 0.8476 Batch Accuracy: 67.2057\n",
            "620th Batch Loss: 0.6890 Batch Accuracy: 67.1825\n",
            "640th Batch Loss: 0.6423 Batch Accuracy: 67.2241\n",
            "660th Batch Loss: 1.0407 Batch Accuracy: 67.2633\n",
            "Epoch [5/10] Loss: 0.9681 Epoch Accuracy: 67.2454\n",
            "0\n",
            "Validation Accuracy: 74.94%\n",
            "20th Batch Loss: 0.8273 Batch Accuracy: 70.3906\n",
            "40th Batch Loss: 0.6818 Batch Accuracy: 69.8438\n",
            "60th Batch Loss: 0.9008 Batch Accuracy: 69.3229\n",
            "80th Batch Loss: 1.0880 Batch Accuracy: 69.0234\n",
            "100th Batch Loss: 1.0648 Batch Accuracy: 68.6094\n",
            "120th Batch Loss: 1.0198 Batch Accuracy: 68.9062\n",
            "140th Batch Loss: 0.8238 Batch Accuracy: 68.7946\n",
            "160th Batch Loss: 0.4774 Batch Accuracy: 69.0723\n",
            "180th Batch Loss: 0.8167 Batch Accuracy: 69.3663\n",
            "200th Batch Loss: 0.8550 Batch Accuracy: 69.1875\n",
            "220th Batch Loss: 1.1220 Batch Accuracy: 69.0341\n",
            "240th Batch Loss: 0.6765 Batch Accuracy: 69.1797\n",
            "260th Batch Loss: 0.5700 Batch Accuracy: 69.1046\n",
            "280th Batch Loss: 0.7571 Batch Accuracy: 69.0737\n",
            "300th Batch Loss: 0.8917 Batch Accuracy: 69.0104\n",
            "320th Batch Loss: 0.7277 Batch Accuracy: 68.9990\n",
            "340th Batch Loss: 0.8023 Batch Accuracy: 68.8327\n",
            "360th Batch Loss: 0.7870 Batch Accuracy: 68.9410\n",
            "380th Batch Loss: 0.6983 Batch Accuracy: 68.9062\n",
            "400th Batch Loss: 0.5604 Batch Accuracy: 68.9180\n",
            "420th Batch Loss: 0.8743 Batch Accuracy: 68.9137\n",
            "440th Batch Loss: 0.9391 Batch Accuracy: 68.8317\n",
            "460th Batch Loss: 0.6771 Batch Accuracy: 68.7330\n",
            "480th Batch Loss: 0.8312 Batch Accuracy: 68.8118\n",
            "500th Batch Loss: 0.9077 Batch Accuracy: 68.8906\n",
            "520th Batch Loss: 0.9405 Batch Accuracy: 68.9062\n",
            "540th Batch Loss: 0.6877 Batch Accuracy: 68.9005\n",
            "560th Batch Loss: 1.0487 Batch Accuracy: 68.8449\n",
            "580th Batch Loss: 1.2005 Batch Accuracy: 68.8874\n",
            "600th Batch Loss: 1.1576 Batch Accuracy: 68.8828\n",
            "620th Batch Loss: 0.9587 Batch Accuracy: 68.9037\n",
            "640th Batch Loss: 0.8233 Batch Accuracy: 68.9575\n",
            "660th Batch Loss: 1.0351 Batch Accuracy: 68.9228\n",
            "Epoch [6/10] Loss: 0.9307 Epoch Accuracy: 68.9583\n",
            "0\n",
            "Validation Accuracy: 75.46%\n",
            "20th Batch Loss: 0.8896 Batch Accuracy: 70.3906\n",
            "40th Batch Loss: 0.8642 Batch Accuracy: 71.1719\n",
            "60th Batch Loss: 0.5000 Batch Accuracy: 71.5625\n",
            "80th Batch Loss: 0.6416 Batch Accuracy: 70.9375\n",
            "100th Batch Loss: 0.8568 Batch Accuracy: 70.8750\n",
            "120th Batch Loss: 0.7268 Batch Accuracy: 71.0677\n",
            "140th Batch Loss: 0.6690 Batch Accuracy: 70.6696\n",
            "160th Batch Loss: 0.5895 Batch Accuracy: 70.7617\n",
            "180th Batch Loss: 0.5141 Batch Accuracy: 70.4601\n",
            "200th Batch Loss: 0.7767 Batch Accuracy: 70.6016\n",
            "220th Batch Loss: 0.9388 Batch Accuracy: 70.6037\n",
            "240th Batch Loss: 0.8288 Batch Accuracy: 70.6055\n",
            "260th Batch Loss: 0.8043 Batch Accuracy: 70.5829\n",
            "280th Batch Loss: 1.2274 Batch Accuracy: 70.5301\n",
            "300th Batch Loss: 1.0133 Batch Accuracy: 70.2552\n",
            "320th Batch Loss: 0.8297 Batch Accuracy: 70.1562\n",
            "340th Batch Loss: 1.0719 Batch Accuracy: 70.0000\n",
            "360th Batch Loss: 0.7302 Batch Accuracy: 69.9913\n",
            "380th Batch Loss: 0.5917 Batch Accuracy: 70.1275\n",
            "400th Batch Loss: 0.7198 Batch Accuracy: 70.0938\n",
            "420th Batch Loss: 1.0908 Batch Accuracy: 69.9814\n",
            "440th Batch Loss: 0.7196 Batch Accuracy: 70.0604\n",
            "460th Batch Loss: 0.9638 Batch Accuracy: 69.9355\n",
            "480th Batch Loss: 0.7187 Batch Accuracy: 69.8600\n",
            "500th Batch Loss: 0.4980 Batch Accuracy: 69.9000\n",
            "520th Batch Loss: 0.7301 Batch Accuracy: 69.9429\n",
            "540th Batch Loss: 0.6631 Batch Accuracy: 70.0723\n",
            "560th Batch Loss: 0.5286 Batch Accuracy: 70.1256\n",
            "580th Batch Loss: 0.6476 Batch Accuracy: 70.0835\n",
            "600th Batch Loss: 0.6417 Batch Accuracy: 70.0781\n",
            "620th Batch Loss: 0.8915 Batch Accuracy: 70.1386\n",
            "640th Batch Loss: 0.5643 Batch Accuracy: 70.1807\n",
            "660th Batch Loss: 1.0598 Batch Accuracy: 70.1468\n",
            "Epoch [7/10] Loss: 0.8061 Epoch Accuracy: 70.1782\n",
            "0\n",
            "Validation Accuracy: 76.33%\n",
            "20th Batch Loss: 0.7062 Batch Accuracy: 71.4844\n",
            "40th Batch Loss: 0.8878 Batch Accuracy: 72.3438\n",
            "60th Batch Loss: 0.6169 Batch Accuracy: 72.3698\n",
            "80th Batch Loss: 0.8452 Batch Accuracy: 72.4609\n",
            "100th Batch Loss: 0.7157 Batch Accuracy: 73.1406\n",
            "120th Batch Loss: 0.9324 Batch Accuracy: 72.9688\n",
            "140th Batch Loss: 0.8805 Batch Accuracy: 72.1987\n",
            "160th Batch Loss: 0.7555 Batch Accuracy: 72.0996\n",
            "180th Batch Loss: 0.9689 Batch Accuracy: 72.2569\n",
            "200th Batch Loss: 0.8441 Batch Accuracy: 72.0781\n",
            "220th Batch Loss: 0.7758 Batch Accuracy: 72.0597\n",
            "240th Batch Loss: 0.9347 Batch Accuracy: 72.0443\n",
            "260th Batch Loss: 0.9349 Batch Accuracy: 71.9050\n",
            "280th Batch Loss: 0.6929 Batch Accuracy: 71.9754\n",
            "300th Batch Loss: 0.9799 Batch Accuracy: 71.9792\n",
            "320th Batch Loss: 0.9856 Batch Accuracy: 71.9434\n",
            "340th Batch Loss: 0.6692 Batch Accuracy: 72.0588\n",
            "360th Batch Loss: 0.6908 Batch Accuracy: 72.1094\n",
            "380th Batch Loss: 0.5828 Batch Accuracy: 72.1094\n",
            "400th Batch Loss: 0.4435 Batch Accuracy: 72.1367\n",
            "420th Batch Loss: 0.6463 Batch Accuracy: 72.2768\n",
            "440th Batch Loss: 1.3160 Batch Accuracy: 72.0952\n",
            "460th Batch Loss: 0.7495 Batch Accuracy: 72.0686\n",
            "480th Batch Loss: 0.7219 Batch Accuracy: 71.9759\n",
            "500th Batch Loss: 0.6895 Batch Accuracy: 72.0812\n",
            "520th Batch Loss: 0.6276 Batch Accuracy: 72.0282\n",
            "540th Batch Loss: 1.1871 Batch Accuracy: 71.9994\n",
            "560th Batch Loss: 0.7627 Batch Accuracy: 71.9782\n",
            "580th Batch Loss: 0.6103 Batch Accuracy: 72.0312\n",
            "600th Batch Loss: 0.8170 Batch Accuracy: 72.0547\n",
            "620th Batch Loss: 1.0404 Batch Accuracy: 72.0640\n",
            "640th Batch Loss: 0.8216 Batch Accuracy: 71.9946\n",
            "660th Batch Loss: 0.5724 Batch Accuracy: 72.0218\n",
            "Epoch [8/10] Loss: 0.8190 Epoch Accuracy: 71.9954\n",
            "0\n",
            "Validation Accuracy: 77.75%\n",
            "20th Batch Loss: 0.8158 Batch Accuracy: 74.6875\n",
            "40th Batch Loss: 0.7417 Batch Accuracy: 72.5000\n",
            "60th Batch Loss: 0.8839 Batch Accuracy: 72.0833\n",
            "80th Batch Loss: 0.9571 Batch Accuracy: 71.7383\n",
            "100th Batch Loss: 0.7473 Batch Accuracy: 71.7969\n",
            "120th Batch Loss: 0.5894 Batch Accuracy: 71.9401\n",
            "140th Batch Loss: 0.6619 Batch Accuracy: 72.1652\n",
            "160th Batch Loss: 0.9242 Batch Accuracy: 72.3047\n",
            "180th Batch Loss: 0.7027 Batch Accuracy: 72.0052\n",
            "200th Batch Loss: 0.9031 Batch Accuracy: 71.9688\n",
            "220th Batch Loss: 1.1005 Batch Accuracy: 71.8963\n",
            "240th Batch Loss: 0.6948 Batch Accuracy: 72.0378\n",
            "260th Batch Loss: 0.7029 Batch Accuracy: 72.1034\n",
            "280th Batch Loss: 0.6710 Batch Accuracy: 72.2042\n",
            "300th Batch Loss: 0.7528 Batch Accuracy: 72.2240\n",
            "320th Batch Loss: 0.6099 Batch Accuracy: 72.2461\n",
            "340th Batch Loss: 0.5793 Batch Accuracy: 72.1921\n",
            "360th Batch Loss: 1.0153 Batch Accuracy: 72.0833\n",
            "380th Batch Loss: 0.5620 Batch Accuracy: 71.9778\n",
            "400th Batch Loss: 0.8977 Batch Accuracy: 71.7969\n",
            "420th Batch Loss: 0.7504 Batch Accuracy: 71.8043\n",
            "440th Batch Loss: 0.5903 Batch Accuracy: 71.8999\n",
            "460th Batch Loss: 0.6236 Batch Accuracy: 71.9327\n",
            "480th Batch Loss: 0.6113 Batch Accuracy: 71.9434\n",
            "500th Batch Loss: 0.8040 Batch Accuracy: 72.0156\n",
            "520th Batch Loss: 0.7072 Batch Accuracy: 72.0042\n",
            "540th Batch Loss: 1.1075 Batch Accuracy: 72.0428\n",
            "560th Batch Loss: 0.5578 Batch Accuracy: 72.1038\n",
            "580th Batch Loss: 1.2375 Batch Accuracy: 72.1525\n",
            "600th Batch Loss: 0.8942 Batch Accuracy: 72.0234\n",
            "620th Batch Loss: 0.5686 Batch Accuracy: 72.0968\n",
            "640th Batch Loss: 0.8967 Batch Accuracy: 72.1826\n",
            "660th Batch Loss: 0.5755 Batch Accuracy: 72.2348\n",
            "Epoch [9/10] Loss: 0.6043 Epoch Accuracy: 72.2708\n",
            "0\n",
            "Validation Accuracy: 78.38%\n",
            "20th Batch Loss: 0.7505 Batch Accuracy: 73.5938\n",
            "40th Batch Loss: 1.0656 Batch Accuracy: 71.3672\n",
            "60th Batch Loss: 0.8121 Batch Accuracy: 71.2240\n",
            "80th Batch Loss: 0.9648 Batch Accuracy: 71.8945\n",
            "100th Batch Loss: 0.9481 Batch Accuracy: 72.7656\n",
            "120th Batch Loss: 0.5292 Batch Accuracy: 73.0208\n",
            "140th Batch Loss: 0.8703 Batch Accuracy: 73.3259\n",
            "160th Batch Loss: 0.9881 Batch Accuracy: 73.4375\n",
            "180th Batch Loss: 0.5675 Batch Accuracy: 73.4115\n",
            "200th Batch Loss: 0.8555 Batch Accuracy: 73.2422\n",
            "220th Batch Loss: 0.5303 Batch Accuracy: 73.2528\n",
            "240th Batch Loss: 0.8751 Batch Accuracy: 73.1445\n",
            "260th Batch Loss: 0.6684 Batch Accuracy: 73.1971\n",
            "280th Batch Loss: 0.7447 Batch Accuracy: 73.1975\n",
            "300th Batch Loss: 0.5970 Batch Accuracy: 73.1302\n",
            "320th Batch Loss: 0.8864 Batch Accuracy: 73.1299\n",
            "340th Batch Loss: 0.6378 Batch Accuracy: 73.1801\n",
            "360th Batch Loss: 0.6431 Batch Accuracy: 73.2726\n",
            "380th Batch Loss: 0.5834 Batch Accuracy: 73.2854\n",
            "400th Batch Loss: 0.6213 Batch Accuracy: 73.3086\n",
            "420th Batch Loss: 0.6327 Batch Accuracy: 73.2850\n",
            "440th Batch Loss: 0.5421 Batch Accuracy: 73.3310\n",
            "460th Batch Loss: 0.6511 Batch Accuracy: 73.4477\n",
            "480th Batch Loss: 0.6694 Batch Accuracy: 73.5189\n",
            "500th Batch Loss: 0.6963 Batch Accuracy: 73.5594\n",
            "520th Batch Loss: 0.5800 Batch Accuracy: 73.5998\n",
            "540th Batch Loss: 0.8787 Batch Accuracy: 73.6516\n",
            "560th Batch Loss: 0.4131 Batch Accuracy: 73.7054\n",
            "580th Batch Loss: 0.8262 Batch Accuracy: 73.6853\n",
            "600th Batch Loss: 0.8050 Batch Accuracy: 73.5339\n",
            "620th Batch Loss: 0.7619 Batch Accuracy: 73.4501\n",
            "640th Batch Loss: 1.0802 Batch Accuracy: 73.4155\n",
            "660th Batch Loss: 0.6382 Batch Accuracy: 73.3830\n",
            "Epoch [10/10] Loss: 0.6433 Epoch Accuracy: 73.2824\n",
            "1\n",
            "Validation Accuracy: 76.56%\n",
            "Fold 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 2.0649 Batch Accuracy: 18.2031\n",
            "40th Batch Loss: 1.9758 Batch Accuracy: 22.8516\n",
            "60th Batch Loss: 1.7973 Batch Accuracy: 26.5104\n",
            "80th Batch Loss: 1.9143 Batch Accuracy: 27.8125\n",
            "100th Batch Loss: 1.9717 Batch Accuracy: 28.8750\n",
            "120th Batch Loss: 1.5834 Batch Accuracy: 29.7917\n",
            "140th Batch Loss: 1.6843 Batch Accuracy: 30.6920\n",
            "160th Batch Loss: 1.6858 Batch Accuracy: 31.4062\n",
            "180th Batch Loss: 1.5582 Batch Accuracy: 32.3698\n",
            "200th Batch Loss: 1.7960 Batch Accuracy: 33.3984\n",
            "220th Batch Loss: 1.7438 Batch Accuracy: 33.8778\n",
            "240th Batch Loss: 1.5983 Batch Accuracy: 34.7526\n",
            "260th Batch Loss: 1.7247 Batch Accuracy: 35.4567\n",
            "280th Batch Loss: 1.5850 Batch Accuracy: 35.9766\n",
            "300th Batch Loss: 1.8746 Batch Accuracy: 36.4219\n",
            "320th Batch Loss: 1.5473 Batch Accuracy: 36.9971\n",
            "340th Batch Loss: 1.2830 Batch Accuracy: 37.4494\n",
            "360th Batch Loss: 1.3993 Batch Accuracy: 37.8733\n",
            "380th Batch Loss: 1.4981 Batch Accuracy: 38.4169\n",
            "400th Batch Loss: 1.4524 Batch Accuracy: 39.0000\n",
            "420th Batch Loss: 1.3246 Batch Accuracy: 39.3750\n",
            "440th Batch Loss: 1.3083 Batch Accuracy: 39.8793\n",
            "460th Batch Loss: 1.4183 Batch Accuracy: 40.2717\n",
            "480th Batch Loss: 1.4030 Batch Accuracy: 40.7650\n",
            "500th Batch Loss: 1.5031 Batch Accuracy: 40.9844\n",
            "520th Batch Loss: 1.2975 Batch Accuracy: 41.3672\n",
            "540th Batch Loss: 1.1161 Batch Accuracy: 41.6985\n",
            "560th Batch Loss: 1.4955 Batch Accuracy: 42.0759\n",
            "580th Batch Loss: 1.6975 Batch Accuracy: 42.3788\n",
            "600th Batch Loss: 1.3313 Batch Accuracy: 42.7057\n",
            "620th Batch Loss: 1.1679 Batch Accuracy: 43.0872\n",
            "640th Batch Loss: 1.6712 Batch Accuracy: 43.3643\n",
            "660th Batch Loss: 1.3460 Batch Accuracy: 43.7311\n",
            "Epoch [1/10] Loss: 1.2684 Epoch Accuracy: 43.9769\n",
            "0\n",
            "Validation Accuracy: 58.35%\n",
            "20th Batch Loss: 1.6940 Batch Accuracy: 43.8281\n",
            "40th Batch Loss: 1.5698 Batch Accuracy: 42.9688\n",
            "60th Batch Loss: 1.2640 Batch Accuracy: 44.3750\n",
            "80th Batch Loss: 1.2488 Batch Accuracy: 46.0547\n",
            "100th Batch Loss: 1.1827 Batch Accuracy: 46.9062\n",
            "120th Batch Loss: 1.4162 Batch Accuracy: 47.9297\n",
            "140th Batch Loss: 1.1898 Batch Accuracy: 49.3304\n",
            "160th Batch Loss: 1.2827 Batch Accuracy: 49.3750\n",
            "180th Batch Loss: 1.6177 Batch Accuracy: 49.9653\n",
            "200th Batch Loss: 0.9998 Batch Accuracy: 50.3359\n",
            "220th Batch Loss: 1.1819 Batch Accuracy: 50.5043\n",
            "240th Batch Loss: 1.2257 Batch Accuracy: 51.0286\n",
            "260th Batch Loss: 1.4848 Batch Accuracy: 51.3762\n",
            "280th Batch Loss: 1.2425 Batch Accuracy: 51.7634\n",
            "300th Batch Loss: 1.2594 Batch Accuracy: 51.9427\n",
            "320th Batch Loss: 0.9520 Batch Accuracy: 52.3926\n",
            "340th Batch Loss: 1.1589 Batch Accuracy: 52.5230\n",
            "360th Batch Loss: 0.8963 Batch Accuracy: 52.9427\n",
            "380th Batch Loss: 1.1741 Batch Accuracy: 53.3265\n",
            "400th Batch Loss: 0.9832 Batch Accuracy: 53.5977\n",
            "420th Batch Loss: 1.3685 Batch Accuracy: 53.7054\n",
            "440th Batch Loss: 1.0281 Batch Accuracy: 54.0021\n",
            "460th Batch Loss: 1.3216 Batch Accuracy: 54.1406\n",
            "480th Batch Loss: 1.1314 Batch Accuracy: 54.1764\n",
            "500th Batch Loss: 0.8190 Batch Accuracy: 54.5219\n",
            "520th Batch Loss: 0.8530 Batch Accuracy: 54.7596\n",
            "540th Batch Loss: 0.9876 Batch Accuracy: 54.8351\n",
            "560th Batch Loss: 0.9299 Batch Accuracy: 55.0112\n",
            "580th Batch Loss: 1.1106 Batch Accuracy: 55.0754\n",
            "600th Batch Loss: 1.2379 Batch Accuracy: 55.1042\n",
            "620th Batch Loss: 1.4511 Batch Accuracy: 55.1890\n",
            "640th Batch Loss: 1.5058 Batch Accuracy: 55.3101\n",
            "660th Batch Loss: 1.0352 Batch Accuracy: 55.4853\n",
            "Epoch [2/10] Loss: 1.3390 Epoch Accuracy: 55.5949\n",
            "0\n",
            "Validation Accuracy: 65.50%\n",
            "20th Batch Loss: 0.9196 Batch Accuracy: 60.7031\n",
            "40th Batch Loss: 1.2251 Batch Accuracy: 60.4688\n",
            "60th Batch Loss: 1.1727 Batch Accuracy: 61.3802\n",
            "80th Batch Loss: 1.1465 Batch Accuracy: 60.7227\n",
            "100th Batch Loss: 1.2276 Batch Accuracy: 60.9375\n",
            "120th Batch Loss: 1.2396 Batch Accuracy: 60.5990\n",
            "140th Batch Loss: 1.0015 Batch Accuracy: 60.7031\n",
            "160th Batch Loss: 1.0829 Batch Accuracy: 60.8691\n",
            "180th Batch Loss: 1.1619 Batch Accuracy: 61.1806\n",
            "200th Batch Loss: 0.8102 Batch Accuracy: 61.2344\n",
            "220th Batch Loss: 0.9631 Batch Accuracy: 61.3423\n",
            "240th Batch Loss: 1.0786 Batch Accuracy: 61.4128\n",
            "260th Batch Loss: 1.0434 Batch Accuracy: 61.5625\n",
            "280th Batch Loss: 1.1302 Batch Accuracy: 61.6853\n",
            "300th Batch Loss: 1.1753 Batch Accuracy: 61.8125\n",
            "320th Batch Loss: 1.0222 Batch Accuracy: 61.8750\n",
            "340th Batch Loss: 0.9707 Batch Accuracy: 62.0818\n",
            "360th Batch Loss: 0.7345 Batch Accuracy: 62.0443\n",
            "380th Batch Loss: 0.8439 Batch Accuracy: 62.1464\n",
            "400th Batch Loss: 1.0170 Batch Accuracy: 62.2617\n",
            "420th Batch Loss: 0.9045 Batch Accuracy: 62.2805\n",
            "440th Batch Loss: 0.7813 Batch Accuracy: 62.2159\n",
            "460th Batch Loss: 1.2910 Batch Accuracy: 62.2622\n",
            "480th Batch Loss: 0.7895 Batch Accuracy: 62.2982\n",
            "500th Batch Loss: 0.9234 Batch Accuracy: 62.3031\n",
            "520th Batch Loss: 1.2812 Batch Accuracy: 62.3708\n",
            "540th Batch Loss: 0.7322 Batch Accuracy: 62.3032\n",
            "560th Batch Loss: 0.8759 Batch Accuracy: 62.2461\n",
            "580th Batch Loss: 0.8956 Batch Accuracy: 62.3060\n",
            "600th Batch Loss: 0.8004 Batch Accuracy: 62.3724\n",
            "620th Batch Loss: 0.8520 Batch Accuracy: 62.4017\n",
            "640th Batch Loss: 0.9045 Batch Accuracy: 62.4609\n",
            "660th Batch Loss: 0.9080 Batch Accuracy: 62.5189\n",
            "Epoch [3/10] Loss: 1.0880 Epoch Accuracy: 62.5093\n",
            "0\n",
            "Validation Accuracy: 70.06%\n",
            "20th Batch Loss: 1.2683 Batch Accuracy: 61.3281\n",
            "40th Batch Loss: 1.0189 Batch Accuracy: 63.2422\n",
            "60th Batch Loss: 0.9579 Batch Accuracy: 63.6458\n",
            "80th Batch Loss: 1.2170 Batch Accuracy: 63.5742\n",
            "100th Batch Loss: 1.2079 Batch Accuracy: 63.5781\n",
            "120th Batch Loss: 0.7965 Batch Accuracy: 63.2552\n",
            "140th Batch Loss: 0.9855 Batch Accuracy: 63.2143\n",
            "160th Batch Loss: 0.5386 Batch Accuracy: 63.5352\n",
            "180th Batch Loss: 0.9935 Batch Accuracy: 63.4809\n",
            "200th Batch Loss: 0.8748 Batch Accuracy: 63.4922\n",
            "220th Batch Loss: 0.8524 Batch Accuracy: 63.8281\n",
            "240th Batch Loss: 0.7165 Batch Accuracy: 63.8281\n",
            "260th Batch Loss: 0.9773 Batch Accuracy: 64.1046\n",
            "280th Batch Loss: 1.1074 Batch Accuracy: 63.9732\n",
            "300th Batch Loss: 0.8054 Batch Accuracy: 63.9948\n",
            "320th Batch Loss: 1.0185 Batch Accuracy: 64.0918\n",
            "340th Batch Loss: 0.9342 Batch Accuracy: 64.2463\n",
            "360th Batch Loss: 1.0674 Batch Accuracy: 64.3142\n",
            "380th Batch Loss: 1.3026 Batch Accuracy: 64.3257\n",
            "400th Batch Loss: 0.9327 Batch Accuracy: 64.4375\n",
            "420th Batch Loss: 0.7589 Batch Accuracy: 64.4085\n",
            "440th Batch Loss: 1.1804 Batch Accuracy: 64.3821\n",
            "460th Batch Loss: 0.9122 Batch Accuracy: 64.5346\n",
            "480th Batch Loss: 1.4675 Batch Accuracy: 64.5215\n",
            "500th Batch Loss: 1.2319 Batch Accuracy: 64.6000\n",
            "520th Batch Loss: 1.0004 Batch Accuracy: 64.5252\n",
            "540th Batch Loss: 0.7607 Batch Accuracy: 64.6209\n",
            "560th Batch Loss: 1.1692 Batch Accuracy: 64.6875\n",
            "580th Batch Loss: 0.7003 Batch Accuracy: 64.7522\n",
            "600th Batch Loss: 1.0644 Batch Accuracy: 64.7266\n",
            "620th Batch Loss: 1.0412 Batch Accuracy: 64.7152\n",
            "640th Batch Loss: 0.7602 Batch Accuracy: 64.7803\n",
            "660th Batch Loss: 0.8543 Batch Accuracy: 64.9455\n",
            "Epoch [4/10] Loss: 0.7511 Epoch Accuracy: 64.9815\n",
            "0\n",
            "Validation Accuracy: 72.79%\n",
            "20th Batch Loss: 0.6919 Batch Accuracy: 67.7344\n",
            "40th Batch Loss: 0.8134 Batch Accuracy: 67.0312\n",
            "60th Batch Loss: 1.1469 Batch Accuracy: 66.9271\n",
            "80th Batch Loss: 0.7953 Batch Accuracy: 66.7773\n",
            "100th Batch Loss: 0.7604 Batch Accuracy: 67.0312\n",
            "120th Batch Loss: 0.8773 Batch Accuracy: 67.4219\n",
            "140th Batch Loss: 0.6813 Batch Accuracy: 67.5223\n",
            "160th Batch Loss: 0.9436 Batch Accuracy: 67.4414\n",
            "180th Batch Loss: 0.9764 Batch Accuracy: 67.3524\n",
            "200th Batch Loss: 1.2691 Batch Accuracy: 67.5938\n",
            "220th Batch Loss: 0.6692 Batch Accuracy: 67.5426\n",
            "240th Batch Loss: 0.9125 Batch Accuracy: 67.6497\n",
            "260th Batch Loss: 0.7382 Batch Accuracy: 67.4940\n",
            "280th Batch Loss: 0.8656 Batch Accuracy: 67.4163\n",
            "300th Batch Loss: 0.9877 Batch Accuracy: 67.4375\n",
            "320th Batch Loss: 0.4231 Batch Accuracy: 67.6074\n",
            "340th Batch Loss: 0.7042 Batch Accuracy: 67.5873\n",
            "360th Batch Loss: 0.6915 Batch Accuracy: 67.6693\n",
            "380th Batch Loss: 0.9540 Batch Accuracy: 67.6110\n",
            "400th Batch Loss: 0.7862 Batch Accuracy: 67.6211\n",
            "420th Batch Loss: 0.9353 Batch Accuracy: 67.5967\n",
            "440th Batch Loss: 0.6463 Batch Accuracy: 67.7521\n",
            "460th Batch Loss: 0.8897 Batch Accuracy: 67.7208\n",
            "480th Batch Loss: 0.9590 Batch Accuracy: 67.7637\n",
            "500th Batch Loss: 0.9919 Batch Accuracy: 67.7438\n",
            "520th Batch Loss: 0.7294 Batch Accuracy: 67.7945\n",
            "540th Batch Loss: 0.9827 Batch Accuracy: 67.7025\n",
            "560th Batch Loss: 0.5108 Batch Accuracy: 67.6144\n",
            "580th Batch Loss: 0.9930 Batch Accuracy: 67.6589\n",
            "600th Batch Loss: 1.3078 Batch Accuracy: 67.6354\n",
            "620th Batch Loss: 0.7842 Batch Accuracy: 67.5302\n",
            "640th Batch Loss: 0.7891 Batch Accuracy: 67.5146\n",
            "660th Batch Loss: 0.8804 Batch Accuracy: 67.5734\n",
            "Epoch [5/10] Loss: 0.7499 Epoch Accuracy: 67.5694\n",
            "1\n",
            "Validation Accuracy: 72.15%\n",
            "20th Batch Loss: 0.8603 Batch Accuracy: 68.5938\n",
            "40th Batch Loss: 0.5061 Batch Accuracy: 70.5078\n",
            "60th Batch Loss: 0.6825 Batch Accuracy: 69.8177\n",
            "80th Batch Loss: 0.9843 Batch Accuracy: 69.5117\n",
            "100th Batch Loss: 0.8309 Batch Accuracy: 69.6719\n",
            "120th Batch Loss: 0.6501 Batch Accuracy: 69.4401\n",
            "140th Batch Loss: 0.7374 Batch Accuracy: 69.6875\n",
            "160th Batch Loss: 1.1496 Batch Accuracy: 69.7656\n",
            "180th Batch Loss: 0.8726 Batch Accuracy: 69.6701\n",
            "200th Batch Loss: 0.7106 Batch Accuracy: 69.4219\n",
            "220th Batch Loss: 0.8407 Batch Accuracy: 69.4815\n",
            "240th Batch Loss: 0.7672 Batch Accuracy: 69.3880\n",
            "260th Batch Loss: 0.7201 Batch Accuracy: 69.4952\n",
            "280th Batch Loss: 0.9582 Batch Accuracy: 69.3917\n",
            "300th Batch Loss: 0.8273 Batch Accuracy: 69.4792\n",
            "320th Batch Loss: 0.5950 Batch Accuracy: 69.3213\n",
            "340th Batch Loss: 0.7616 Batch Accuracy: 69.1452\n",
            "360th Batch Loss: 0.9406 Batch Accuracy: 68.9714\n",
            "380th Batch Loss: 0.6669 Batch Accuracy: 68.8939\n",
            "400th Batch Loss: 0.5940 Batch Accuracy: 69.0117\n",
            "420th Batch Loss: 0.8272 Batch Accuracy: 68.9137\n",
            "440th Batch Loss: 0.9006 Batch Accuracy: 68.8565\n",
            "460th Batch Loss: 0.7481 Batch Accuracy: 68.7602\n",
            "480th Batch Loss: 0.8700 Batch Accuracy: 68.7305\n",
            "500th Batch Loss: 0.7745 Batch Accuracy: 68.9688\n",
            "520th Batch Loss: 0.6737 Batch Accuracy: 68.8942\n",
            "540th Batch Loss: 0.9216 Batch Accuracy: 68.9207\n",
            "560th Batch Loss: 0.6577 Batch Accuracy: 68.9816\n",
            "580th Batch Loss: 0.6762 Batch Accuracy: 68.9574\n",
            "600th Batch Loss: 1.1235 Batch Accuracy: 68.9714\n",
            "620th Batch Loss: 0.7829 Batch Accuracy: 69.0272\n",
            "640th Batch Loss: 0.8196 Batch Accuracy: 69.0259\n",
            "660th Batch Loss: 0.7434 Batch Accuracy: 69.0672\n",
            "Epoch [6/10] Loss: 0.7156 Epoch Accuracy: 69.0579\n",
            "0\n",
            "Validation Accuracy: 74.31%\n",
            "20th Batch Loss: 1.0664 Batch Accuracy: 65.8594\n",
            "40th Batch Loss: 0.9441 Batch Accuracy: 68.3594\n",
            "60th Batch Loss: 0.6674 Batch Accuracy: 70.0000\n",
            "80th Batch Loss: 0.8321 Batch Accuracy: 70.1562\n",
            "100th Batch Loss: 1.2338 Batch Accuracy: 69.7031\n",
            "120th Batch Loss: 0.6494 Batch Accuracy: 70.1432\n",
            "140th Batch Loss: 0.9081 Batch Accuracy: 70.7366\n",
            "160th Batch Loss: 0.6594 Batch Accuracy: 70.8398\n",
            "180th Batch Loss: 0.7956 Batch Accuracy: 71.3542\n",
            "200th Batch Loss: 1.0989 Batch Accuracy: 71.0625\n",
            "220th Batch Loss: 0.7788 Batch Accuracy: 70.8381\n",
            "240th Batch Loss: 0.7615 Batch Accuracy: 70.9115\n",
            "260th Batch Loss: 0.9340 Batch Accuracy: 70.7632\n",
            "280th Batch Loss: 0.7227 Batch Accuracy: 70.5748\n",
            "300th Batch Loss: 1.2156 Batch Accuracy: 70.4844\n",
            "320th Batch Loss: 0.5689 Batch Accuracy: 70.5859\n",
            "340th Batch Loss: 1.1731 Batch Accuracy: 70.6296\n",
            "360th Batch Loss: 0.9233 Batch Accuracy: 70.7509\n",
            "380th Batch Loss: 0.5518 Batch Accuracy: 70.7977\n",
            "400th Batch Loss: 0.6870 Batch Accuracy: 70.8672\n",
            "420th Batch Loss: 0.8590 Batch Accuracy: 70.8743\n",
            "440th Batch Loss: 0.5604 Batch Accuracy: 70.8558\n",
            "460th Batch Loss: 0.4639 Batch Accuracy: 70.8390\n",
            "480th Batch Loss: 0.8880 Batch Accuracy: 70.9147\n",
            "500th Batch Loss: 0.8225 Batch Accuracy: 70.9781\n",
            "520th Batch Loss: 0.5191 Batch Accuracy: 71.0547\n",
            "540th Batch Loss: 0.9989 Batch Accuracy: 70.9751\n",
            "560th Batch Loss: 0.9058 Batch Accuracy: 71.1217\n",
            "580th Batch Loss: 0.7453 Batch Accuracy: 70.9644\n",
            "600th Batch Loss: 0.6922 Batch Accuracy: 71.0729\n",
            "620th Batch Loss: 0.6232 Batch Accuracy: 71.0156\n",
            "640th Batch Loss: 0.8094 Batch Accuracy: 71.0645\n",
            "660th Batch Loss: 0.7330 Batch Accuracy: 71.0251\n",
            "Epoch [7/10] Loss: 0.9129 Epoch Accuracy: 71.0579\n",
            "0\n",
            "Validation Accuracy: 76.50%\n",
            "20th Batch Loss: 0.6625 Batch Accuracy: 72.1875\n",
            "40th Batch Loss: 1.2125 Batch Accuracy: 71.7188\n",
            "60th Batch Loss: 0.5966 Batch Accuracy: 70.3646\n",
            "80th Batch Loss: 0.7749 Batch Accuracy: 70.6445\n",
            "100th Batch Loss: 0.7960 Batch Accuracy: 70.6875\n",
            "120th Batch Loss: 0.6438 Batch Accuracy: 71.7057\n",
            "140th Batch Loss: 0.5174 Batch Accuracy: 71.4844\n",
            "160th Batch Loss: 0.5969 Batch Accuracy: 71.3477\n",
            "180th Batch Loss: 1.0175 Batch Accuracy: 71.5712\n",
            "200th Batch Loss: 0.6993 Batch Accuracy: 71.5938\n",
            "220th Batch Loss: 0.5387 Batch Accuracy: 71.5128\n",
            "240th Batch Loss: 0.8727 Batch Accuracy: 71.4714\n",
            "260th Batch Loss: 0.8437 Batch Accuracy: 71.4303\n",
            "280th Batch Loss: 0.5979 Batch Accuracy: 71.3560\n",
            "300th Batch Loss: 0.7020 Batch Accuracy: 71.4844\n",
            "320th Batch Loss: 0.9640 Batch Accuracy: 71.6943\n",
            "340th Batch Loss: 0.8292 Batch Accuracy: 71.6085\n",
            "360th Batch Loss: 0.8328 Batch Accuracy: 71.6753\n",
            "380th Batch Loss: 0.5954 Batch Accuracy: 71.7270\n",
            "400th Batch Loss: 0.4620 Batch Accuracy: 71.6836\n",
            "420th Batch Loss: 0.5782 Batch Accuracy: 71.7448\n",
            "440th Batch Loss: 0.7674 Batch Accuracy: 71.6939\n",
            "460th Batch Loss: 0.8524 Batch Accuracy: 71.6372\n",
            "480th Batch Loss: 0.4039 Batch Accuracy: 71.6602\n",
            "500th Batch Loss: 0.9861 Batch Accuracy: 71.6188\n",
            "520th Batch Loss: 0.7923 Batch Accuracy: 71.6166\n",
            "540th Batch Loss: 1.2309 Batch Accuracy: 71.7159\n",
            "560th Batch Loss: 0.8632 Batch Accuracy: 71.7104\n",
            "580th Batch Loss: 0.9451 Batch Accuracy: 71.6352\n",
            "600th Batch Loss: 0.5018 Batch Accuracy: 71.6510\n",
            "620th Batch Loss: 0.8152 Batch Accuracy: 71.7742\n",
            "640th Batch Loss: 0.5890 Batch Accuracy: 71.7749\n",
            "660th Batch Loss: 0.9866 Batch Accuracy: 71.7472\n",
            "Epoch [8/10] Loss: 0.8760 Epoch Accuracy: 71.8287\n",
            "0\n",
            "Validation Accuracy: 77.31%\n",
            "20th Batch Loss: 0.9102 Batch Accuracy: 72.3438\n",
            "40th Batch Loss: 0.6249 Batch Accuracy: 71.7578\n",
            "60th Batch Loss: 0.7528 Batch Accuracy: 71.6927\n",
            "80th Batch Loss: 0.6027 Batch Accuracy: 72.5781\n",
            "100th Batch Loss: 0.6896 Batch Accuracy: 72.5469\n",
            "120th Batch Loss: 0.5973 Batch Accuracy: 72.9557\n",
            "140th Batch Loss: 0.9173 Batch Accuracy: 72.9018\n",
            "160th Batch Loss: 0.4589 Batch Accuracy: 73.4180\n",
            "180th Batch Loss: 0.5604 Batch Accuracy: 73.5330\n",
            "200th Batch Loss: 0.5704 Batch Accuracy: 73.6875\n",
            "220th Batch Loss: 0.6959 Batch Accuracy: 73.3736\n",
            "240th Batch Loss: 0.5679 Batch Accuracy: 73.0273\n",
            "260th Batch Loss: 0.6084 Batch Accuracy: 73.0469\n",
            "280th Batch Loss: 0.6781 Batch Accuracy: 73.1920\n",
            "300th Batch Loss: 0.8730 Batch Accuracy: 73.1667\n",
            "320th Batch Loss: 0.8210 Batch Accuracy: 73.1738\n",
            "340th Batch Loss: 1.2666 Batch Accuracy: 72.9825\n",
            "360th Batch Loss: 0.4893 Batch Accuracy: 72.7908\n",
            "380th Batch Loss: 0.5279 Batch Accuracy: 72.8289\n",
            "400th Batch Loss: 0.4958 Batch Accuracy: 72.8438\n",
            "420th Batch Loss: 0.6148 Batch Accuracy: 72.9836\n",
            "440th Batch Loss: 1.0424 Batch Accuracy: 72.9510\n",
            "460th Batch Loss: 1.2523 Batch Accuracy: 72.9959\n",
            "480th Batch Loss: 0.4841 Batch Accuracy: 72.8581\n",
            "500th Batch Loss: 0.9794 Batch Accuracy: 72.8906\n",
            "520th Batch Loss: 0.5746 Batch Accuracy: 72.8275\n",
            "540th Batch Loss: 0.6805 Batch Accuracy: 72.9282\n",
            "560th Batch Loss: 0.7144 Batch Accuracy: 72.8460\n",
            "580th Batch Loss: 0.8300 Batch Accuracy: 72.9068\n",
            "600th Batch Loss: 0.8693 Batch Accuracy: 72.8724\n",
            "620th Batch Loss: 1.3173 Batch Accuracy: 72.8705\n",
            "640th Batch Loss: 0.8936 Batch Accuracy: 72.7393\n",
            "660th Batch Loss: 0.6932 Batch Accuracy: 72.8433\n",
            "Epoch [9/10] Loss: 0.6164 Epoch Accuracy: 72.8750\n",
            "0\n",
            "Validation Accuracy: 77.96%\n",
            "20th Batch Loss: 0.9023 Batch Accuracy: 73.9062\n",
            "40th Batch Loss: 0.9032 Batch Accuracy: 73.4766\n",
            "60th Batch Loss: 0.9072 Batch Accuracy: 74.2708\n",
            "80th Batch Loss: 0.7485 Batch Accuracy: 74.9414\n",
            "100th Batch Loss: 0.6002 Batch Accuracy: 74.6094\n",
            "120th Batch Loss: 1.1218 Batch Accuracy: 73.9583\n",
            "140th Batch Loss: 1.0592 Batch Accuracy: 74.3080\n",
            "160th Batch Loss: 0.9065 Batch Accuracy: 74.1016\n",
            "180th Batch Loss: 0.5755 Batch Accuracy: 74.2361\n",
            "200th Batch Loss: 0.7272 Batch Accuracy: 74.3203\n",
            "220th Batch Loss: 0.6781 Batch Accuracy: 74.6236\n",
            "240th Batch Loss: 1.0351 Batch Accuracy: 74.7070\n",
            "260th Batch Loss: 0.7248 Batch Accuracy: 74.7416\n",
            "280th Batch Loss: 1.1159 Batch Accuracy: 74.6038\n",
            "300th Batch Loss: 0.7613 Batch Accuracy: 74.5833\n",
            "320th Batch Loss: 0.5232 Batch Accuracy: 74.8438\n",
            "340th Batch Loss: 0.4566 Batch Accuracy: 74.7610\n",
            "360th Batch Loss: 0.7432 Batch Accuracy: 74.7439\n",
            "380th Batch Loss: 0.5728 Batch Accuracy: 74.7492\n",
            "400th Batch Loss: 0.9085 Batch Accuracy: 74.5000\n",
            "420th Batch Loss: 0.6686 Batch Accuracy: 74.5499\n",
            "440th Batch Loss: 0.4957 Batch Accuracy: 74.6697\n",
            "460th Batch Loss: 1.0454 Batch Accuracy: 74.4735\n",
            "480th Batch Loss: 0.9740 Batch Accuracy: 74.4661\n",
            "500th Batch Loss: 0.9417 Batch Accuracy: 74.4375\n",
            "520th Batch Loss: 0.5193 Batch Accuracy: 74.3510\n",
            "540th Batch Loss: 0.8530 Batch Accuracy: 74.2679\n",
            "560th Batch Loss: 0.9008 Batch Accuracy: 74.2801\n",
            "580th Batch Loss: 0.8823 Batch Accuracy: 74.1406\n",
            "600th Batch Loss: 0.8953 Batch Accuracy: 74.1510\n",
            "620th Batch Loss: 0.5321 Batch Accuracy: 74.1431\n",
            "640th Batch Loss: 0.6617 Batch Accuracy: 74.1357\n",
            "660th Batch Loss: 0.4652 Batch Accuracy: 74.1596\n",
            "Epoch [10/10] Loss: 0.7357 Epoch Accuracy: 74.0972\n",
            "0\n",
            "Validation Accuracy: 78.73%\n",
            "Fold 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 2.1286 Batch Accuracy: 19.2188\n",
            "40th Batch Loss: 1.7393 Batch Accuracy: 24.1797\n",
            "60th Batch Loss: 1.8011 Batch Accuracy: 26.8490\n",
            "80th Batch Loss: 2.0341 Batch Accuracy: 27.8125\n",
            "100th Batch Loss: 1.8069 Batch Accuracy: 28.9062\n",
            "120th Batch Loss: 1.5750 Batch Accuracy: 29.9479\n",
            "140th Batch Loss: 1.5700 Batch Accuracy: 31.3616\n",
            "160th Batch Loss: 1.5629 Batch Accuracy: 32.5586\n",
            "180th Batch Loss: 1.7652 Batch Accuracy: 33.4462\n",
            "200th Batch Loss: 1.5635 Batch Accuracy: 33.8906\n",
            "220th Batch Loss: 1.8003 Batch Accuracy: 34.5455\n",
            "240th Batch Loss: 1.3682 Batch Accuracy: 35.2799\n",
            "260th Batch Loss: 1.5920 Batch Accuracy: 35.9555\n",
            "280th Batch Loss: 1.8020 Batch Accuracy: 36.0770\n",
            "300th Batch Loss: 1.7504 Batch Accuracy: 36.8177\n",
            "320th Batch Loss: 1.3725 Batch Accuracy: 37.3291\n",
            "340th Batch Loss: 1.4293 Batch Accuracy: 37.7711\n",
            "360th Batch Loss: 1.3656 Batch Accuracy: 38.2161\n",
            "380th Batch Loss: 1.2632 Batch Accuracy: 38.6308\n",
            "400th Batch Loss: 1.4080 Batch Accuracy: 39.0117\n",
            "420th Batch Loss: 1.1633 Batch Accuracy: 39.5089\n",
            "440th Batch Loss: 1.3932 Batch Accuracy: 39.9361\n",
            "460th Batch Loss: 1.0727 Batch Accuracy: 40.3397\n",
            "480th Batch Loss: 1.5671 Batch Accuracy: 40.7227\n",
            "500th Batch Loss: 1.3966 Batch Accuracy: 41.0125\n",
            "520th Batch Loss: 1.5092 Batch Accuracy: 41.4002\n",
            "540th Batch Loss: 1.1212 Batch Accuracy: 41.6638\n",
            "560th Batch Loss: 1.8425 Batch Accuracy: 42.0619\n",
            "580th Batch Loss: 1.1735 Batch Accuracy: 42.2764\n",
            "600th Batch Loss: 1.3812 Batch Accuracy: 42.6094\n",
            "620th Batch Loss: 1.5451 Batch Accuracy: 42.9410\n",
            "640th Batch Loss: 1.2869 Batch Accuracy: 43.3398\n",
            "660th Batch Loss: 1.1914 Batch Accuracy: 43.5559\n",
            "Epoch [1/10] Loss: 1.1414 Epoch Accuracy: 43.7153\n",
            "0\n",
            "Validation Accuracy: 59.75%\n",
            "20th Batch Loss: 1.6680 Batch Accuracy: 38.5156\n",
            "40th Batch Loss: 1.4160 Batch Accuracy: 41.0938\n",
            "60th Batch Loss: 1.4389 Batch Accuracy: 44.6354\n",
            "80th Batch Loss: 1.1807 Batch Accuracy: 45.8398\n",
            "100th Batch Loss: 1.0408 Batch Accuracy: 47.4688\n",
            "120th Batch Loss: 1.2309 Batch Accuracy: 48.7370\n",
            "140th Batch Loss: 1.2952 Batch Accuracy: 50.0670\n",
            "160th Batch Loss: 1.1187 Batch Accuracy: 51.0449\n",
            "180th Batch Loss: 1.1287 Batch Accuracy: 51.6493\n",
            "200th Batch Loss: 1.4556 Batch Accuracy: 51.2656\n",
            "220th Batch Loss: 1.4443 Batch Accuracy: 51.4418\n",
            "240th Batch Loss: 1.2513 Batch Accuracy: 51.6016\n",
            "260th Batch Loss: 1.3196 Batch Accuracy: 52.0553\n",
            "280th Batch Loss: 1.3669 Batch Accuracy: 52.3605\n",
            "300th Batch Loss: 1.1943 Batch Accuracy: 52.8490\n",
            "320th Batch Loss: 1.3657 Batch Accuracy: 53.2373\n",
            "340th Batch Loss: 0.8902 Batch Accuracy: 53.5432\n",
            "360th Batch Loss: 0.7906 Batch Accuracy: 53.8542\n",
            "380th Batch Loss: 1.0244 Batch Accuracy: 53.9638\n",
            "400th Batch Loss: 1.3970 Batch Accuracy: 54.1719\n",
            "420th Batch Loss: 1.2081 Batch Accuracy: 54.2188\n",
            "440th Batch Loss: 1.0147 Batch Accuracy: 54.3679\n",
            "460th Batch Loss: 1.0384 Batch Accuracy: 54.4973\n",
            "480th Batch Loss: 0.8392 Batch Accuracy: 54.6680\n",
            "500th Batch Loss: 1.1907 Batch Accuracy: 54.7656\n",
            "520th Batch Loss: 1.1302 Batch Accuracy: 54.9129\n",
            "540th Batch Loss: 0.9926 Batch Accuracy: 55.1042\n",
            "560th Batch Loss: 1.0897 Batch Accuracy: 55.2762\n",
            "580th Batch Loss: 0.9687 Batch Accuracy: 55.2936\n",
            "600th Batch Loss: 1.7193 Batch Accuracy: 55.4766\n",
            "620th Batch Loss: 1.2894 Batch Accuracy: 55.5973\n",
            "640th Batch Loss: 1.0414 Batch Accuracy: 55.6372\n",
            "660th Batch Loss: 0.9777 Batch Accuracy: 55.8144\n",
            "Epoch [2/10] Loss: 1.4545 Epoch Accuracy: 55.8218\n",
            "0\n",
            "Validation Accuracy: 63.71%\n",
            "20th Batch Loss: 1.1689 Batch Accuracy: 57.1094\n",
            "40th Batch Loss: 0.9857 Batch Accuracy: 58.9844\n",
            "60th Batch Loss: 0.9770 Batch Accuracy: 59.8438\n",
            "80th Batch Loss: 0.9654 Batch Accuracy: 60.9570\n",
            "100th Batch Loss: 0.8409 Batch Accuracy: 61.7812\n",
            "120th Batch Loss: 0.9742 Batch Accuracy: 61.3672\n",
            "140th Batch Loss: 0.9891 Batch Accuracy: 61.6741\n",
            "160th Batch Loss: 1.3570 Batch Accuracy: 61.4355\n",
            "180th Batch Loss: 1.1320 Batch Accuracy: 61.5972\n",
            "200th Batch Loss: 0.9176 Batch Accuracy: 61.8516\n",
            "220th Batch Loss: 1.5627 Batch Accuracy: 61.7472\n",
            "240th Batch Loss: 0.8141 Batch Accuracy: 61.6927\n",
            "260th Batch Loss: 1.2636 Batch Accuracy: 61.6046\n",
            "280th Batch Loss: 1.2076 Batch Accuracy: 61.5737\n",
            "300th Batch Loss: 0.9086 Batch Accuracy: 61.6875\n",
            "320th Batch Loss: 1.0049 Batch Accuracy: 61.4062\n",
            "340th Batch Loss: 0.9793 Batch Accuracy: 61.3741\n",
            "360th Batch Loss: 0.9952 Batch Accuracy: 61.5061\n",
            "380th Batch Loss: 0.8677 Batch Accuracy: 61.4967\n",
            "400th Batch Loss: 1.0507 Batch Accuracy: 61.6250\n",
            "420th Batch Loss: 0.8549 Batch Accuracy: 61.7411\n",
            "440th Batch Loss: 0.8508 Batch Accuracy: 61.8075\n",
            "460th Batch Loss: 0.8118 Batch Accuracy: 61.9158\n",
            "480th Batch Loss: 0.5898 Batch Accuracy: 61.9987\n",
            "500th Batch Loss: 0.9493 Batch Accuracy: 61.9719\n",
            "520th Batch Loss: 1.0412 Batch Accuracy: 62.0192\n",
            "540th Batch Loss: 0.9803 Batch Accuracy: 62.0255\n",
            "560th Batch Loss: 0.9687 Batch Accuracy: 62.1205\n",
            "580th Batch Loss: 0.8457 Batch Accuracy: 62.1202\n",
            "600th Batch Loss: 1.4138 Batch Accuracy: 62.1146\n",
            "620th Batch Loss: 1.2424 Batch Accuracy: 62.2404\n",
            "640th Batch Loss: 1.3726 Batch Accuracy: 62.1680\n",
            "660th Batch Loss: 1.2233 Batch Accuracy: 62.2301\n",
            "Epoch [3/10] Loss: 0.6249 Epoch Accuracy: 62.3009\n",
            "0\n",
            "Validation Accuracy: 70.88%\n",
            "20th Batch Loss: 1.2245 Batch Accuracy: 62.4219\n",
            "40th Batch Loss: 1.4285 Batch Accuracy: 64.4141\n",
            "60th Batch Loss: 0.7912 Batch Accuracy: 63.9062\n",
            "80th Batch Loss: 0.8667 Batch Accuracy: 65.1953\n",
            "100th Batch Loss: 1.1220 Batch Accuracy: 65.0312\n",
            "120th Batch Loss: 1.0161 Batch Accuracy: 64.8047\n",
            "140th Batch Loss: 1.1626 Batch Accuracy: 65.1339\n",
            "160th Batch Loss: 1.1339 Batch Accuracy: 64.9316\n",
            "180th Batch Loss: 0.7270 Batch Accuracy: 64.4184\n",
            "200th Batch Loss: 1.5818 Batch Accuracy: 64.6328\n",
            "220th Batch Loss: 0.8307 Batch Accuracy: 64.5241\n",
            "240th Batch Loss: 0.8591 Batch Accuracy: 64.5964\n",
            "260th Batch Loss: 1.1155 Batch Accuracy: 64.5974\n",
            "280th Batch Loss: 0.9569 Batch Accuracy: 64.6708\n",
            "300th Batch Loss: 0.9380 Batch Accuracy: 64.9844\n",
            "320th Batch Loss: 1.0484 Batch Accuracy: 64.9512\n",
            "340th Batch Loss: 1.0366 Batch Accuracy: 64.9540\n",
            "360th Batch Loss: 0.9690 Batch Accuracy: 65.1172\n",
            "380th Batch Loss: 1.2667 Batch Accuracy: 65.0329\n",
            "400th Batch Loss: 1.1853 Batch Accuracy: 64.9570\n",
            "420th Batch Loss: 0.8362 Batch Accuracy: 65.0074\n",
            "440th Batch Loss: 0.7107 Batch Accuracy: 64.9645\n",
            "460th Batch Loss: 1.0640 Batch Accuracy: 65.0000\n",
            "480th Batch Loss: 0.8395 Batch Accuracy: 65.0553\n",
            "500th Batch Loss: 0.9159 Batch Accuracy: 65.0250\n",
            "520th Batch Loss: 1.0436 Batch Accuracy: 64.9910\n",
            "540th Batch Loss: 1.3728 Batch Accuracy: 64.9306\n",
            "560th Batch Loss: 0.8538 Batch Accuracy: 64.8996\n",
            "580th Batch Loss: 0.7059 Batch Accuracy: 65.0700\n",
            "600th Batch Loss: 0.8193 Batch Accuracy: 65.1146\n",
            "620th Batch Loss: 1.1521 Batch Accuracy: 65.1084\n",
            "640th Batch Loss: 0.8795 Batch Accuracy: 65.1758\n",
            "660th Batch Loss: 0.7378 Batch Accuracy: 65.2699\n",
            "Epoch [4/10] Loss: 0.8148 Epoch Accuracy: 65.2755\n",
            "0\n",
            "Validation Accuracy: 73.23%\n",
            "20th Batch Loss: 1.2666 Batch Accuracy: 68.8281\n",
            "40th Batch Loss: 0.9532 Batch Accuracy: 68.0078\n",
            "60th Batch Loss: 1.3855 Batch Accuracy: 67.2396\n",
            "80th Batch Loss: 0.6549 Batch Accuracy: 67.8320\n",
            "100th Batch Loss: 0.9606 Batch Accuracy: 67.9531\n",
            "120th Batch Loss: 0.8975 Batch Accuracy: 67.6693\n",
            "140th Batch Loss: 0.7985 Batch Accuracy: 68.0022\n",
            "160th Batch Loss: 0.8320 Batch Accuracy: 67.8027\n",
            "180th Batch Loss: 0.7891 Batch Accuracy: 67.4826\n",
            "200th Batch Loss: 0.7027 Batch Accuracy: 67.6250\n",
            "220th Batch Loss: 1.1237 Batch Accuracy: 67.6634\n",
            "240th Batch Loss: 0.6650 Batch Accuracy: 67.7930\n",
            "260th Batch Loss: 0.8786 Batch Accuracy: 67.8425\n",
            "280th Batch Loss: 0.9463 Batch Accuracy: 67.9688\n",
            "300th Batch Loss: 0.9787 Batch Accuracy: 67.8906\n",
            "320th Batch Loss: 1.5491 Batch Accuracy: 67.9004\n",
            "340th Batch Loss: 0.6322 Batch Accuracy: 67.8768\n",
            "360th Batch Loss: 0.9598 Batch Accuracy: 67.8906\n",
            "380th Batch Loss: 0.8793 Batch Accuracy: 68.0304\n",
            "400th Batch Loss: 1.2866 Batch Accuracy: 68.0312\n",
            "420th Batch Loss: 0.8684 Batch Accuracy: 68.1771\n",
            "440th Batch Loss: 0.8657 Batch Accuracy: 68.0078\n",
            "460th Batch Loss: 1.3456 Batch Accuracy: 67.9348\n",
            "480th Batch Loss: 1.0757 Batch Accuracy: 67.9525\n",
            "500th Batch Loss: 1.1281 Batch Accuracy: 67.8063\n",
            "520th Batch Loss: 0.9899 Batch Accuracy: 67.6833\n",
            "540th Batch Loss: 1.3430 Batch Accuracy: 67.6910\n",
            "560th Batch Loss: 1.2650 Batch Accuracy: 67.8432\n",
            "580th Batch Loss: 0.8213 Batch Accuracy: 67.7856\n",
            "600th Batch Loss: 0.7622 Batch Accuracy: 67.7839\n",
            "620th Batch Loss: 1.0074 Batch Accuracy: 67.7646\n",
            "640th Batch Loss: 0.6251 Batch Accuracy: 67.9419\n",
            "660th Batch Loss: 1.0737 Batch Accuracy: 67.7794\n",
            "Epoch [5/10] Loss: 1.1254 Epoch Accuracy: 67.8194\n",
            "0\n",
            "Validation Accuracy: 73.52%\n",
            "20th Batch Loss: 0.6987 Batch Accuracy: 70.3125\n",
            "40th Batch Loss: 1.1277 Batch Accuracy: 69.5312\n",
            "60th Batch Loss: 0.8527 Batch Accuracy: 67.8906\n",
            "80th Batch Loss: 0.9215 Batch Accuracy: 67.9492\n",
            "100th Batch Loss: 0.7942 Batch Accuracy: 67.8906\n",
            "120th Batch Loss: 0.7618 Batch Accuracy: 68.4115\n",
            "140th Batch Loss: 0.6042 Batch Accuracy: 68.9397\n",
            "160th Batch Loss: 0.7230 Batch Accuracy: 69.3359\n",
            "180th Batch Loss: 0.7784 Batch Accuracy: 68.9497\n",
            "200th Batch Loss: 0.5670 Batch Accuracy: 68.8594\n",
            "220th Batch Loss: 0.9941 Batch Accuracy: 68.9844\n",
            "240th Batch Loss: 1.0439 Batch Accuracy: 68.9779\n",
            "260th Batch Loss: 0.9231 Batch Accuracy: 68.9303\n",
            "280th Batch Loss: 0.8189 Batch Accuracy: 68.8672\n",
            "300th Batch Loss: 0.6430 Batch Accuracy: 68.8333\n",
            "320th Batch Loss: 0.9345 Batch Accuracy: 68.6523\n",
            "340th Batch Loss: 0.6502 Batch Accuracy: 68.8051\n",
            "360th Batch Loss: 1.2711 Batch Accuracy: 68.9540\n",
            "380th Batch Loss: 1.1343 Batch Accuracy: 68.9803\n",
            "400th Batch Loss: 0.7581 Batch Accuracy: 69.1250\n",
            "420th Batch Loss: 0.7799 Batch Accuracy: 69.0848\n",
            "440th Batch Loss: 0.7716 Batch Accuracy: 69.2010\n",
            "460th Batch Loss: 1.4088 Batch Accuracy: 69.2120\n",
            "480th Batch Loss: 0.9888 Batch Accuracy: 69.3034\n",
            "500th Batch Loss: 0.7974 Batch Accuracy: 69.3750\n",
            "520th Batch Loss: 0.8109 Batch Accuracy: 69.4411\n",
            "540th Batch Loss: 0.7535 Batch Accuracy: 69.4271\n",
            "560th Batch Loss: 0.7873 Batch Accuracy: 69.5285\n",
            "580th Batch Loss: 0.5955 Batch Accuracy: 69.5797\n",
            "600th Batch Loss: 0.9984 Batch Accuracy: 69.5495\n",
            "620th Batch Loss: 0.9335 Batch Accuracy: 69.5262\n",
            "640th Batch Loss: 0.6391 Batch Accuracy: 69.5264\n",
            "660th Batch Loss: 1.2386 Batch Accuracy: 69.4223\n",
            "Epoch [6/10] Loss: 0.6952 Epoch Accuracy: 69.3773\n",
            "0\n",
            "Validation Accuracy: 74.90%\n",
            "20th Batch Loss: 0.9763 Batch Accuracy: 71.4062\n",
            "40th Batch Loss: 0.7381 Batch Accuracy: 70.6641\n",
            "60th Batch Loss: 0.7844 Batch Accuracy: 71.2240\n",
            "80th Batch Loss: 0.8957 Batch Accuracy: 70.2148\n",
            "100th Batch Loss: 0.9691 Batch Accuracy: 70.0625\n",
            "120th Batch Loss: 0.6644 Batch Accuracy: 70.1693\n",
            "140th Batch Loss: 0.6268 Batch Accuracy: 69.9219\n",
            "160th Batch Loss: 1.0503 Batch Accuracy: 69.6484\n",
            "180th Batch Loss: 0.8111 Batch Accuracy: 69.7743\n",
            "200th Batch Loss: 1.2492 Batch Accuracy: 70.0156\n",
            "220th Batch Loss: 0.8777 Batch Accuracy: 69.9858\n",
            "240th Batch Loss: 1.1654 Batch Accuracy: 70.1758\n",
            "260th Batch Loss: 0.9146 Batch Accuracy: 70.1442\n",
            "280th Batch Loss: 1.0187 Batch Accuracy: 70.0279\n",
            "300th Batch Loss: 0.9771 Batch Accuracy: 70.0417\n",
            "320th Batch Loss: 1.0396 Batch Accuracy: 70.0488\n",
            "340th Batch Loss: 0.7085 Batch Accuracy: 70.0597\n",
            "360th Batch Loss: 1.1794 Batch Accuracy: 70.0564\n",
            "380th Batch Loss: 0.6833 Batch Accuracy: 70.0082\n",
            "400th Batch Loss: 0.6651 Batch Accuracy: 70.0938\n",
            "420th Batch Loss: 0.9104 Batch Accuracy: 69.9293\n",
            "440th Batch Loss: 1.2034 Batch Accuracy: 69.8366\n",
            "460th Batch Loss: 0.5533 Batch Accuracy: 69.9694\n",
            "480th Batch Loss: 0.5776 Batch Accuracy: 70.1270\n",
            "500th Batch Loss: 0.5154 Batch Accuracy: 70.2938\n",
            "520th Batch Loss: 0.6813 Batch Accuracy: 70.3065\n",
            "540th Batch Loss: 1.0289 Batch Accuracy: 70.2083\n",
            "560th Batch Loss: 0.9794 Batch Accuracy: 70.1925\n",
            "580th Batch Loss: 1.0938 Batch Accuracy: 70.2802\n",
            "600th Batch Loss: 0.9566 Batch Accuracy: 70.2578\n",
            "620th Batch Loss: 0.5996 Batch Accuracy: 70.2974\n",
            "640th Batch Loss: 0.6040 Batch Accuracy: 70.3442\n",
            "660th Batch Loss: 0.8163 Batch Accuracy: 70.3243\n",
            "Epoch [7/10] Loss: 0.7675 Epoch Accuracy: 70.2708\n",
            "1\n",
            "Validation Accuracy: 74.00%\n",
            "20th Batch Loss: 0.4289 Batch Accuracy: 71.8750\n",
            "40th Batch Loss: 0.8414 Batch Accuracy: 70.0391\n",
            "60th Batch Loss: 0.6087 Batch Accuracy: 71.3802\n",
            "80th Batch Loss: 1.3154 Batch Accuracy: 71.5625\n",
            "100th Batch Loss: 0.8120 Batch Accuracy: 71.9531\n",
            "120th Batch Loss: 0.6319 Batch Accuracy: 72.1615\n",
            "140th Batch Loss: 0.8514 Batch Accuracy: 72.1987\n",
            "160th Batch Loss: 0.6932 Batch Accuracy: 72.4121\n",
            "180th Batch Loss: 0.8016 Batch Accuracy: 72.1615\n",
            "200th Batch Loss: 0.8037 Batch Accuracy: 71.9531\n",
            "220th Batch Loss: 0.7992 Batch Accuracy: 72.1165\n",
            "240th Batch Loss: 0.7595 Batch Accuracy: 72.3047\n",
            "260th Batch Loss: 1.0167 Batch Accuracy: 72.2356\n",
            "280th Batch Loss: 0.7371 Batch Accuracy: 72.0592\n",
            "300th Batch Loss: 0.6601 Batch Accuracy: 71.9010\n",
            "320th Batch Loss: 0.6372 Batch Accuracy: 71.9092\n",
            "340th Batch Loss: 0.6293 Batch Accuracy: 71.7831\n",
            "360th Batch Loss: 0.9358 Batch Accuracy: 71.8273\n",
            "380th Batch Loss: 0.5810 Batch Accuracy: 71.7928\n",
            "400th Batch Loss: 0.7987 Batch Accuracy: 71.8867\n",
            "420th Batch Loss: 0.6696 Batch Accuracy: 72.0647\n",
            "440th Batch Loss: 1.0865 Batch Accuracy: 72.1946\n",
            "460th Batch Loss: 0.5339 Batch Accuracy: 72.2452\n",
            "480th Batch Loss: 0.9405 Batch Accuracy: 72.2331\n",
            "500th Batch Loss: 0.7307 Batch Accuracy: 72.1500\n",
            "520th Batch Loss: 0.8685 Batch Accuracy: 72.1124\n",
            "540th Batch Loss: 0.8597 Batch Accuracy: 72.1586\n",
            "560th Batch Loss: 0.9078 Batch Accuracy: 72.0954\n",
            "580th Batch Loss: 0.6834 Batch Accuracy: 72.0797\n",
            "600th Batch Loss: 0.6109 Batch Accuracy: 72.0469\n",
            "620th Batch Loss: 1.0611 Batch Accuracy: 72.0766\n",
            "640th Batch Loss: 0.7426 Batch Accuracy: 72.1021\n",
            "660th Batch Loss: 0.6574 Batch Accuracy: 72.1993\n",
            "Epoch [8/10] Loss: 0.7964 Epoch Accuracy: 72.1435\n",
            "0\n",
            "Validation Accuracy: 76.06%\n",
            "20th Batch Loss: 0.7627 Batch Accuracy: 72.5000\n",
            "40th Batch Loss: 0.5958 Batch Accuracy: 72.1094\n",
            "60th Batch Loss: 0.7692 Batch Accuracy: 70.7031\n",
            "80th Batch Loss: 0.7632 Batch Accuracy: 71.0547\n",
            "100th Batch Loss: 0.6522 Batch Accuracy: 71.7656\n",
            "120th Batch Loss: 0.9417 Batch Accuracy: 71.7839\n",
            "140th Batch Loss: 0.5895 Batch Accuracy: 72.1205\n",
            "160th Batch Loss: 0.7303 Batch Accuracy: 71.8945\n",
            "180th Batch Loss: 0.7370 Batch Accuracy: 71.9184\n",
            "200th Batch Loss: 0.4987 Batch Accuracy: 72.1094\n",
            "220th Batch Loss: 0.5249 Batch Accuracy: 72.3366\n",
            "240th Batch Loss: 0.4009 Batch Accuracy: 72.2070\n",
            "260th Batch Loss: 0.4865 Batch Accuracy: 72.2776\n",
            "280th Batch Loss: 0.7360 Batch Accuracy: 72.3772\n",
            "300th Batch Loss: 0.9256 Batch Accuracy: 72.5260\n",
            "320th Batch Loss: 0.9416 Batch Accuracy: 72.5244\n",
            "340th Batch Loss: 0.7514 Batch Accuracy: 72.4403\n",
            "360th Batch Loss: 1.0085 Batch Accuracy: 72.4175\n",
            "380th Batch Loss: 0.3764 Batch Accuracy: 72.4959\n",
            "400th Batch Loss: 1.0870 Batch Accuracy: 72.6133\n",
            "420th Batch Loss: 0.5953 Batch Accuracy: 72.5037\n",
            "440th Batch Loss: 1.1610 Batch Accuracy: 72.6740\n",
            "460th Batch Loss: 0.5461 Batch Accuracy: 72.5849\n",
            "480th Batch Loss: 0.7429 Batch Accuracy: 72.5618\n",
            "500th Batch Loss: 0.5793 Batch Accuracy: 72.5031\n",
            "520th Batch Loss: 0.5289 Batch Accuracy: 72.6292\n",
            "540th Batch Loss: 0.7308 Batch Accuracy: 72.5087\n",
            "560th Batch Loss: 0.7417 Batch Accuracy: 72.4526\n",
            "580th Batch Loss: 0.5941 Batch Accuracy: 72.5673\n",
            "600th Batch Loss: 0.4710 Batch Accuracy: 72.6172\n",
            "620th Batch Loss: 0.7029 Batch Accuracy: 72.6386\n",
            "640th Batch Loss: 1.1154 Batch Accuracy: 72.6367\n",
            "660th Batch Loss: 0.8660 Batch Accuracy: 72.5781\n",
            "Epoch [9/10] Loss: 0.5392 Epoch Accuracy: 72.6088\n",
            "0\n",
            "Validation Accuracy: 76.96%\n",
            "20th Batch Loss: 0.9612 Batch Accuracy: 74.4531\n",
            "40th Batch Loss: 0.7098 Batch Accuracy: 74.4922\n",
            "60th Batch Loss: 0.5694 Batch Accuracy: 74.7396\n",
            "80th Batch Loss: 1.2210 Batch Accuracy: 74.7852\n",
            "100th Batch Loss: 1.2254 Batch Accuracy: 74.0156\n",
            "120th Batch Loss: 1.0982 Batch Accuracy: 73.6198\n",
            "140th Batch Loss: 0.9077 Batch Accuracy: 73.4263\n",
            "160th Batch Loss: 0.7240 Batch Accuracy: 73.1445\n",
            "180th Batch Loss: 0.8966 Batch Accuracy: 72.9427\n",
            "200th Batch Loss: 0.9776 Batch Accuracy: 73.1016\n",
            "220th Batch Loss: 0.6452 Batch Accuracy: 73.2599\n",
            "240th Batch Loss: 0.5848 Batch Accuracy: 73.4701\n",
            "260th Batch Loss: 0.5669 Batch Accuracy: 73.4675\n",
            "280th Batch Loss: 0.7634 Batch Accuracy: 73.3761\n",
            "300th Batch Loss: 1.0854 Batch Accuracy: 73.3854\n",
            "320th Batch Loss: 0.9590 Batch Accuracy: 73.3154\n",
            "340th Batch Loss: 0.8188 Batch Accuracy: 73.3272\n",
            "360th Batch Loss: 0.5709 Batch Accuracy: 73.4462\n",
            "380th Batch Loss: 0.8875 Batch Accuracy: 73.3388\n",
            "400th Batch Loss: 1.4201 Batch Accuracy: 73.3359\n",
            "420th Batch Loss: 0.6981 Batch Accuracy: 73.2552\n",
            "440th Batch Loss: 0.7211 Batch Accuracy: 73.1996\n",
            "460th Batch Loss: 0.7978 Batch Accuracy: 73.2099\n",
            "480th Batch Loss: 1.1194 Batch Accuracy: 73.1868\n",
            "500th Batch Loss: 0.6325 Batch Accuracy: 73.1063\n",
            "520th Batch Loss: 0.5473 Batch Accuracy: 73.1310\n",
            "540th Batch Loss: 0.7094 Batch Accuracy: 73.1713\n",
            "560th Batch Loss: 1.1045 Batch Accuracy: 73.1752\n",
            "580th Batch Loss: 0.7283 Batch Accuracy: 73.1277\n",
            "600th Batch Loss: 1.0644 Batch Accuracy: 73.1901\n",
            "620th Batch Loss: 0.7535 Batch Accuracy: 73.2586\n",
            "640th Batch Loss: 0.6533 Batch Accuracy: 73.2666\n",
            "660th Batch Loss: 0.7771 Batch Accuracy: 73.2718\n",
            "Epoch [10/10] Loss: 0.8657 Epoch Accuracy: 73.3056\n",
            "0\n",
            "Validation Accuracy: 77.46%\n",
            "Fold 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 2.0683 Batch Accuracy: 19.0625\n",
            "40th Batch Loss: 1.8711 Batch Accuracy: 23.1641\n",
            "60th Batch Loss: 1.6365 Batch Accuracy: 26.4062\n",
            "80th Batch Loss: 2.0086 Batch Accuracy: 28.3398\n",
            "100th Batch Loss: 1.6532 Batch Accuracy: 29.1406\n",
            "120th Batch Loss: 1.7631 Batch Accuracy: 30.0000\n",
            "140th Batch Loss: 1.4994 Batch Accuracy: 30.7478\n",
            "160th Batch Loss: 1.5679 Batch Accuracy: 31.5527\n",
            "180th Batch Loss: 1.4073 Batch Accuracy: 32.1875\n",
            "200th Batch Loss: 1.5987 Batch Accuracy: 33.0000\n",
            "220th Batch Loss: 1.7278 Batch Accuracy: 33.5156\n",
            "240th Batch Loss: 1.8046 Batch Accuracy: 34.2708\n",
            "260th Batch Loss: 1.7831 Batch Accuracy: 34.8978\n",
            "280th Batch Loss: 1.5956 Batch Accuracy: 35.6920\n",
            "300th Batch Loss: 1.4687 Batch Accuracy: 35.9583\n",
            "320th Batch Loss: 1.5782 Batch Accuracy: 36.5430\n",
            "340th Batch Loss: 1.3880 Batch Accuracy: 37.1278\n",
            "360th Batch Loss: 1.6288 Batch Accuracy: 37.6519\n",
            "380th Batch Loss: 1.3515 Batch Accuracy: 38.1579\n",
            "400th Batch Loss: 1.3508 Batch Accuracy: 38.7031\n",
            "420th Batch Loss: 1.2730 Batch Accuracy: 39.0774\n",
            "440th Batch Loss: 1.5273 Batch Accuracy: 39.4673\n",
            "460th Batch Loss: 1.6163 Batch Accuracy: 39.7792\n",
            "480th Batch Loss: 1.3057 Batch Accuracy: 40.2214\n",
            "500th Batch Loss: 1.7262 Batch Accuracy: 40.6469\n",
            "520th Batch Loss: 1.2669 Batch Accuracy: 41.1208\n",
            "540th Batch Loss: 1.1253 Batch Accuracy: 41.5480\n",
            "560th Batch Loss: 1.2069 Batch Accuracy: 42.0033\n",
            "580th Batch Loss: 1.3456 Batch Accuracy: 42.2548\n",
            "600th Batch Loss: 1.4277 Batch Accuracy: 42.6328\n",
            "620th Batch Loss: 1.5456 Batch Accuracy: 42.9032\n",
            "640th Batch Loss: 1.5867 Batch Accuracy: 43.1372\n",
            "660th Batch Loss: 1.2005 Batch Accuracy: 43.4399\n",
            "Epoch [1/10] Loss: 1.4024 Epoch Accuracy: 43.6528\n",
            "0\n",
            "Validation Accuracy: 57.02%\n",
            "20th Batch Loss: 1.5475 Batch Accuracy: 40.8594\n",
            "40th Batch Loss: 1.4509 Batch Accuracy: 46.2500\n",
            "60th Batch Loss: 1.2785 Batch Accuracy: 47.0052\n",
            "80th Batch Loss: 1.3128 Batch Accuracy: 47.2852\n",
            "100th Batch Loss: 1.8155 Batch Accuracy: 48.0938\n",
            "120th Batch Loss: 1.3189 Batch Accuracy: 48.5938\n",
            "140th Batch Loss: 1.2485 Batch Accuracy: 49.2188\n",
            "160th Batch Loss: 1.2784 Batch Accuracy: 49.6191\n",
            "180th Batch Loss: 1.0857 Batch Accuracy: 50.0955\n",
            "200th Batch Loss: 1.0975 Batch Accuracy: 50.4141\n",
            "220th Batch Loss: 1.1020 Batch Accuracy: 50.9943\n",
            "240th Batch Loss: 1.6274 Batch Accuracy: 51.1328\n",
            "260th Batch Loss: 1.3702 Batch Accuracy: 51.3822\n",
            "280th Batch Loss: 0.9300 Batch Accuracy: 51.7243\n",
            "300th Batch Loss: 0.6985 Batch Accuracy: 52.0885\n",
            "320th Batch Loss: 1.1165 Batch Accuracy: 52.3828\n",
            "340th Batch Loss: 1.3414 Batch Accuracy: 52.4403\n",
            "360th Batch Loss: 0.8038 Batch Accuracy: 52.4609\n",
            "380th Batch Loss: 1.0246 Batch Accuracy: 52.5164\n",
            "400th Batch Loss: 0.8798 Batch Accuracy: 52.6914\n",
            "420th Batch Loss: 1.1826 Batch Accuracy: 52.9315\n",
            "440th Batch Loss: 1.1420 Batch Accuracy: 53.1570\n",
            "460th Batch Loss: 1.0931 Batch Accuracy: 53.4647\n",
            "480th Batch Loss: 1.2226 Batch Accuracy: 53.7826\n",
            "500th Batch Loss: 1.4214 Batch Accuracy: 53.8906\n",
            "520th Batch Loss: 1.2692 Batch Accuracy: 53.8822\n",
            "540th Batch Loss: 1.1080 Batch Accuracy: 54.1580\n",
            "560th Batch Loss: 1.4633 Batch Accuracy: 54.2243\n",
            "580th Batch Loss: 1.1315 Batch Accuracy: 54.3238\n",
            "600th Batch Loss: 1.3483 Batch Accuracy: 54.4401\n",
            "620th Batch Loss: 1.3630 Batch Accuracy: 54.6421\n",
            "640th Batch Loss: 1.2049 Batch Accuracy: 54.7949\n",
            "660th Batch Loss: 1.1702 Batch Accuracy: 54.9574\n",
            "Epoch [2/10] Loss: 1.1601 Epoch Accuracy: 55.0370\n",
            "0\n",
            "Validation Accuracy: 65.15%\n",
            "20th Batch Loss: 1.2533 Batch Accuracy: 60.8594\n",
            "40th Batch Loss: 1.0574 Batch Accuracy: 60.5859\n",
            "60th Batch Loss: 1.1131 Batch Accuracy: 59.7656\n",
            "80th Batch Loss: 1.3813 Batch Accuracy: 59.5117\n",
            "100th Batch Loss: 0.9856 Batch Accuracy: 59.8906\n",
            "120th Batch Loss: 1.1842 Batch Accuracy: 59.7917\n",
            "140th Batch Loss: 1.0351 Batch Accuracy: 59.9665\n",
            "160th Batch Loss: 0.7938 Batch Accuracy: 60.3125\n",
            "180th Batch Loss: 1.4930 Batch Accuracy: 60.1389\n",
            "200th Batch Loss: 0.9089 Batch Accuracy: 60.0000\n",
            "220th Batch Loss: 1.3414 Batch Accuracy: 59.8438\n",
            "240th Batch Loss: 1.2476 Batch Accuracy: 60.0391\n",
            "260th Batch Loss: 1.1022 Batch Accuracy: 60.2704\n",
            "280th Batch Loss: 0.9105 Batch Accuracy: 60.4353\n",
            "300th Batch Loss: 1.0318 Batch Accuracy: 60.4323\n",
            "320th Batch Loss: 0.6911 Batch Accuracy: 60.6738\n",
            "340th Batch Loss: 0.8826 Batch Accuracy: 60.5699\n",
            "360th Batch Loss: 0.8552 Batch Accuracy: 60.6380\n",
            "380th Batch Loss: 1.4499 Batch Accuracy: 60.6373\n",
            "400th Batch Loss: 0.9828 Batch Accuracy: 60.6797\n",
            "420th Batch Loss: 1.1209 Batch Accuracy: 60.7924\n",
            "440th Batch Loss: 1.0185 Batch Accuracy: 60.8203\n",
            "460th Batch Loss: 1.1451 Batch Accuracy: 60.9137\n",
            "480th Batch Loss: 1.4213 Batch Accuracy: 60.9798\n",
            "500th Batch Loss: 0.8662 Batch Accuracy: 61.1344\n",
            "520th Batch Loss: 0.8427 Batch Accuracy: 61.3251\n",
            "540th Batch Loss: 0.8976 Batch Accuracy: 61.3976\n",
            "560th Batch Loss: 0.9687 Batch Accuracy: 61.3979\n",
            "580th Batch Loss: 1.1771 Batch Accuracy: 61.4736\n",
            "600th Batch Loss: 0.9939 Batch Accuracy: 61.4922\n",
            "620th Batch Loss: 0.7608 Batch Accuracy: 61.5776\n",
            "640th Batch Loss: 0.9788 Batch Accuracy: 61.5479\n",
            "660th Batch Loss: 1.0309 Batch Accuracy: 61.6075\n",
            "Epoch [3/10] Loss: 1.1643 Epoch Accuracy: 61.6759\n",
            "0\n",
            "Validation Accuracy: 67.06%\n",
            "20th Batch Loss: 0.9002 Batch Accuracy: 65.3125\n",
            "40th Batch Loss: 0.9781 Batch Accuracy: 65.8203\n",
            "60th Batch Loss: 0.9853 Batch Accuracy: 65.3906\n",
            "80th Batch Loss: 0.7546 Batch Accuracy: 64.8242\n",
            "100th Batch Loss: 0.7882 Batch Accuracy: 64.0000\n",
            "120th Batch Loss: 0.8968 Batch Accuracy: 63.3594\n",
            "140th Batch Loss: 1.3605 Batch Accuracy: 63.4598\n",
            "160th Batch Loss: 0.8313 Batch Accuracy: 63.3887\n",
            "180th Batch Loss: 1.2644 Batch Accuracy: 63.5330\n",
            "200th Batch Loss: 1.1310 Batch Accuracy: 63.8594\n",
            "220th Batch Loss: 0.8059 Batch Accuracy: 63.9418\n",
            "240th Batch Loss: 1.2101 Batch Accuracy: 63.9714\n",
            "260th Batch Loss: 1.0648 Batch Accuracy: 64.0204\n",
            "280th Batch Loss: 0.9959 Batch Accuracy: 64.1239\n",
            "300th Batch Loss: 1.4289 Batch Accuracy: 64.0312\n",
            "320th Batch Loss: 0.8470 Batch Accuracy: 64.0430\n",
            "340th Batch Loss: 0.9767 Batch Accuracy: 64.0533\n",
            "360th Batch Loss: 0.6608 Batch Accuracy: 64.0104\n",
            "380th Batch Loss: 0.8173 Batch Accuracy: 64.0954\n",
            "400th Batch Loss: 0.9100 Batch Accuracy: 64.0938\n",
            "420th Batch Loss: 1.0327 Batch Accuracy: 64.0588\n",
            "440th Batch Loss: 1.2489 Batch Accuracy: 64.0270\n",
            "460th Batch Loss: 1.0784 Batch Accuracy: 64.0115\n",
            "480th Batch Loss: 1.2101 Batch Accuracy: 64.1276\n",
            "500th Batch Loss: 1.2126 Batch Accuracy: 64.0094\n",
            "520th Batch Loss: 1.0145 Batch Accuracy: 63.9844\n",
            "540th Batch Loss: 1.0063 Batch Accuracy: 64.1175\n",
            "560th Batch Loss: 0.8402 Batch Accuracy: 64.2048\n",
            "580th Batch Loss: 1.0610 Batch Accuracy: 64.3292\n",
            "600th Batch Loss: 1.1505 Batch Accuracy: 64.3594\n",
            "620th Batch Loss: 1.0568 Batch Accuracy: 64.2414\n",
            "640th Batch Loss: 0.7689 Batch Accuracy: 64.2554\n",
            "660th Batch Loss: 0.8580 Batch Accuracy: 64.2708\n",
            "Epoch [4/10] Loss: 0.8429 Epoch Accuracy: 64.3310\n",
            "0\n",
            "Validation Accuracy: 72.12%\n",
            "20th Batch Loss: 1.1443 Batch Accuracy: 68.3594\n",
            "40th Batch Loss: 0.6899 Batch Accuracy: 66.9531\n",
            "60th Batch Loss: 0.8232 Batch Accuracy: 66.2500\n",
            "80th Batch Loss: 0.9062 Batch Accuracy: 66.5430\n",
            "100th Batch Loss: 0.5791 Batch Accuracy: 66.6875\n",
            "120th Batch Loss: 1.0675 Batch Accuracy: 66.3411\n",
            "140th Batch Loss: 0.8201 Batch Accuracy: 66.5625\n",
            "160th Batch Loss: 0.9513 Batch Accuracy: 66.5234\n",
            "180th Batch Loss: 0.8205 Batch Accuracy: 66.2760\n",
            "200th Batch Loss: 1.1434 Batch Accuracy: 66.3281\n",
            "220th Batch Loss: 1.0315 Batch Accuracy: 66.1861\n",
            "240th Batch Loss: 1.0993 Batch Accuracy: 66.1198\n",
            "260th Batch Loss: 1.2752 Batch Accuracy: 66.1959\n",
            "280th Batch Loss: 1.1240 Batch Accuracy: 66.1886\n",
            "300th Batch Loss: 1.0167 Batch Accuracy: 66.4271\n",
            "320th Batch Loss: 0.6997 Batch Accuracy: 66.5234\n",
            "340th Batch Loss: 1.3158 Batch Accuracy: 66.5211\n",
            "360th Batch Loss: 0.5920 Batch Accuracy: 66.8273\n",
            "380th Batch Loss: 0.8773 Batch Accuracy: 66.9449\n",
            "400th Batch Loss: 0.8949 Batch Accuracy: 67.0859\n",
            "420th Batch Loss: 1.0211 Batch Accuracy: 67.1763\n",
            "440th Batch Loss: 1.0297 Batch Accuracy: 67.1662\n",
            "460th Batch Loss: 1.0212 Batch Accuracy: 67.1433\n",
            "480th Batch Loss: 0.9693 Batch Accuracy: 67.1354\n",
            "500th Batch Loss: 0.9927 Batch Accuracy: 67.0531\n",
            "520th Batch Loss: 0.7665 Batch Accuracy: 67.0883\n",
            "540th Batch Loss: 0.6471 Batch Accuracy: 67.1383\n",
            "560th Batch Loss: 1.1589 Batch Accuracy: 67.2210\n",
            "580th Batch Loss: 1.0883 Batch Accuracy: 67.2198\n",
            "600th Batch Loss: 1.3547 Batch Accuracy: 67.1823\n",
            "620th Batch Loss: 0.9513 Batch Accuracy: 67.0842\n",
            "640th Batch Loss: 0.8363 Batch Accuracy: 67.1191\n",
            "660th Batch Loss: 0.7883 Batch Accuracy: 67.2135\n",
            "Epoch [5/10] Loss: 1.1257 Epoch Accuracy: 67.2153\n",
            "1\n",
            "Validation Accuracy: 69.56%\n",
            "20th Batch Loss: 0.6992 Batch Accuracy: 68.6719\n",
            "40th Batch Loss: 0.6369 Batch Accuracy: 69.6875\n",
            "60th Batch Loss: 0.9191 Batch Accuracy: 68.3594\n",
            "80th Batch Loss: 1.1134 Batch Accuracy: 68.9844\n",
            "100th Batch Loss: 0.7891 Batch Accuracy: 69.0625\n",
            "120th Batch Loss: 1.0036 Batch Accuracy: 68.5547\n",
            "140th Batch Loss: 0.9535 Batch Accuracy: 68.7388\n",
            "160th Batch Loss: 1.1461 Batch Accuracy: 68.7207\n",
            "180th Batch Loss: 0.9828 Batch Accuracy: 68.3420\n",
            "200th Batch Loss: 1.0423 Batch Accuracy: 68.2656\n",
            "220th Batch Loss: 1.0746 Batch Accuracy: 67.9474\n",
            "240th Batch Loss: 0.7722 Batch Accuracy: 68.1380\n",
            "260th Batch Loss: 1.0326 Batch Accuracy: 68.1130\n",
            "280th Batch Loss: 1.0147 Batch Accuracy: 68.0469\n",
            "300th Batch Loss: 0.5817 Batch Accuracy: 68.2344\n",
            "320th Batch Loss: 0.6025 Batch Accuracy: 68.4180\n",
            "340th Batch Loss: 0.6465 Batch Accuracy: 68.4007\n",
            "360th Batch Loss: 0.8238 Batch Accuracy: 68.3767\n",
            "380th Batch Loss: 0.9553 Batch Accuracy: 68.2525\n",
            "400th Batch Loss: 0.8109 Batch Accuracy: 68.3125\n",
            "420th Batch Loss: 0.8947 Batch Accuracy: 68.2664\n",
            "440th Batch Loss: 1.0405 Batch Accuracy: 68.1108\n",
            "460th Batch Loss: 1.0598 Batch Accuracy: 68.2507\n",
            "480th Batch Loss: 0.7048 Batch Accuracy: 68.3789\n",
            "500th Batch Loss: 0.9213 Batch Accuracy: 68.3094\n",
            "520th Batch Loss: 1.1460 Batch Accuracy: 68.2602\n",
            "540th Batch Loss: 0.7503 Batch Accuracy: 68.2292\n",
            "560th Batch Loss: 0.9683 Batch Accuracy: 68.2645\n",
            "580th Batch Loss: 0.6999 Batch Accuracy: 68.4591\n",
            "600th Batch Loss: 0.8959 Batch Accuracy: 68.5208\n",
            "620th Batch Loss: 0.5343 Batch Accuracy: 68.6391\n",
            "640th Batch Loss: 1.0994 Batch Accuracy: 68.6572\n",
            "660th Batch Loss: 0.6312 Batch Accuracy: 68.6648\n",
            "Epoch [6/10] Loss: 1.0560 Epoch Accuracy: 68.5764\n",
            "0\n",
            "Validation Accuracy: 73.46%\n",
            "20th Batch Loss: 0.7004 Batch Accuracy: 73.3594\n",
            "40th Batch Loss: 0.9260 Batch Accuracy: 72.7734\n",
            "60th Batch Loss: 0.9847 Batch Accuracy: 71.1979\n",
            "80th Batch Loss: 0.8181 Batch Accuracy: 70.0781\n",
            "100th Batch Loss: 0.7627 Batch Accuracy: 70.2812\n",
            "120th Batch Loss: 0.6925 Batch Accuracy: 70.0521\n",
            "140th Batch Loss: 0.9222 Batch Accuracy: 69.9888\n",
            "160th Batch Loss: 1.1195 Batch Accuracy: 69.9414\n",
            "180th Batch Loss: 0.6808 Batch Accuracy: 69.9653\n",
            "200th Batch Loss: 1.0325 Batch Accuracy: 70.0234\n",
            "220th Batch Loss: 0.6537 Batch Accuracy: 70.1420\n",
            "240th Batch Loss: 0.9402 Batch Accuracy: 70.0065\n",
            "260th Batch Loss: 0.9496 Batch Accuracy: 69.8317\n",
            "280th Batch Loss: 0.7753 Batch Accuracy: 69.8493\n",
            "300th Batch Loss: 0.6647 Batch Accuracy: 69.9688\n",
            "320th Batch Loss: 0.7879 Batch Accuracy: 69.9756\n",
            "340th Batch Loss: 0.6838 Batch Accuracy: 70.1471\n",
            "360th Batch Loss: 0.8015 Batch Accuracy: 70.0304\n",
            "380th Batch Loss: 0.8958 Batch Accuracy: 70.0863\n",
            "400th Batch Loss: 0.8059 Batch Accuracy: 70.1484\n",
            "420th Batch Loss: 0.5946 Batch Accuracy: 70.0000\n",
            "440th Batch Loss: 0.6802 Batch Accuracy: 70.0355\n",
            "460th Batch Loss: 0.8742 Batch Accuracy: 69.9762\n",
            "480th Batch Loss: 0.5075 Batch Accuracy: 70.0358\n",
            "500th Batch Loss: 1.0933 Batch Accuracy: 69.9938\n",
            "520th Batch Loss: 0.9505 Batch Accuracy: 70.1352\n",
            "540th Batch Loss: 0.6830 Batch Accuracy: 70.2054\n",
            "560th Batch Loss: 0.7966 Batch Accuracy: 70.2372\n",
            "580th Batch Loss: 0.8800 Batch Accuracy: 70.1562\n",
            "600th Batch Loss: 0.9942 Batch Accuracy: 70.2083\n",
            "620th Batch Loss: 0.5839 Batch Accuracy: 70.1663\n",
            "640th Batch Loss: 0.8607 Batch Accuracy: 70.2026\n",
            "660th Batch Loss: 0.8198 Batch Accuracy: 70.1728\n",
            "Epoch [7/10] Loss: 0.9308 Epoch Accuracy: 70.1921\n",
            "0\n",
            "Validation Accuracy: 74.73%\n",
            "20th Batch Loss: 0.8750 Batch Accuracy: 74.4531\n",
            "40th Batch Loss: 0.6533 Batch Accuracy: 73.1250\n",
            "60th Batch Loss: 1.0320 Batch Accuracy: 71.4323\n",
            "80th Batch Loss: 0.8237 Batch Accuracy: 71.0352\n",
            "100th Batch Loss: 0.7417 Batch Accuracy: 71.1719\n",
            "120th Batch Loss: 0.8529 Batch Accuracy: 71.3672\n",
            "140th Batch Loss: 0.8145 Batch Accuracy: 70.8036\n",
            "160th Batch Loss: 0.7437 Batch Accuracy: 70.9082\n",
            "180th Batch Loss: 0.6609 Batch Accuracy: 70.7986\n",
            "200th Batch Loss: 0.8023 Batch Accuracy: 70.8359\n",
            "220th Batch Loss: 0.7313 Batch Accuracy: 70.6889\n",
            "240th Batch Loss: 0.8937 Batch Accuracy: 70.7747\n",
            "260th Batch Loss: 0.6854 Batch Accuracy: 70.8954\n",
            "280th Batch Loss: 0.7031 Batch Accuracy: 71.0379\n",
            "300th Batch Loss: 0.9004 Batch Accuracy: 71.0521\n",
            "320th Batch Loss: 0.8420 Batch Accuracy: 70.9668\n",
            "340th Batch Loss: 1.1666 Batch Accuracy: 70.9099\n",
            "360th Batch Loss: 1.1283 Batch Accuracy: 70.9766\n",
            "380th Batch Loss: 0.6794 Batch Accuracy: 71.1719\n",
            "400th Batch Loss: 0.9745 Batch Accuracy: 71.1719\n",
            "420th Batch Loss: 0.9891 Batch Accuracy: 70.9598\n",
            "440th Batch Loss: 0.8156 Batch Accuracy: 71.0334\n",
            "460th Batch Loss: 0.5904 Batch Accuracy: 71.1243\n",
            "480th Batch Loss: 0.8129 Batch Accuracy: 71.1784\n",
            "500th Batch Loss: 0.6663 Batch Accuracy: 71.0938\n",
            "520th Batch Loss: 0.8814 Batch Accuracy: 71.0727\n",
            "540th Batch Loss: 0.8923 Batch Accuracy: 71.0330\n",
            "560th Batch Loss: 0.9098 Batch Accuracy: 71.0714\n",
            "580th Batch Loss: 0.6497 Batch Accuracy: 71.1153\n",
            "600th Batch Loss: 0.5126 Batch Accuracy: 71.0938\n",
            "620th Batch Loss: 0.6665 Batch Accuracy: 71.0887\n",
            "640th Batch Loss: 0.6237 Batch Accuracy: 71.0229\n",
            "660th Batch Loss: 0.6755 Batch Accuracy: 71.0559\n",
            "Epoch [8/10] Loss: 0.9495 Epoch Accuracy: 71.0417\n",
            "0\n",
            "Validation Accuracy: 75.96%\n",
            "20th Batch Loss: 0.7696 Batch Accuracy: 75.4688\n",
            "40th Batch Loss: 0.8616 Batch Accuracy: 73.6719\n",
            "60th Batch Loss: 0.8637 Batch Accuracy: 73.3333\n",
            "80th Batch Loss: 1.0230 Batch Accuracy: 72.5195\n",
            "100th Batch Loss: 0.6661 Batch Accuracy: 72.8281\n",
            "120th Batch Loss: 0.7057 Batch Accuracy: 72.8125\n",
            "140th Batch Loss: 0.6268 Batch Accuracy: 73.0804\n",
            "160th Batch Loss: 1.1406 Batch Accuracy: 73.1152\n",
            "180th Batch Loss: 0.9274 Batch Accuracy: 72.9948\n",
            "200th Batch Loss: 0.8456 Batch Accuracy: 73.0000\n",
            "220th Batch Loss: 1.1610 Batch Accuracy: 73.0540\n",
            "240th Batch Loss: 0.8136 Batch Accuracy: 72.9753\n",
            "260th Batch Loss: 0.9847 Batch Accuracy: 72.7825\n",
            "280th Batch Loss: 0.5983 Batch Accuracy: 72.8404\n",
            "300th Batch Loss: 0.8455 Batch Accuracy: 73.0781\n",
            "320th Batch Loss: 1.0212 Batch Accuracy: 73.4229\n",
            "340th Batch Loss: 0.9858 Batch Accuracy: 73.3778\n",
            "360th Batch Loss: 1.0271 Batch Accuracy: 73.1944\n",
            "380th Batch Loss: 0.8700 Batch Accuracy: 73.0469\n",
            "400th Batch Loss: 0.9927 Batch Accuracy: 73.2383\n",
            "420th Batch Loss: 0.7513 Batch Accuracy: 73.2552\n",
            "440th Batch Loss: 0.6016 Batch Accuracy: 73.2706\n",
            "460th Batch Loss: 0.9324 Batch Accuracy: 73.1760\n",
            "480th Batch Loss: 0.7602 Batch Accuracy: 73.1934\n",
            "500th Batch Loss: 0.5317 Batch Accuracy: 73.2188\n",
            "520th Batch Loss: 0.8134 Batch Accuracy: 73.1701\n",
            "540th Batch Loss: 0.7182 Batch Accuracy: 73.1250\n",
            "560th Batch Loss: 0.8888 Batch Accuracy: 72.9660\n",
            "580th Batch Loss: 0.7844 Batch Accuracy: 72.9607\n",
            "600th Batch Loss: 0.5221 Batch Accuracy: 72.9557\n",
            "620th Batch Loss: 1.0708 Batch Accuracy: 72.8402\n",
            "640th Batch Loss: 0.8621 Batch Accuracy: 72.8369\n",
            "660th Batch Loss: 0.7594 Batch Accuracy: 72.7296\n",
            "Epoch [9/10] Loss: 0.7746 Epoch Accuracy: 72.6806\n",
            "0\n",
            "Validation Accuracy: 77.40%\n",
            "20th Batch Loss: 0.8522 Batch Accuracy: 75.6250\n",
            "40th Batch Loss: 0.7649 Batch Accuracy: 73.5938\n",
            "60th Batch Loss: 0.4550 Batch Accuracy: 73.7760\n",
            "80th Batch Loss: 0.4529 Batch Accuracy: 73.4961\n",
            "100th Batch Loss: 0.4370 Batch Accuracy: 73.6562\n",
            "120th Batch Loss: 1.0895 Batch Accuracy: 73.4505\n",
            "140th Batch Loss: 0.6308 Batch Accuracy: 73.6719\n",
            "160th Batch Loss: 0.6105 Batch Accuracy: 73.6035\n",
            "180th Batch Loss: 0.7976 Batch Accuracy: 73.1510\n",
            "200th Batch Loss: 0.5319 Batch Accuracy: 73.0156\n",
            "220th Batch Loss: 0.7977 Batch Accuracy: 72.9688\n",
            "240th Batch Loss: 0.9250 Batch Accuracy: 73.1055\n",
            "260th Batch Loss: 0.6795 Batch Accuracy: 73.1671\n",
            "280th Batch Loss: 0.5288 Batch Accuracy: 73.0301\n",
            "300th Batch Loss: 0.8509 Batch Accuracy: 73.2083\n",
            "320th Batch Loss: 0.7509 Batch Accuracy: 73.3057\n",
            "340th Batch Loss: 0.4324 Batch Accuracy: 73.4513\n",
            "360th Batch Loss: 0.6210 Batch Accuracy: 73.3984\n",
            "380th Batch Loss: 0.5832 Batch Accuracy: 73.3758\n",
            "400th Batch Loss: 1.2040 Batch Accuracy: 73.3945\n",
            "420th Batch Loss: 0.8207 Batch Accuracy: 73.5193\n",
            "440th Batch Loss: 0.8702 Batch Accuracy: 73.5050\n",
            "460th Batch Loss: 0.8436 Batch Accuracy: 73.4069\n",
            "480th Batch Loss: 0.8394 Batch Accuracy: 73.2617\n",
            "500th Batch Loss: 0.7511 Batch Accuracy: 73.3094\n",
            "520th Batch Loss: 0.6629 Batch Accuracy: 73.4766\n",
            "540th Batch Loss: 1.0178 Batch Accuracy: 73.5301\n",
            "560th Batch Loss: 1.1101 Batch Accuracy: 73.4515\n",
            "580th Batch Loss: 0.8092 Batch Accuracy: 73.3297\n",
            "600th Batch Loss: 0.5403 Batch Accuracy: 73.4635\n",
            "620th Batch Loss: 0.8255 Batch Accuracy: 73.5232\n",
            "640th Batch Loss: 0.7205 Batch Accuracy: 73.5400\n",
            "660th Batch Loss: 0.6536 Batch Accuracy: 73.5417\n",
            "Epoch [10/10] Loss: 0.6520 Epoch Accuracy: 73.5995\n",
            "0\n",
            "Validation Accuracy: 77.90%\n",
            "Fold 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 2.0907 Batch Accuracy: 17.7344\n",
            "40th Batch Loss: 1.7628 Batch Accuracy: 23.0078\n",
            "60th Batch Loss: 1.8672 Batch Accuracy: 25.2604\n",
            "80th Batch Loss: 1.9181 Batch Accuracy: 27.2461\n",
            "100th Batch Loss: 1.4736 Batch Accuracy: 28.9531\n",
            "120th Batch Loss: 1.6071 Batch Accuracy: 30.4948\n",
            "140th Batch Loss: 1.6745 Batch Accuracy: 31.3281\n",
            "160th Batch Loss: 1.6399 Batch Accuracy: 32.0508\n",
            "180th Batch Loss: 1.6267 Batch Accuracy: 32.7431\n",
            "200th Batch Loss: 1.4883 Batch Accuracy: 33.6719\n",
            "220th Batch Loss: 1.4068 Batch Accuracy: 34.2259\n",
            "240th Batch Loss: 1.6058 Batch Accuracy: 34.6680\n",
            "260th Batch Loss: 1.5557 Batch Accuracy: 35.0841\n",
            "280th Batch Loss: 1.3192 Batch Accuracy: 35.9096\n",
            "300th Batch Loss: 1.1511 Batch Accuracy: 36.5833\n",
            "320th Batch Loss: 1.6782 Batch Accuracy: 36.9531\n",
            "340th Batch Loss: 1.2365 Batch Accuracy: 37.4632\n",
            "360th Batch Loss: 1.3304 Batch Accuracy: 37.9774\n",
            "380th Batch Loss: 1.2172 Batch Accuracy: 38.3594\n",
            "400th Batch Loss: 1.3866 Batch Accuracy: 38.8281\n",
            "420th Batch Loss: 1.1826 Batch Accuracy: 39.2485\n",
            "440th Batch Loss: 1.6167 Batch Accuracy: 39.7550\n",
            "460th Batch Loss: 1.2279 Batch Accuracy: 40.2955\n",
            "480th Batch Loss: 1.2057 Batch Accuracy: 40.7357\n",
            "500th Batch Loss: 1.3162 Batch Accuracy: 41.0375\n",
            "520th Batch Loss: 1.2491 Batch Accuracy: 41.4213\n",
            "540th Batch Loss: 1.3044 Batch Accuracy: 41.8113\n",
            "560th Batch Loss: 1.2906 Batch Accuracy: 42.0312\n",
            "580th Batch Loss: 1.4086 Batch Accuracy: 42.2872\n",
            "600th Batch Loss: 1.2407 Batch Accuracy: 42.6510\n",
            "620th Batch Loss: 1.3398 Batch Accuracy: 42.9284\n",
            "640th Batch Loss: 1.1890 Batch Accuracy: 43.2910\n",
            "660th Batch Loss: 1.5290 Batch Accuracy: 43.5464\n",
            "Epoch [1/10] Loss: 1.2548 Epoch Accuracy: 43.7801\n",
            "0\n",
            "Validation Accuracy: 59.21%\n",
            "20th Batch Loss: 1.6205 Batch Accuracy: 43.5156\n",
            "40th Batch Loss: 1.4164 Batch Accuracy: 44.3359\n",
            "60th Batch Loss: 1.6577 Batch Accuracy: 44.5833\n",
            "80th Batch Loss: 1.0300 Batch Accuracy: 45.9570\n",
            "100th Batch Loss: 1.3561 Batch Accuracy: 46.7188\n",
            "120th Batch Loss: 1.4807 Batch Accuracy: 47.9036\n",
            "140th Batch Loss: 1.1987 Batch Accuracy: 48.0469\n",
            "160th Batch Loss: 1.0602 Batch Accuracy: 48.6719\n",
            "180th Batch Loss: 1.3300 Batch Accuracy: 49.2708\n",
            "200th Batch Loss: 1.1230 Batch Accuracy: 49.6250\n",
            "220th Batch Loss: 1.3479 Batch Accuracy: 50.1705\n",
            "240th Batch Loss: 0.9773 Batch Accuracy: 50.8854\n",
            "260th Batch Loss: 1.0637 Batch Accuracy: 51.2380\n",
            "280th Batch Loss: 1.4144 Batch Accuracy: 51.4732\n",
            "300th Batch Loss: 1.1098 Batch Accuracy: 51.6510\n",
            "320th Batch Loss: 1.2335 Batch Accuracy: 51.7920\n",
            "340th Batch Loss: 1.0259 Batch Accuracy: 52.1186\n",
            "360th Batch Loss: 1.2920 Batch Accuracy: 52.4175\n",
            "380th Batch Loss: 1.2703 Batch Accuracy: 52.6275\n",
            "400th Batch Loss: 1.1581 Batch Accuracy: 52.9570\n",
            "420th Batch Loss: 0.9751 Batch Accuracy: 53.1957\n",
            "440th Batch Loss: 1.4357 Batch Accuracy: 53.1001\n",
            "460th Batch Loss: 1.0538 Batch Accuracy: 53.1997\n",
            "480th Batch Loss: 1.5285 Batch Accuracy: 53.3854\n",
            "500th Batch Loss: 1.3672 Batch Accuracy: 53.5500\n",
            "520th Batch Loss: 0.9869 Batch Accuracy: 53.9453\n",
            "540th Batch Loss: 1.3589 Batch Accuracy: 54.1146\n",
            "560th Batch Loss: 1.0758 Batch Accuracy: 54.3694\n",
            "580th Batch Loss: 1.1109 Batch Accuracy: 54.5366\n",
            "600th Batch Loss: 1.0075 Batch Accuracy: 54.6510\n",
            "620th Batch Loss: 0.9765 Batch Accuracy: 54.7833\n",
            "640th Batch Loss: 1.1411 Batch Accuracy: 54.8682\n",
            "660th Batch Loss: 1.4461 Batch Accuracy: 54.9148\n",
            "Epoch [2/10] Loss: 0.8431 Epoch Accuracy: 55.0116\n",
            "0\n",
            "Validation Accuracy: 67.46%\n",
            "20th Batch Loss: 1.1392 Batch Accuracy: 63.0469\n",
            "40th Batch Loss: 1.0455 Batch Accuracy: 62.7344\n",
            "60th Batch Loss: 1.1023 Batch Accuracy: 61.4583\n",
            "80th Batch Loss: 1.2788 Batch Accuracy: 60.9766\n",
            "100th Batch Loss: 1.2854 Batch Accuracy: 61.0000\n",
            "120th Batch Loss: 0.9673 Batch Accuracy: 61.0547\n",
            "140th Batch Loss: 1.1038 Batch Accuracy: 61.2388\n",
            "160th Batch Loss: 1.2585 Batch Accuracy: 61.0840\n",
            "180th Batch Loss: 0.7934 Batch Accuracy: 61.1198\n",
            "200th Batch Loss: 1.0411 Batch Accuracy: 61.1484\n",
            "220th Batch Loss: 1.2007 Batch Accuracy: 60.9943\n",
            "240th Batch Loss: 1.1244 Batch Accuracy: 60.8789\n",
            "260th Batch Loss: 0.9448 Batch Accuracy: 60.8654\n",
            "280th Batch Loss: 1.1773 Batch Accuracy: 60.7812\n",
            "300th Batch Loss: 1.3744 Batch Accuracy: 60.7552\n",
            "320th Batch Loss: 1.4362 Batch Accuracy: 60.7861\n",
            "340th Batch Loss: 0.8003 Batch Accuracy: 60.9972\n",
            "360th Batch Loss: 1.1021 Batch Accuracy: 61.1632\n",
            "380th Batch Loss: 0.6924 Batch Accuracy: 61.3857\n",
            "400th Batch Loss: 1.0219 Batch Accuracy: 61.3047\n",
            "420th Batch Loss: 0.9447 Batch Accuracy: 61.4993\n",
            "440th Batch Loss: 0.9560 Batch Accuracy: 61.5199\n",
            "460th Batch Loss: 1.1146 Batch Accuracy: 61.6372\n",
            "480th Batch Loss: 0.9112 Batch Accuracy: 61.8001\n",
            "500th Batch Loss: 1.4953 Batch Accuracy: 61.7969\n",
            "520th Batch Loss: 0.7727 Batch Accuracy: 61.7849\n",
            "540th Batch Loss: 1.0762 Batch Accuracy: 61.8866\n",
            "560th Batch Loss: 1.1883 Batch Accuracy: 61.9169\n",
            "580th Batch Loss: 1.1720 Batch Accuracy: 61.9639\n",
            "600th Batch Loss: 1.0665 Batch Accuracy: 62.0547\n",
            "620th Batch Loss: 0.7083 Batch Accuracy: 62.0817\n",
            "640th Batch Loss: 0.9893 Batch Accuracy: 62.1436\n",
            "660th Batch Loss: 1.4682 Batch Accuracy: 62.1591\n",
            "Epoch [3/10] Loss: 1.1385 Epoch Accuracy: 62.1134\n",
            "0\n",
            "Validation Accuracy: 70.35%\n",
            "20th Batch Loss: 1.1509 Batch Accuracy: 61.7188\n",
            "40th Batch Loss: 0.7539 Batch Accuracy: 63.1250\n",
            "60th Batch Loss: 0.9405 Batch Accuracy: 64.1927\n",
            "80th Batch Loss: 1.2230 Batch Accuracy: 63.4766\n",
            "100th Batch Loss: 1.1529 Batch Accuracy: 63.6094\n",
            "120th Batch Loss: 0.9006 Batch Accuracy: 63.3724\n",
            "140th Batch Loss: 0.7865 Batch Accuracy: 63.7277\n",
            "160th Batch Loss: 1.3687 Batch Accuracy: 64.0723\n",
            "180th Batch Loss: 0.9983 Batch Accuracy: 64.0104\n",
            "200th Batch Loss: 0.9176 Batch Accuracy: 64.2891\n",
            "220th Batch Loss: 1.2096 Batch Accuracy: 64.0270\n",
            "240th Batch Loss: 0.7393 Batch Accuracy: 63.9909\n",
            "260th Batch Loss: 1.0879 Batch Accuracy: 64.1166\n",
            "280th Batch Loss: 0.9373 Batch Accuracy: 64.2132\n",
            "300th Batch Loss: 1.5402 Batch Accuracy: 64.0469\n",
            "320th Batch Loss: 1.2599 Batch Accuracy: 64.2334\n",
            "340th Batch Loss: 0.7443 Batch Accuracy: 64.2969\n",
            "360th Batch Loss: 0.9052 Batch Accuracy: 64.2274\n",
            "380th Batch Loss: 0.9961 Batch Accuracy: 64.2928\n",
            "400th Batch Loss: 0.9748 Batch Accuracy: 64.1484\n",
            "420th Batch Loss: 1.0841 Batch Accuracy: 64.1890\n",
            "440th Batch Loss: 0.7791 Batch Accuracy: 64.2614\n",
            "460th Batch Loss: 0.9876 Batch Accuracy: 64.1746\n",
            "480th Batch Loss: 1.1317 Batch Accuracy: 64.2025\n",
            "500th Batch Loss: 0.8257 Batch Accuracy: 64.3125\n",
            "520th Batch Loss: 1.6622 Batch Accuracy: 64.3510\n",
            "540th Batch Loss: 1.1290 Batch Accuracy: 64.3258\n",
            "560th Batch Loss: 0.8724 Batch Accuracy: 64.4448\n",
            "580th Batch Loss: 0.8667 Batch Accuracy: 64.6282\n",
            "600th Batch Loss: 0.7433 Batch Accuracy: 64.7214\n",
            "620th Batch Loss: 1.0656 Batch Accuracy: 64.8110\n",
            "640th Batch Loss: 1.0418 Batch Accuracy: 64.9243\n",
            "660th Batch Loss: 0.9934 Batch Accuracy: 64.9077\n",
            "Epoch [4/10] Loss: 1.0096 Epoch Accuracy: 64.9884\n",
            "0\n",
            "Validation Accuracy: 71.77%\n",
            "20th Batch Loss: 0.5452 Batch Accuracy: 66.0156\n",
            "40th Batch Loss: 0.8428 Batch Accuracy: 65.7812\n",
            "60th Batch Loss: 1.2530 Batch Accuracy: 65.8073\n",
            "80th Batch Loss: 0.8536 Batch Accuracy: 65.6836\n",
            "100th Batch Loss: 0.9216 Batch Accuracy: 65.5781\n",
            "120th Batch Loss: 0.8554 Batch Accuracy: 65.5859\n",
            "140th Batch Loss: 0.8844 Batch Accuracy: 65.3348\n",
            "160th Batch Loss: 1.1628 Batch Accuracy: 65.3516\n",
            "180th Batch Loss: 1.2191 Batch Accuracy: 65.5642\n",
            "200th Batch Loss: 1.0113 Batch Accuracy: 65.7109\n",
            "220th Batch Loss: 0.8001 Batch Accuracy: 65.8665\n",
            "240th Batch Loss: 1.2916 Batch Accuracy: 65.8203\n",
            "260th Batch Loss: 1.2354 Batch Accuracy: 65.8474\n",
            "280th Batch Loss: 0.9152 Batch Accuracy: 65.9821\n",
            "300th Batch Loss: 1.0323 Batch Accuracy: 66.0729\n",
            "320th Batch Loss: 1.0177 Batch Accuracy: 66.1865\n",
            "340th Batch Loss: 0.7947 Batch Accuracy: 66.3557\n",
            "360th Batch Loss: 0.8665 Batch Accuracy: 66.4974\n",
            "380th Batch Loss: 0.8711 Batch Accuracy: 66.5173\n",
            "400th Batch Loss: 1.0644 Batch Accuracy: 66.6914\n",
            "420th Batch Loss: 0.9289 Batch Accuracy: 66.6741\n",
            "440th Batch Loss: 0.9474 Batch Accuracy: 66.7294\n",
            "460th Batch Loss: 0.9694 Batch Accuracy: 66.7221\n",
            "480th Batch Loss: 1.0264 Batch Accuracy: 66.7188\n",
            "500th Batch Loss: 0.6277 Batch Accuracy: 66.7500\n",
            "520th Batch Loss: 0.7404 Batch Accuracy: 66.7248\n",
            "540th Batch Loss: 1.0530 Batch Accuracy: 66.7853\n",
            "560th Batch Loss: 0.8940 Batch Accuracy: 66.8136\n",
            "580th Batch Loss: 0.8060 Batch Accuracy: 66.9019\n",
            "600th Batch Loss: 0.6884 Batch Accuracy: 66.9896\n",
            "620th Batch Loss: 1.1051 Batch Accuracy: 66.9834\n",
            "640th Batch Loss: 1.0122 Batch Accuracy: 67.0459\n",
            "660th Batch Loss: 1.1118 Batch Accuracy: 67.0715\n",
            "Epoch [5/10] Loss: 0.6752 Epoch Accuracy: 67.0509\n",
            "0\n",
            "Validation Accuracy: 71.88%\n",
            "20th Batch Loss: 0.8759 Batch Accuracy: 65.0000\n",
            "40th Batch Loss: 1.1617 Batch Accuracy: 66.6797\n",
            "60th Batch Loss: 1.0978 Batch Accuracy: 67.4219\n",
            "80th Batch Loss: 0.7602 Batch Accuracy: 68.4766\n",
            "100th Batch Loss: 0.8992 Batch Accuracy: 68.5781\n",
            "120th Batch Loss: 0.8105 Batch Accuracy: 67.8906\n",
            "140th Batch Loss: 0.8543 Batch Accuracy: 68.2143\n",
            "160th Batch Loss: 0.7616 Batch Accuracy: 68.5645\n",
            "180th Batch Loss: 0.5982 Batch Accuracy: 68.6285\n",
            "200th Batch Loss: 0.6248 Batch Accuracy: 68.8906\n",
            "220th Batch Loss: 0.8418 Batch Accuracy: 68.9347\n",
            "240th Batch Loss: 0.8165 Batch Accuracy: 68.7956\n",
            "260th Batch Loss: 0.5536 Batch Accuracy: 68.8221\n",
            "280th Batch Loss: 0.8517 Batch Accuracy: 68.5156\n",
            "300th Batch Loss: 0.7325 Batch Accuracy: 68.5677\n",
            "320th Batch Loss: 1.3442 Batch Accuracy: 68.5938\n",
            "340th Batch Loss: 0.7749 Batch Accuracy: 68.4835\n",
            "360th Batch Loss: 0.6188 Batch Accuracy: 68.6589\n",
            "380th Batch Loss: 0.7708 Batch Accuracy: 68.7048\n",
            "400th Batch Loss: 0.8124 Batch Accuracy: 68.5938\n",
            "420th Batch Loss: 0.8328 Batch Accuracy: 68.5454\n",
            "440th Batch Loss: 1.2514 Batch Accuracy: 68.6257\n",
            "460th Batch Loss: 0.7868 Batch Accuracy: 68.7160\n",
            "480th Batch Loss: 0.5825 Batch Accuracy: 68.8216\n",
            "500th Batch Loss: 0.9412 Batch Accuracy: 68.9469\n",
            "520th Batch Loss: 1.1435 Batch Accuracy: 69.0986\n",
            "540th Batch Loss: 0.6216 Batch Accuracy: 69.2159\n",
            "560th Batch Loss: 0.8667 Batch Accuracy: 69.2578\n",
            "580th Batch Loss: 0.7098 Batch Accuracy: 69.3077\n",
            "600th Batch Loss: 1.0492 Batch Accuracy: 69.2760\n",
            "620th Batch Loss: 0.9197 Batch Accuracy: 69.3649\n",
            "640th Batch Loss: 1.1143 Batch Accuracy: 69.2822\n",
            "660th Batch Loss: 1.0794 Batch Accuracy: 69.3324\n",
            "Epoch [6/10] Loss: 1.0308 Epoch Accuracy: 69.3565\n",
            "0\n",
            "Validation Accuracy: 75.83%\n",
            "20th Batch Loss: 0.7381 Batch Accuracy: 70.6250\n",
            "40th Batch Loss: 0.8169 Batch Accuracy: 70.3516\n",
            "60th Batch Loss: 0.8976 Batch Accuracy: 71.3542\n",
            "80th Batch Loss: 1.1652 Batch Accuracy: 71.4648\n",
            "100th Batch Loss: 1.0409 Batch Accuracy: 71.2344\n",
            "120th Batch Loss: 0.7446 Batch Accuracy: 70.9245\n",
            "140th Batch Loss: 1.2422 Batch Accuracy: 70.6473\n",
            "160th Batch Loss: 0.9308 Batch Accuracy: 70.4883\n",
            "180th Batch Loss: 0.8815 Batch Accuracy: 70.3646\n",
            "200th Batch Loss: 0.5893 Batch Accuracy: 70.6016\n",
            "220th Batch Loss: 1.0271 Batch Accuracy: 70.7955\n",
            "240th Batch Loss: 0.7206 Batch Accuracy: 70.7487\n",
            "260th Batch Loss: 1.1678 Batch Accuracy: 70.5649\n",
            "280th Batch Loss: 1.0425 Batch Accuracy: 70.6641\n",
            "300th Batch Loss: 0.5201 Batch Accuracy: 70.4896\n",
            "320th Batch Loss: 0.6450 Batch Accuracy: 70.4541\n",
            "340th Batch Loss: 0.7347 Batch Accuracy: 70.5423\n",
            "360th Batch Loss: 0.6683 Batch Accuracy: 70.7292\n",
            "380th Batch Loss: 0.7453 Batch Accuracy: 70.6003\n",
            "400th Batch Loss: 0.9300 Batch Accuracy: 70.5664\n",
            "420th Batch Loss: 0.7041 Batch Accuracy: 70.4799\n",
            "440th Batch Loss: 0.9154 Batch Accuracy: 70.4297\n",
            "460th Batch Loss: 0.8871 Batch Accuracy: 70.4721\n",
            "480th Batch Loss: 0.8373 Batch Accuracy: 70.4883\n",
            "500th Batch Loss: 0.4464 Batch Accuracy: 70.5531\n",
            "520th Batch Loss: 0.9663 Batch Accuracy: 70.5799\n",
            "540th Batch Loss: 0.9267 Batch Accuracy: 70.5527\n",
            "560th Batch Loss: 0.7543 Batch Accuracy: 70.5999\n",
            "580th Batch Loss: 0.9287 Batch Accuracy: 70.6115\n",
            "600th Batch Loss: 0.5984 Batch Accuracy: 70.5729\n",
            "620th Batch Loss: 0.9884 Batch Accuracy: 70.5494\n",
            "640th Batch Loss: 0.7036 Batch Accuracy: 70.5347\n",
            "660th Batch Loss: 0.5607 Batch Accuracy: 70.5824\n",
            "Epoch [7/10] Loss: 1.1073 Epoch Accuracy: 70.5833\n",
            "1\n",
            "Validation Accuracy: 75.79%\n",
            "20th Batch Loss: 0.5970 Batch Accuracy: 70.3125\n",
            "40th Batch Loss: 1.3339 Batch Accuracy: 71.8359\n",
            "60th Batch Loss: 0.4994 Batch Accuracy: 72.4479\n",
            "80th Batch Loss: 0.9022 Batch Accuracy: 71.6602\n",
            "100th Batch Loss: 0.9223 Batch Accuracy: 72.2188\n",
            "120th Batch Loss: 0.9613 Batch Accuracy: 71.8620\n",
            "140th Batch Loss: 0.8355 Batch Accuracy: 71.5737\n",
            "160th Batch Loss: 0.7798 Batch Accuracy: 71.2695\n",
            "180th Batch Loss: 0.9154 Batch Accuracy: 70.9115\n",
            "200th Batch Loss: 0.6506 Batch Accuracy: 71.0625\n",
            "220th Batch Loss: 0.5084 Batch Accuracy: 70.9588\n",
            "240th Batch Loss: 0.7512 Batch Accuracy: 70.7552\n",
            "260th Batch Loss: 1.1477 Batch Accuracy: 70.9075\n",
            "280th Batch Loss: 0.5719 Batch Accuracy: 71.0212\n",
            "300th Batch Loss: 0.6113 Batch Accuracy: 71.0573\n",
            "320th Batch Loss: 0.6535 Batch Accuracy: 71.0596\n",
            "340th Batch Loss: 0.8241 Batch Accuracy: 71.2500\n",
            "360th Batch Loss: 0.6967 Batch Accuracy: 71.1458\n",
            "380th Batch Loss: 0.8591 Batch Accuracy: 71.2459\n",
            "400th Batch Loss: 0.7360 Batch Accuracy: 71.2930\n",
            "420th Batch Loss: 0.8165 Batch Accuracy: 71.2426\n",
            "440th Batch Loss: 0.8776 Batch Accuracy: 71.3672\n",
            "460th Batch Loss: 0.7943 Batch Accuracy: 71.2636\n",
            "480th Batch Loss: 0.7829 Batch Accuracy: 71.2240\n",
            "500th Batch Loss: 0.9058 Batch Accuracy: 71.1781\n",
            "520th Batch Loss: 0.4940 Batch Accuracy: 71.1779\n",
            "540th Batch Loss: 0.6546 Batch Accuracy: 71.1777\n",
            "560th Batch Loss: 0.7592 Batch Accuracy: 71.1719\n",
            "580th Batch Loss: 1.2609 Batch Accuracy: 71.1261\n",
            "600th Batch Loss: 0.9683 Batch Accuracy: 71.0599\n",
            "620th Batch Loss: 0.6267 Batch Accuracy: 71.0862\n",
            "640th Batch Loss: 0.5587 Batch Accuracy: 71.0400\n",
            "660th Batch Loss: 1.0302 Batch Accuracy: 71.1056\n",
            "Epoch [8/10] Loss: 1.0597 Epoch Accuracy: 71.1019\n",
            "0\n",
            "Validation Accuracy: 76.92%\n",
            "20th Batch Loss: 0.5896 Batch Accuracy: 73.2812\n",
            "40th Batch Loss: 1.0066 Batch Accuracy: 71.9141\n",
            "60th Batch Loss: 0.7382 Batch Accuracy: 71.9010\n",
            "80th Batch Loss: 0.7078 Batch Accuracy: 72.4023\n",
            "100th Batch Loss: 0.5536 Batch Accuracy: 72.4531\n",
            "120th Batch Loss: 0.8131 Batch Accuracy: 72.9948\n",
            "140th Batch Loss: 1.0940 Batch Accuracy: 72.4665\n",
            "160th Batch Loss: 1.1613 Batch Accuracy: 72.5488\n",
            "180th Batch Loss: 0.8840 Batch Accuracy: 72.6128\n",
            "200th Batch Loss: 0.4374 Batch Accuracy: 72.6875\n",
            "220th Batch Loss: 0.5999 Batch Accuracy: 72.4148\n",
            "240th Batch Loss: 0.5909 Batch Accuracy: 72.6172\n",
            "260th Batch Loss: 0.6977 Batch Accuracy: 72.6382\n",
            "280th Batch Loss: 0.5218 Batch Accuracy: 72.5446\n",
            "300th Batch Loss: 1.0505 Batch Accuracy: 72.6458\n",
            "320th Batch Loss: 0.9786 Batch Accuracy: 72.4805\n",
            "340th Batch Loss: 0.9426 Batch Accuracy: 72.3162\n",
            "360th Batch Loss: 0.6908 Batch Accuracy: 72.2222\n",
            "380th Batch Loss: 0.5262 Batch Accuracy: 72.3766\n",
            "400th Batch Loss: 0.4635 Batch Accuracy: 72.5117\n",
            "420th Batch Loss: 1.1204 Batch Accuracy: 72.4219\n",
            "440th Batch Loss: 0.8262 Batch Accuracy: 72.4751\n",
            "460th Batch Loss: 0.7341 Batch Accuracy: 72.4355\n",
            "480th Batch Loss: 0.7230 Batch Accuracy: 72.3926\n",
            "500th Batch Loss: 1.0705 Batch Accuracy: 72.3531\n",
            "520th Batch Loss: 0.9190 Batch Accuracy: 72.3287\n",
            "540th Batch Loss: 1.0890 Batch Accuracy: 72.3119\n",
            "560th Batch Loss: 0.9875 Batch Accuracy: 72.3689\n",
            "580th Batch Loss: 0.6682 Batch Accuracy: 72.4030\n",
            "600th Batch Loss: 0.6379 Batch Accuracy: 72.4115\n",
            "620th Batch Loss: 0.7532 Batch Accuracy: 72.3438\n",
            "640th Batch Loss: 0.7111 Batch Accuracy: 72.3462\n",
            "660th Batch Loss: 0.8886 Batch Accuracy: 72.3982\n",
            "Epoch [9/10] Loss: 0.4856 Epoch Accuracy: 72.4259\n",
            "0\n",
            "Validation Accuracy: 78.15%\n",
            "20th Batch Loss: 0.9262 Batch Accuracy: 76.1719\n",
            "40th Batch Loss: 0.8798 Batch Accuracy: 73.8281\n",
            "60th Batch Loss: 0.8622 Batch Accuracy: 73.7240\n",
            "80th Batch Loss: 0.8861 Batch Accuracy: 73.3984\n",
            "100th Batch Loss: 0.6437 Batch Accuracy: 72.8906\n",
            "120th Batch Loss: 0.8083 Batch Accuracy: 73.3333\n",
            "140th Batch Loss: 0.5158 Batch Accuracy: 73.2924\n",
            "160th Batch Loss: 0.8787 Batch Accuracy: 73.3594\n",
            "180th Batch Loss: 0.5104 Batch Accuracy: 73.3594\n",
            "200th Batch Loss: 0.4666 Batch Accuracy: 73.1797\n",
            "220th Batch Loss: 0.8392 Batch Accuracy: 73.0043\n",
            "240th Batch Loss: 0.6509 Batch Accuracy: 72.8711\n",
            "260th Batch Loss: 0.9412 Batch Accuracy: 72.9026\n",
            "280th Batch Loss: 0.9410 Batch Accuracy: 72.9464\n",
            "300th Batch Loss: 0.7714 Batch Accuracy: 73.2188\n",
            "320th Batch Loss: 0.6179 Batch Accuracy: 73.1299\n",
            "340th Batch Loss: 0.8129 Batch Accuracy: 73.1847\n",
            "360th Batch Loss: 0.5748 Batch Accuracy: 73.1944\n",
            "380th Batch Loss: 0.7835 Batch Accuracy: 73.2278\n",
            "400th Batch Loss: 0.7006 Batch Accuracy: 73.1875\n",
            "420th Batch Loss: 0.7257 Batch Accuracy: 73.1362\n",
            "440th Batch Loss: 1.0810 Batch Accuracy: 73.1392\n",
            "460th Batch Loss: 0.7961 Batch Accuracy: 73.1080\n",
            "480th Batch Loss: 0.7734 Batch Accuracy: 73.1250\n",
            "500th Batch Loss: 0.9810 Batch Accuracy: 73.1375\n",
            "520th Batch Loss: 0.6806 Batch Accuracy: 73.1370\n",
            "540th Batch Loss: 0.5394 Batch Accuracy: 73.1800\n",
            "560th Batch Loss: 0.8993 Batch Accuracy: 73.2896\n",
            "580th Batch Loss: 0.5908 Batch Accuracy: 73.2839\n",
            "600th Batch Loss: 0.5274 Batch Accuracy: 73.3047\n",
            "620th Batch Loss: 0.8589 Batch Accuracy: 73.2434\n",
            "640th Batch Loss: 1.0279 Batch Accuracy: 73.1836\n",
            "660th Batch Loss: 0.9936 Batch Accuracy: 73.1795\n",
            "Epoch [10/10] Loss: 0.6118 Epoch Accuracy: 73.1782\n",
            "1\n",
            "Validation Accuracy: 77.60%\n",
            "Fold 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 2.0818 Batch Accuracy: 19.9219\n",
            "40th Batch Loss: 1.7757 Batch Accuracy: 21.9922\n",
            "60th Batch Loss: 1.6884 Batch Accuracy: 24.8438\n",
            "80th Batch Loss: 1.7717 Batch Accuracy: 26.9336\n",
            "100th Batch Loss: 1.7133 Batch Accuracy: 27.7969\n",
            "120th Batch Loss: 1.5870 Batch Accuracy: 28.9323\n",
            "140th Batch Loss: 1.4024 Batch Accuracy: 30.0223\n",
            "160th Batch Loss: 1.6976 Batch Accuracy: 30.1562\n",
            "180th Batch Loss: 1.6227 Batch Accuracy: 31.0243\n",
            "200th Batch Loss: 1.3720 Batch Accuracy: 32.0391\n",
            "220th Batch Loss: 1.8053 Batch Accuracy: 32.5923\n",
            "240th Batch Loss: 1.7242 Batch Accuracy: 33.3789\n",
            "260th Batch Loss: 1.8640 Batch Accuracy: 33.9663\n",
            "280th Batch Loss: 1.3022 Batch Accuracy: 34.7935\n",
            "300th Batch Loss: 1.7208 Batch Accuracy: 35.4844\n",
            "320th Batch Loss: 1.2388 Batch Accuracy: 36.0742\n",
            "340th Batch Loss: 1.3369 Batch Accuracy: 36.3465\n",
            "360th Batch Loss: 1.2111 Batch Accuracy: 36.8533\n",
            "380th Batch Loss: 1.5502 Batch Accuracy: 37.2903\n",
            "400th Batch Loss: 1.5669 Batch Accuracy: 37.8477\n",
            "420th Batch Loss: 1.3933 Batch Accuracy: 38.1510\n",
            "440th Batch Loss: 1.2251 Batch Accuracy: 38.6648\n",
            "460th Batch Loss: 1.3711 Batch Accuracy: 39.0829\n",
            "480th Batch Loss: 1.4445 Batch Accuracy: 39.4531\n",
            "500th Batch Loss: 1.3150 Batch Accuracy: 39.8156\n",
            "520th Batch Loss: 1.3519 Batch Accuracy: 40.2945\n",
            "540th Batch Loss: 1.2729 Batch Accuracy: 40.7176\n",
            "560th Batch Loss: 1.4160 Batch Accuracy: 41.0686\n",
            "580th Batch Loss: 1.2629 Batch Accuracy: 41.4036\n",
            "600th Batch Loss: 1.6688 Batch Accuracy: 41.6901\n",
            "620th Batch Loss: 1.3727 Batch Accuracy: 42.0413\n",
            "640th Batch Loss: 1.2569 Batch Accuracy: 42.3706\n",
            "660th Batch Loss: 1.4927 Batch Accuracy: 42.7202\n",
            "Epoch [1/10] Loss: 1.1904 Epoch Accuracy: 42.9375\n",
            "0\n",
            "Validation Accuracy: 54.77%\n",
            "20th Batch Loss: 1.5767 Batch Accuracy: 42.5000\n",
            "40th Batch Loss: 1.1358 Batch Accuracy: 46.1328\n",
            "60th Batch Loss: 1.1706 Batch Accuracy: 46.8490\n",
            "80th Batch Loss: 1.3541 Batch Accuracy: 48.0078\n",
            "100th Batch Loss: 1.3128 Batch Accuracy: 48.8438\n",
            "120th Batch Loss: 1.1562 Batch Accuracy: 49.9349\n",
            "140th Batch Loss: 1.4528 Batch Accuracy: 50.4799\n",
            "160th Batch Loss: 1.5463 Batch Accuracy: 50.5859\n",
            "180th Batch Loss: 1.3999 Batch Accuracy: 50.9722\n",
            "200th Batch Loss: 1.7377 Batch Accuracy: 51.1250\n",
            "220th Batch Loss: 1.2582 Batch Accuracy: 51.2713\n",
            "240th Batch Loss: 1.2653 Batch Accuracy: 51.6276\n",
            "260th Batch Loss: 1.0868 Batch Accuracy: 51.8269\n",
            "280th Batch Loss: 1.2800 Batch Accuracy: 52.0089\n",
            "300th Batch Loss: 1.6527 Batch Accuracy: 52.2656\n",
            "320th Batch Loss: 0.9846 Batch Accuracy: 52.3242\n",
            "340th Batch Loss: 1.0891 Batch Accuracy: 52.5322\n",
            "360th Batch Loss: 1.3117 Batch Accuracy: 52.6823\n",
            "380th Batch Loss: 1.0713 Batch Accuracy: 52.9770\n",
            "400th Batch Loss: 1.3159 Batch Accuracy: 53.1758\n",
            "420th Batch Loss: 1.0460 Batch Accuracy: 53.4524\n",
            "440th Batch Loss: 0.9069 Batch Accuracy: 53.5724\n",
            "460th Batch Loss: 0.9594 Batch Accuracy: 53.7840\n",
            "480th Batch Loss: 1.2369 Batch Accuracy: 53.7793\n",
            "500th Batch Loss: 1.0518 Batch Accuracy: 53.9813\n",
            "520th Batch Loss: 0.8676 Batch Accuracy: 54.0775\n",
            "540th Batch Loss: 1.2555 Batch Accuracy: 54.2535\n",
            "560th Batch Loss: 0.8870 Batch Accuracy: 54.4782\n",
            "580th Batch Loss: 1.3756 Batch Accuracy: 54.6740\n",
            "600th Batch Loss: 0.9136 Batch Accuracy: 54.9219\n",
            "620th Batch Loss: 1.2884 Batch Accuracy: 55.0932\n",
            "640th Batch Loss: 0.8771 Batch Accuracy: 55.2905\n",
            "660th Batch Loss: 1.0715 Batch Accuracy: 55.4545\n",
            "Epoch [2/10] Loss: 1.2576 Epoch Accuracy: 55.4560\n",
            "0\n",
            "Validation Accuracy: 64.92%\n",
            "20th Batch Loss: 1.3635 Batch Accuracy: 59.2969\n",
            "40th Batch Loss: 1.0672 Batch Accuracy: 57.8906\n",
            "60th Batch Loss: 1.0260 Batch Accuracy: 59.5052\n",
            "80th Batch Loss: 1.2372 Batch Accuracy: 59.7852\n",
            "100th Batch Loss: 1.1190 Batch Accuracy: 59.0625\n",
            "120th Batch Loss: 1.5139 Batch Accuracy: 59.4531\n",
            "140th Batch Loss: 0.7941 Batch Accuracy: 59.7879\n",
            "160th Batch Loss: 1.0452 Batch Accuracy: 59.5801\n",
            "180th Batch Loss: 1.4331 Batch Accuracy: 59.8438\n",
            "200th Batch Loss: 1.5101 Batch Accuracy: 59.7891\n",
            "220th Batch Loss: 0.8882 Batch Accuracy: 60.0284\n",
            "240th Batch Loss: 0.8438 Batch Accuracy: 60.1432\n",
            "260th Batch Loss: 0.8813 Batch Accuracy: 60.2825\n",
            "280th Batch Loss: 1.0129 Batch Accuracy: 60.2734\n",
            "300th Batch Loss: 0.9244 Batch Accuracy: 60.4740\n",
            "320th Batch Loss: 1.1959 Batch Accuracy: 60.6445\n",
            "340th Batch Loss: 0.6395 Batch Accuracy: 60.7675\n",
            "360th Batch Loss: 1.1546 Batch Accuracy: 60.7726\n",
            "380th Batch Loss: 1.2052 Batch Accuracy: 60.7977\n",
            "400th Batch Loss: 0.9527 Batch Accuracy: 60.8750\n",
            "420th Batch Loss: 1.2270 Batch Accuracy: 60.9747\n",
            "440th Batch Loss: 1.4783 Batch Accuracy: 60.9801\n",
            "460th Batch Loss: 1.1104 Batch Accuracy: 60.9749\n",
            "480th Batch Loss: 0.9246 Batch Accuracy: 61.0254\n",
            "500th Batch Loss: 1.1196 Batch Accuracy: 61.1375\n",
            "520th Batch Loss: 0.7483 Batch Accuracy: 61.2079\n",
            "540th Batch Loss: 0.9893 Batch Accuracy: 61.2818\n",
            "560th Batch Loss: 0.9374 Batch Accuracy: 61.2388\n",
            "580th Batch Loss: 1.2828 Batch Accuracy: 61.2069\n",
            "600th Batch Loss: 1.3196 Batch Accuracy: 61.2240\n",
            "620th Batch Loss: 0.9560 Batch Accuracy: 61.3382\n",
            "640th Batch Loss: 0.9239 Batch Accuracy: 61.3647\n",
            "660th Batch Loss: 1.0123 Batch Accuracy: 61.4347\n",
            "Epoch [3/10] Loss: 1.1184 Epoch Accuracy: 61.5440\n",
            "0\n",
            "Validation Accuracy: 68.85%\n",
            "20th Batch Loss: 0.9142 Batch Accuracy: 65.5469\n",
            "40th Batch Loss: 0.8623 Batch Accuracy: 64.6875\n",
            "60th Batch Loss: 0.7598 Batch Accuracy: 63.6979\n",
            "80th Batch Loss: 1.0676 Batch Accuracy: 63.1055\n",
            "100th Batch Loss: 0.9938 Batch Accuracy: 63.1719\n",
            "120th Batch Loss: 1.0005 Batch Accuracy: 63.1120\n",
            "140th Batch Loss: 0.6490 Batch Accuracy: 63.5379\n",
            "160th Batch Loss: 1.0634 Batch Accuracy: 63.5059\n",
            "180th Batch Loss: 0.7522 Batch Accuracy: 63.4809\n",
            "200th Batch Loss: 1.1518 Batch Accuracy: 63.7500\n",
            "220th Batch Loss: 1.1032 Batch Accuracy: 63.6861\n",
            "240th Batch Loss: 1.1083 Batch Accuracy: 63.7565\n",
            "260th Batch Loss: 1.1399 Batch Accuracy: 63.5697\n",
            "280th Batch Loss: 1.0750 Batch Accuracy: 63.6217\n",
            "300th Batch Loss: 1.0211 Batch Accuracy: 63.5625\n",
            "320th Batch Loss: 1.1416 Batch Accuracy: 63.5889\n",
            "340th Batch Loss: 0.6052 Batch Accuracy: 63.7592\n",
            "360th Batch Loss: 0.7221 Batch Accuracy: 63.8759\n",
            "380th Batch Loss: 1.1808 Batch Accuracy: 63.8569\n",
            "400th Batch Loss: 1.0397 Batch Accuracy: 63.8008\n",
            "420th Batch Loss: 0.7667 Batch Accuracy: 63.8504\n",
            "440th Batch Loss: 0.7475 Batch Accuracy: 64.0341\n",
            "460th Batch Loss: 1.0287 Batch Accuracy: 64.1474\n",
            "480th Batch Loss: 0.8842 Batch Accuracy: 64.2090\n",
            "500th Batch Loss: 0.6182 Batch Accuracy: 64.3625\n",
            "520th Batch Loss: 1.1910 Batch Accuracy: 64.2969\n",
            "540th Batch Loss: 1.0336 Batch Accuracy: 64.3605\n",
            "560th Batch Loss: 0.7644 Batch Accuracy: 64.4671\n",
            "580th Batch Loss: 0.7416 Batch Accuracy: 64.5151\n",
            "600th Batch Loss: 0.8444 Batch Accuracy: 64.5156\n",
            "620th Batch Loss: 0.8247 Batch Accuracy: 64.5212\n",
            "640th Batch Loss: 0.7642 Batch Accuracy: 64.5996\n",
            "660th Batch Loss: 1.0180 Batch Accuracy: 64.6070\n",
            "Epoch [4/10] Loss: 0.9438 Epoch Accuracy: 64.6204\n",
            "0\n",
            "Validation Accuracy: 71.25%\n",
            "20th Batch Loss: 0.7894 Batch Accuracy: 64.6094\n",
            "40th Batch Loss: 0.6678 Batch Accuracy: 66.4453\n",
            "60th Batch Loss: 1.2366 Batch Accuracy: 65.8594\n",
            "80th Batch Loss: 0.9310 Batch Accuracy: 66.7188\n",
            "100th Batch Loss: 0.7949 Batch Accuracy: 66.5938\n",
            "120th Batch Loss: 0.8675 Batch Accuracy: 66.5365\n",
            "140th Batch Loss: 0.6840 Batch Accuracy: 66.4732\n",
            "160th Batch Loss: 0.9912 Batch Accuracy: 66.3477\n",
            "180th Batch Loss: 0.8642 Batch Accuracy: 66.3021\n",
            "200th Batch Loss: 0.8524 Batch Accuracy: 66.4141\n",
            "220th Batch Loss: 1.2721 Batch Accuracy: 66.3139\n",
            "240th Batch Loss: 1.1941 Batch Accuracy: 66.1914\n",
            "260th Batch Loss: 0.6875 Batch Accuracy: 66.3702\n",
            "280th Batch Loss: 1.2176 Batch Accuracy: 66.2054\n",
            "300th Batch Loss: 0.8796 Batch Accuracy: 66.1875\n",
            "320th Batch Loss: 0.7266 Batch Accuracy: 66.4502\n",
            "340th Batch Loss: 0.8290 Batch Accuracy: 66.3971\n",
            "360th Batch Loss: 0.7837 Batch Accuracy: 66.3194\n",
            "380th Batch Loss: 0.9375 Batch Accuracy: 66.3240\n",
            "400th Batch Loss: 0.8508 Batch Accuracy: 66.3203\n",
            "420th Batch Loss: 0.8242 Batch Accuracy: 66.4509\n",
            "440th Batch Loss: 1.0316 Batch Accuracy: 66.4915\n",
            "460th Batch Loss: 0.7655 Batch Accuracy: 66.5999\n",
            "480th Batch Loss: 0.8153 Batch Accuracy: 66.5658\n",
            "500th Batch Loss: 0.6304 Batch Accuracy: 66.6063\n",
            "520th Batch Loss: 1.0944 Batch Accuracy: 66.6256\n",
            "540th Batch Loss: 0.8233 Batch Accuracy: 66.7882\n",
            "560th Batch Loss: 0.6851 Batch Accuracy: 66.8359\n",
            "580th Batch Loss: 0.9496 Batch Accuracy: 66.8346\n",
            "600th Batch Loss: 0.8035 Batch Accuracy: 66.8672\n",
            "620th Batch Loss: 0.7546 Batch Accuracy: 66.9254\n",
            "640th Batch Loss: 0.8445 Batch Accuracy: 66.9238\n",
            "660th Batch Loss: 0.9778 Batch Accuracy: 66.8253\n",
            "Epoch [5/10] Loss: 0.4430 Epoch Accuracy: 66.7778\n",
            "0\n",
            "Validation Accuracy: 71.98%\n",
            "20th Batch Loss: 0.8119 Batch Accuracy: 70.4688\n",
            "40th Batch Loss: 1.1910 Batch Accuracy: 68.1250\n",
            "60th Batch Loss: 0.8943 Batch Accuracy: 68.0729\n",
            "80th Batch Loss: 1.2002 Batch Accuracy: 68.2812\n",
            "100th Batch Loss: 1.0301 Batch Accuracy: 68.7969\n",
            "120th Batch Loss: 1.0584 Batch Accuracy: 68.9714\n",
            "140th Batch Loss: 1.0533 Batch Accuracy: 68.9397\n",
            "160th Batch Loss: 1.0029 Batch Accuracy: 68.5156\n",
            "180th Batch Loss: 0.5723 Batch Accuracy: 68.6111\n",
            "200th Batch Loss: 0.6689 Batch Accuracy: 68.7344\n",
            "220th Batch Loss: 0.6032 Batch Accuracy: 68.8423\n",
            "240th Batch Loss: 0.9886 Batch Accuracy: 68.8607\n",
            "260th Batch Loss: 0.7243 Batch Accuracy: 68.9303\n",
            "280th Batch Loss: 0.8376 Batch Accuracy: 69.0569\n",
            "300th Batch Loss: 0.8529 Batch Accuracy: 69.0521\n",
            "320th Batch Loss: 1.0095 Batch Accuracy: 68.9209\n",
            "340th Batch Loss: 1.1268 Batch Accuracy: 68.8419\n",
            "360th Batch Loss: 0.5169 Batch Accuracy: 68.7326\n",
            "380th Batch Loss: 0.8247 Batch Accuracy: 68.5156\n",
            "400th Batch Loss: 0.5277 Batch Accuracy: 68.6406\n",
            "420th Batch Loss: 0.9519 Batch Accuracy: 68.5007\n",
            "440th Batch Loss: 1.0783 Batch Accuracy: 68.5831\n",
            "460th Batch Loss: 0.9823 Batch Accuracy: 68.5734\n",
            "480th Batch Loss: 0.6036 Batch Accuracy: 68.5970\n",
            "500th Batch Loss: 1.2135 Batch Accuracy: 68.6688\n",
            "520th Batch Loss: 0.7912 Batch Accuracy: 68.6508\n",
            "540th Batch Loss: 0.7192 Batch Accuracy: 68.5995\n",
            "560th Batch Loss: 0.6090 Batch Accuracy: 68.6272\n",
            "580th Batch Loss: 0.6067 Batch Accuracy: 68.6395\n",
            "600th Batch Loss: 0.9245 Batch Accuracy: 68.5521\n",
            "620th Batch Loss: 0.6597 Batch Accuracy: 68.5786\n",
            "640th Batch Loss: 0.5686 Batch Accuracy: 68.5962\n",
            "660th Batch Loss: 1.0858 Batch Accuracy: 68.6813\n",
            "Epoch [6/10] Loss: 0.6159 Epoch Accuracy: 68.7176\n",
            "0\n",
            "Validation Accuracy: 74.31%\n",
            "20th Batch Loss: 0.6526 Batch Accuracy: 69.5312\n",
            "40th Batch Loss: 1.0854 Batch Accuracy: 70.0781\n",
            "60th Batch Loss: 0.9539 Batch Accuracy: 70.7292\n",
            "80th Batch Loss: 0.6569 Batch Accuracy: 70.5859\n",
            "100th Batch Loss: 0.8458 Batch Accuracy: 71.0469\n",
            "120th Batch Loss: 0.5568 Batch Accuracy: 70.7161\n",
            "140th Batch Loss: 0.5969 Batch Accuracy: 70.5804\n",
            "160th Batch Loss: 0.7352 Batch Accuracy: 70.8789\n",
            "180th Batch Loss: 0.6356 Batch Accuracy: 70.6597\n",
            "200th Batch Loss: 1.0444 Batch Accuracy: 70.3828\n",
            "220th Batch Loss: 1.2855 Batch Accuracy: 69.9574\n",
            "240th Batch Loss: 1.1189 Batch Accuracy: 69.9349\n",
            "260th Batch Loss: 1.3647 Batch Accuracy: 69.9099\n",
            "280th Batch Loss: 0.7430 Batch Accuracy: 69.9777\n",
            "300th Batch Loss: 0.5062 Batch Accuracy: 70.1875\n",
            "320th Batch Loss: 0.6304 Batch Accuracy: 70.1855\n",
            "340th Batch Loss: 1.0148 Batch Accuracy: 70.1195\n",
            "360th Batch Loss: 0.8839 Batch Accuracy: 70.3299\n",
            "380th Batch Loss: 1.1691 Batch Accuracy: 70.3166\n",
            "400th Batch Loss: 0.6506 Batch Accuracy: 70.1562\n",
            "420th Batch Loss: 0.6830 Batch Accuracy: 70.3051\n",
            "440th Batch Loss: 0.9292 Batch Accuracy: 70.2060\n",
            "460th Batch Loss: 0.7059 Batch Accuracy: 70.2853\n",
            "480th Batch Loss: 0.6962 Batch Accuracy: 70.3516\n",
            "500th Batch Loss: 0.7230 Batch Accuracy: 70.2781\n",
            "520th Batch Loss: 0.7455 Batch Accuracy: 70.3666\n",
            "540th Batch Loss: 0.8446 Batch Accuracy: 70.2488\n",
            "560th Batch Loss: 0.7398 Batch Accuracy: 70.3153\n",
            "580th Batch Loss: 0.6493 Batch Accuracy: 70.4014\n",
            "600th Batch Loss: 0.6280 Batch Accuracy: 70.3984\n",
            "620th Batch Loss: 1.4229 Batch Accuracy: 70.2646\n",
            "640th Batch Loss: 0.7917 Batch Accuracy: 70.2246\n",
            "660th Batch Loss: 0.8813 Batch Accuracy: 70.2675\n",
            "Epoch [7/10] Loss: 0.8215 Epoch Accuracy: 70.2153\n",
            "0\n",
            "Validation Accuracy: 76.04%\n",
            "20th Batch Loss: 0.6096 Batch Accuracy: 72.3438\n",
            "40th Batch Loss: 0.8978 Batch Accuracy: 72.1484\n",
            "60th Batch Loss: 0.5548 Batch Accuracy: 73.6719\n",
            "80th Batch Loss: 0.7421 Batch Accuracy: 73.0859\n",
            "100th Batch Loss: 0.8995 Batch Accuracy: 72.8594\n",
            "120th Batch Loss: 1.1576 Batch Accuracy: 72.2656\n",
            "140th Batch Loss: 0.6655 Batch Accuracy: 72.3103\n",
            "160th Batch Loss: 0.4197 Batch Accuracy: 72.3730\n",
            "180th Batch Loss: 0.7411 Batch Accuracy: 72.0833\n",
            "200th Batch Loss: 0.7120 Batch Accuracy: 72.0234\n",
            "220th Batch Loss: 0.7209 Batch Accuracy: 72.0170\n",
            "240th Batch Loss: 0.8372 Batch Accuracy: 71.8164\n",
            "260th Batch Loss: 0.7742 Batch Accuracy: 71.7909\n",
            "280th Batch Loss: 0.9837 Batch Accuracy: 71.9252\n",
            "300th Batch Loss: 0.7983 Batch Accuracy: 71.7188\n",
            "320th Batch Loss: 1.0046 Batch Accuracy: 71.7236\n",
            "340th Batch Loss: 0.9104 Batch Accuracy: 71.8153\n",
            "360th Batch Loss: 0.7810 Batch Accuracy: 72.0399\n",
            "380th Batch Loss: 0.7281 Batch Accuracy: 71.9901\n",
            "400th Batch Loss: 0.5267 Batch Accuracy: 71.8320\n",
            "420th Batch Loss: 0.6557 Batch Accuracy: 71.7485\n",
            "440th Batch Loss: 0.8113 Batch Accuracy: 71.8146\n",
            "460th Batch Loss: 0.8264 Batch Accuracy: 71.7289\n",
            "480th Batch Loss: 0.7523 Batch Accuracy: 71.6960\n",
            "500th Batch Loss: 0.9435 Batch Accuracy: 71.7500\n",
            "520th Batch Loss: 0.8681 Batch Accuracy: 71.7157\n",
            "540th Batch Loss: 0.9139 Batch Accuracy: 71.4931\n",
            "560th Batch Loss: 0.5661 Batch Accuracy: 71.4844\n",
            "580th Batch Loss: 1.1623 Batch Accuracy: 71.4359\n",
            "600th Batch Loss: 0.5850 Batch Accuracy: 71.4089\n",
            "620th Batch Loss: 0.8112 Batch Accuracy: 71.4062\n",
            "640th Batch Loss: 0.9703 Batch Accuracy: 71.2866\n",
            "660th Batch Loss: 0.7099 Batch Accuracy: 71.3802\n",
            "Epoch [8/10] Loss: 0.6335 Epoch Accuracy: 71.3519\n",
            "0\n",
            "Validation Accuracy: 76.35%\n",
            "20th Batch Loss: 0.6528 Batch Accuracy: 73.2812\n",
            "40th Batch Loss: 0.5525 Batch Accuracy: 72.8125\n",
            "60th Batch Loss: 1.0736 Batch Accuracy: 72.4219\n",
            "80th Batch Loss: 0.6479 Batch Accuracy: 72.3242\n",
            "100th Batch Loss: 0.9873 Batch Accuracy: 71.9375\n",
            "120th Batch Loss: 0.7382 Batch Accuracy: 72.2526\n",
            "140th Batch Loss: 0.6372 Batch Accuracy: 72.1875\n",
            "160th Batch Loss: 0.5963 Batch Accuracy: 72.0898\n",
            "180th Batch Loss: 1.0163 Batch Accuracy: 71.7708\n",
            "200th Batch Loss: 1.1354 Batch Accuracy: 71.4453\n",
            "220th Batch Loss: 0.9495 Batch Accuracy: 71.5057\n",
            "240th Batch Loss: 0.5773 Batch Accuracy: 71.4844\n",
            "260th Batch Loss: 0.9597 Batch Accuracy: 71.6286\n",
            "280th Batch Loss: 0.8440 Batch Accuracy: 71.7578\n",
            "300th Batch Loss: 0.4715 Batch Accuracy: 71.6771\n",
            "320th Batch Loss: 0.7372 Batch Accuracy: 71.6504\n",
            "340th Batch Loss: 0.7195 Batch Accuracy: 71.7050\n",
            "360th Batch Loss: 0.6315 Batch Accuracy: 71.8620\n",
            "380th Batch Loss: 0.6318 Batch Accuracy: 72.0724\n",
            "400th Batch Loss: 0.7720 Batch Accuracy: 72.0078\n",
            "420th Batch Loss: 0.7582 Batch Accuracy: 71.9978\n",
            "440th Batch Loss: 1.0452 Batch Accuracy: 71.9425\n",
            "460th Batch Loss: 0.7319 Batch Accuracy: 71.9973\n",
            "480th Batch Loss: 1.2843 Batch Accuracy: 72.0247\n",
            "500th Batch Loss: 1.1800 Batch Accuracy: 71.9750\n",
            "520th Batch Loss: 0.5304 Batch Accuracy: 72.0343\n",
            "540th Batch Loss: 0.7374 Batch Accuracy: 71.9878\n",
            "560th Batch Loss: 0.6849 Batch Accuracy: 71.9252\n",
            "580th Batch Loss: 1.1335 Batch Accuracy: 71.8534\n",
            "600th Batch Loss: 0.9161 Batch Accuracy: 71.8568\n",
            "620th Batch Loss: 1.1198 Batch Accuracy: 71.8700\n",
            "640th Batch Loss: 0.6784 Batch Accuracy: 71.8896\n",
            "660th Batch Loss: 0.7991 Batch Accuracy: 71.9247\n",
            "Epoch [9/10] Loss: 0.8646 Epoch Accuracy: 71.9745\n",
            "0\n",
            "Validation Accuracy: 78.88%\n",
            "20th Batch Loss: 0.5315 Batch Accuracy: 68.9062\n",
            "40th Batch Loss: 0.4828 Batch Accuracy: 72.5391\n",
            "60th Batch Loss: 0.7412 Batch Accuracy: 73.2552\n",
            "80th Batch Loss: 0.4058 Batch Accuracy: 73.3984\n",
            "100th Batch Loss: 0.7164 Batch Accuracy: 72.8125\n",
            "120th Batch Loss: 0.7232 Batch Accuracy: 73.0729\n",
            "140th Batch Loss: 0.6643 Batch Accuracy: 72.8125\n",
            "160th Batch Loss: 0.9411 Batch Accuracy: 72.7148\n",
            "180th Batch Loss: 0.8372 Batch Accuracy: 72.6649\n",
            "200th Batch Loss: 0.5523 Batch Accuracy: 72.7188\n",
            "220th Batch Loss: 0.5486 Batch Accuracy: 72.5994\n",
            "240th Batch Loss: 0.5674 Batch Accuracy: 72.6628\n",
            "260th Batch Loss: 0.4393 Batch Accuracy: 72.7404\n",
            "280th Batch Loss: 0.9645 Batch Accuracy: 72.7065\n",
            "300th Batch Loss: 0.6594 Batch Accuracy: 72.7135\n",
            "320th Batch Loss: 0.6452 Batch Accuracy: 72.9834\n",
            "340th Batch Loss: 1.0343 Batch Accuracy: 72.9412\n",
            "360th Batch Loss: 0.8639 Batch Accuracy: 73.0339\n",
            "380th Batch Loss: 0.6903 Batch Accuracy: 72.9317\n",
            "400th Batch Loss: 0.8165 Batch Accuracy: 73.0625\n",
            "420th Batch Loss: 1.1430 Batch Accuracy: 73.1027\n",
            "440th Batch Loss: 0.8534 Batch Accuracy: 73.0824\n",
            "460th Batch Loss: 0.6487 Batch Accuracy: 73.0503\n",
            "480th Batch Loss: 1.1454 Batch Accuracy: 73.0176\n",
            "500th Batch Loss: 1.0531 Batch Accuracy: 72.9125\n",
            "520th Batch Loss: 0.6894 Batch Accuracy: 72.7764\n",
            "540th Batch Loss: 0.7720 Batch Accuracy: 72.7170\n",
            "560th Batch Loss: 0.8660 Batch Accuracy: 72.6367\n",
            "580th Batch Loss: 0.9200 Batch Accuracy: 72.6913\n",
            "600th Batch Loss: 0.8523 Batch Accuracy: 72.7943\n",
            "620th Batch Loss: 0.6749 Batch Accuracy: 72.7445\n",
            "640th Batch Loss: 0.7680 Batch Accuracy: 72.7222\n",
            "660th Batch Loss: 0.8393 Batch Accuracy: 72.6539\n",
            "Epoch [10/10] Loss: 0.7189 Epoch Accuracy: 72.6829\n",
            "1\n",
            "Validation Accuracy: 76.15%\n",
            "Fold 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 1.8335 Batch Accuracy: 22.6562\n",
            "40th Batch Loss: 1.9455 Batch Accuracy: 25.8984\n",
            "60th Batch Loss: 1.8244 Batch Accuracy: 25.9896\n",
            "80th Batch Loss: 1.9712 Batch Accuracy: 27.6953\n",
            "100th Batch Loss: 1.6925 Batch Accuracy: 29.2812\n",
            "120th Batch Loss: 1.6151 Batch Accuracy: 30.4948\n",
            "140th Batch Loss: 1.6139 Batch Accuracy: 31.2835\n",
            "160th Batch Loss: 1.3855 Batch Accuracy: 32.5586\n",
            "180th Batch Loss: 1.4090 Batch Accuracy: 33.1163\n",
            "200th Batch Loss: 1.4016 Batch Accuracy: 33.6172\n",
            "220th Batch Loss: 1.4730 Batch Accuracy: 34.5384\n",
            "240th Batch Loss: 1.5228 Batch Accuracy: 35.2799\n",
            "260th Batch Loss: 1.6774 Batch Accuracy: 35.8714\n",
            "280th Batch Loss: 1.4972 Batch Accuracy: 36.5290\n",
            "300th Batch Loss: 1.6232 Batch Accuracy: 37.0990\n",
            "320th Batch Loss: 1.5139 Batch Accuracy: 37.5781\n",
            "340th Batch Loss: 1.4844 Batch Accuracy: 38.2169\n",
            "360th Batch Loss: 1.5630 Batch Accuracy: 38.7196\n",
            "380th Batch Loss: 1.3695 Batch Accuracy: 39.2640\n",
            "400th Batch Loss: 1.5292 Batch Accuracy: 39.7695\n",
            "420th Batch Loss: 1.3390 Batch Accuracy: 40.1600\n",
            "440th Batch Loss: 1.2882 Batch Accuracy: 40.6605\n",
            "460th Batch Loss: 1.0527 Batch Accuracy: 41.0870\n",
            "480th Batch Loss: 1.2037 Batch Accuracy: 41.3249\n",
            "500th Batch Loss: 1.3475 Batch Accuracy: 41.5063\n",
            "520th Batch Loss: 1.4295 Batch Accuracy: 41.7278\n",
            "540th Batch Loss: 1.6559 Batch Accuracy: 41.9531\n",
            "560th Batch Loss: 1.2289 Batch Accuracy: 42.2935\n",
            "580th Batch Loss: 1.5669 Batch Accuracy: 42.6994\n",
            "600th Batch Loss: 2.0827 Batch Accuracy: 43.0078\n",
            "620th Batch Loss: 1.4449 Batch Accuracy: 43.2913\n",
            "640th Batch Loss: 1.5734 Batch Accuracy: 43.5889\n",
            "660th Batch Loss: 1.1245 Batch Accuracy: 43.8423\n",
            "Epoch [1/10] Loss: 1.3351 Epoch Accuracy: 44.0810\n",
            "0\n",
            "Validation Accuracy: 55.25%\n",
            "20th Batch Loss: 1.9917 Batch Accuracy: 34.7656\n",
            "40th Batch Loss: 1.5010 Batch Accuracy: 36.6016\n",
            "60th Batch Loss: 1.2171 Batch Accuracy: 40.0781\n",
            "80th Batch Loss: 1.2370 Batch Accuracy: 42.6367\n",
            "100th Batch Loss: 1.4632 Batch Accuracy: 42.7812\n",
            "120th Batch Loss: 1.5308 Batch Accuracy: 43.6849\n",
            "140th Batch Loss: 1.2793 Batch Accuracy: 45.0670\n",
            "160th Batch Loss: 1.5556 Batch Accuracy: 45.5566\n",
            "180th Batch Loss: 1.5379 Batch Accuracy: 46.1632\n",
            "200th Batch Loss: 0.8442 Batch Accuracy: 46.9844\n",
            "220th Batch Loss: 1.6854 Batch Accuracy: 47.6562\n",
            "240th Batch Loss: 1.2449 Batch Accuracy: 48.2292\n",
            "260th Batch Loss: 1.3833 Batch Accuracy: 48.4796\n",
            "280th Batch Loss: 1.2481 Batch Accuracy: 48.8839\n",
            "300th Batch Loss: 1.2034 Batch Accuracy: 49.6406\n",
            "320th Batch Loss: 1.0613 Batch Accuracy: 49.9707\n",
            "340th Batch Loss: 1.0991 Batch Accuracy: 50.3355\n",
            "360th Batch Loss: 1.1371 Batch Accuracy: 50.6641\n",
            "380th Batch Loss: 1.0623 Batch Accuracy: 50.9046\n",
            "400th Batch Loss: 1.1749 Batch Accuracy: 51.1836\n",
            "420th Batch Loss: 0.9582 Batch Accuracy: 51.5104\n",
            "440th Batch Loss: 0.9740 Batch Accuracy: 51.8572\n",
            "460th Batch Loss: 1.4129 Batch Accuracy: 52.2690\n",
            "480th Batch Loss: 1.4042 Batch Accuracy: 52.4935\n",
            "500th Batch Loss: 0.9527 Batch Accuracy: 52.7656\n",
            "520th Batch Loss: 1.5222 Batch Accuracy: 52.8155\n",
            "540th Batch Loss: 1.0355 Batch Accuracy: 53.0787\n",
            "560th Batch Loss: 1.4536 Batch Accuracy: 53.2450\n",
            "580th Batch Loss: 1.1875 Batch Accuracy: 53.3863\n",
            "600th Batch Loss: 0.9204 Batch Accuracy: 53.5000\n",
            "620th Batch Loss: 0.8372 Batch Accuracy: 53.6013\n",
            "640th Batch Loss: 1.2161 Batch Accuracy: 53.7085\n",
            "660th Batch Loss: 1.6213 Batch Accuracy: 53.8352\n",
            "Epoch [2/10] Loss: 1.1476 Epoch Accuracy: 53.9722\n",
            "0\n",
            "Validation Accuracy: 61.42%\n",
            "20th Batch Loss: 0.9089 Batch Accuracy: 59.6875\n",
            "40th Batch Loss: 1.0526 Batch Accuracy: 58.5547\n",
            "60th Batch Loss: 0.9195 Batch Accuracy: 58.9583\n",
            "80th Batch Loss: 1.1238 Batch Accuracy: 59.5898\n",
            "100th Batch Loss: 1.0707 Batch Accuracy: 59.4844\n",
            "120th Batch Loss: 0.7530 Batch Accuracy: 60.2734\n",
            "140th Batch Loss: 1.3055 Batch Accuracy: 60.0893\n",
            "160th Batch Loss: 1.0714 Batch Accuracy: 60.2539\n",
            "180th Batch Loss: 0.9151 Batch Accuracy: 60.0694\n",
            "200th Batch Loss: 0.9330 Batch Accuracy: 60.1094\n",
            "220th Batch Loss: 1.2440 Batch Accuracy: 60.3693\n",
            "240th Batch Loss: 1.0674 Batch Accuracy: 60.2930\n",
            "260th Batch Loss: 0.9465 Batch Accuracy: 60.4207\n",
            "280th Batch Loss: 0.8482 Batch Accuracy: 60.4855\n",
            "300th Batch Loss: 1.8247 Batch Accuracy: 60.4948\n",
            "320th Batch Loss: 1.1536 Batch Accuracy: 60.4102\n",
            "340th Batch Loss: 1.0284 Batch Accuracy: 60.4596\n",
            "360th Batch Loss: 1.0944 Batch Accuracy: 60.6207\n",
            "380th Batch Loss: 0.9077 Batch Accuracy: 60.6456\n",
            "400th Batch Loss: 1.5508 Batch Accuracy: 60.7227\n",
            "420th Batch Loss: 0.8521 Batch Accuracy: 60.7924\n",
            "440th Batch Loss: 0.9977 Batch Accuracy: 60.6499\n",
            "460th Batch Loss: 1.0754 Batch Accuracy: 60.6216\n",
            "480th Batch Loss: 0.8701 Batch Accuracy: 60.6771\n",
            "500th Batch Loss: 0.9662 Batch Accuracy: 60.8094\n",
            "520th Batch Loss: 0.9782 Batch Accuracy: 60.8444\n",
            "540th Batch Loss: 1.1839 Batch Accuracy: 60.9433\n",
            "560th Batch Loss: 0.7167 Batch Accuracy: 61.0268\n",
            "580th Batch Loss: 0.8449 Batch Accuracy: 61.1099\n",
            "600th Batch Loss: 1.0182 Batch Accuracy: 61.2344\n",
            "620th Batch Loss: 0.9971 Batch Accuracy: 61.3432\n",
            "640th Batch Loss: 0.9843 Batch Accuracy: 61.2866\n",
            "660th Batch Loss: 1.3237 Batch Accuracy: 61.4441\n",
            "Epoch [3/10] Loss: 0.7690 Epoch Accuracy: 61.4560\n",
            "0\n",
            "Validation Accuracy: 65.92%\n",
            "20th Batch Loss: 1.1532 Batch Accuracy: 62.4219\n",
            "40th Batch Loss: 0.7310 Batch Accuracy: 64.1406\n",
            "60th Batch Loss: 0.9653 Batch Accuracy: 63.5156\n",
            "80th Batch Loss: 0.7558 Batch Accuracy: 62.7539\n",
            "100th Batch Loss: 0.4474 Batch Accuracy: 62.9062\n",
            "120th Batch Loss: 1.0248 Batch Accuracy: 62.8776\n",
            "140th Batch Loss: 1.0886 Batch Accuracy: 62.5112\n",
            "160th Batch Loss: 1.1516 Batch Accuracy: 62.9492\n",
            "180th Batch Loss: 0.8460 Batch Accuracy: 63.1684\n",
            "200th Batch Loss: 1.1991 Batch Accuracy: 62.6641\n",
            "220th Batch Loss: 0.9989 Batch Accuracy: 62.8835\n",
            "240th Batch Loss: 1.1855 Batch Accuracy: 62.7995\n",
            "260th Batch Loss: 0.9833 Batch Accuracy: 62.8425\n",
            "280th Batch Loss: 0.9113 Batch Accuracy: 62.9129\n",
            "300th Batch Loss: 0.8095 Batch Accuracy: 63.0312\n",
            "320th Batch Loss: 1.2070 Batch Accuracy: 63.0371\n",
            "340th Batch Loss: 0.8953 Batch Accuracy: 63.0882\n",
            "360th Batch Loss: 0.6749 Batch Accuracy: 63.1684\n",
            "380th Batch Loss: 0.9798 Batch Accuracy: 63.2895\n",
            "400th Batch Loss: 0.8137 Batch Accuracy: 63.3633\n",
            "420th Batch Loss: 1.1885 Batch Accuracy: 63.4412\n",
            "440th Batch Loss: 0.8365 Batch Accuracy: 63.5724\n",
            "460th Batch Loss: 0.7379 Batch Accuracy: 63.8077\n",
            "480th Batch Loss: 0.7896 Batch Accuracy: 63.7793\n",
            "500th Batch Loss: 0.9497 Batch Accuracy: 63.7781\n",
            "520th Batch Loss: 0.9144 Batch Accuracy: 63.9303\n",
            "540th Batch Loss: 1.0188 Batch Accuracy: 63.9873\n",
            "560th Batch Loss: 1.5430 Batch Accuracy: 64.0876\n",
            "580th Batch Loss: 1.0881 Batch Accuracy: 64.0625\n",
            "600th Batch Loss: 1.2893 Batch Accuracy: 64.0625\n",
            "620th Batch Loss: 0.9199 Batch Accuracy: 64.0701\n",
            "640th Batch Loss: 0.8016 Batch Accuracy: 64.0991\n",
            "660th Batch Loss: 1.1843 Batch Accuracy: 64.1477\n",
            "Epoch [4/10] Loss: 1.0486 Epoch Accuracy: 64.1366\n",
            "0\n",
            "Validation Accuracy: 71.60%\n",
            "20th Batch Loss: 0.8594 Batch Accuracy: 68.0469\n",
            "40th Batch Loss: 0.9553 Batch Accuracy: 67.5000\n",
            "60th Batch Loss: 1.2579 Batch Accuracy: 66.5104\n",
            "80th Batch Loss: 1.1336 Batch Accuracy: 66.6602\n",
            "100th Batch Loss: 1.0628 Batch Accuracy: 66.9688\n",
            "120th Batch Loss: 0.7545 Batch Accuracy: 66.7318\n",
            "140th Batch Loss: 0.7411 Batch Accuracy: 67.1763\n",
            "160th Batch Loss: 1.3459 Batch Accuracy: 66.7090\n",
            "180th Batch Loss: 0.9326 Batch Accuracy: 66.8142\n",
            "200th Batch Loss: 0.9042 Batch Accuracy: 66.6953\n",
            "220th Batch Loss: 1.1950 Batch Accuracy: 66.5625\n",
            "240th Batch Loss: 0.9299 Batch Accuracy: 66.7773\n",
            "260th Batch Loss: 0.8030 Batch Accuracy: 66.9050\n",
            "280th Batch Loss: 0.7910 Batch Accuracy: 66.9531\n",
            "300th Batch Loss: 1.0027 Batch Accuracy: 66.9271\n",
            "320th Batch Loss: 0.9172 Batch Accuracy: 66.9922\n",
            "340th Batch Loss: 0.7685 Batch Accuracy: 67.1599\n",
            "360th Batch Loss: 0.9582 Batch Accuracy: 67.2700\n",
            "380th Batch Loss: 0.7014 Batch Accuracy: 67.3355\n",
            "400th Batch Loss: 0.7893 Batch Accuracy: 67.5000\n",
            "420th Batch Loss: 1.0840 Batch Accuracy: 67.5037\n",
            "440th Batch Loss: 1.2036 Batch Accuracy: 67.4751\n",
            "460th Batch Loss: 1.0582 Batch Accuracy: 67.4898\n",
            "480th Batch Loss: 0.8456 Batch Accuracy: 67.5358\n",
            "500th Batch Loss: 1.2554 Batch Accuracy: 67.6375\n",
            "520th Batch Loss: 0.6829 Batch Accuracy: 67.5631\n",
            "540th Batch Loss: 0.8190 Batch Accuracy: 67.6331\n",
            "560th Batch Loss: 0.7485 Batch Accuracy: 67.8181\n",
            "580th Batch Loss: 1.3224 Batch Accuracy: 67.7317\n",
            "600th Batch Loss: 1.3259 Batch Accuracy: 67.6979\n",
            "620th Batch Loss: 0.7756 Batch Accuracy: 67.6336\n",
            "640th Batch Loss: 1.0328 Batch Accuracy: 67.6099\n",
            "660th Batch Loss: 0.6812 Batch Accuracy: 67.5994\n",
            "Epoch [5/10] Loss: 0.6620 Epoch Accuracy: 67.5394\n",
            "0\n",
            "Validation Accuracy: 72.44%\n",
            "20th Batch Loss: 1.2244 Batch Accuracy: 67.3438\n",
            "40th Batch Loss: 1.1991 Batch Accuracy: 67.6562\n",
            "60th Batch Loss: 1.0360 Batch Accuracy: 68.0990\n",
            "80th Batch Loss: 0.6848 Batch Accuracy: 67.9883\n",
            "100th Batch Loss: 0.6552 Batch Accuracy: 68.8125\n",
            "120th Batch Loss: 0.9417 Batch Accuracy: 68.7500\n",
            "140th Batch Loss: 0.8335 Batch Accuracy: 68.9397\n",
            "160th Batch Loss: 0.8026 Batch Accuracy: 68.9941\n",
            "180th Batch Loss: 1.4483 Batch Accuracy: 68.9497\n",
            "200th Batch Loss: 0.9678 Batch Accuracy: 69.2422\n",
            "220th Batch Loss: 0.5951 Batch Accuracy: 69.1051\n",
            "240th Batch Loss: 1.0011 Batch Accuracy: 68.8086\n",
            "260th Batch Loss: 0.9355 Batch Accuracy: 68.6719\n",
            "280th Batch Loss: 1.0876 Batch Accuracy: 68.5714\n",
            "300th Batch Loss: 1.0585 Batch Accuracy: 68.5729\n",
            "320th Batch Loss: 0.8248 Batch Accuracy: 68.5596\n",
            "340th Batch Loss: 0.7742 Batch Accuracy: 68.8603\n",
            "360th Batch Loss: 1.1151 Batch Accuracy: 68.6675\n",
            "380th Batch Loss: 0.8486 Batch Accuracy: 68.6184\n",
            "400th Batch Loss: 1.2136 Batch Accuracy: 68.7070\n",
            "420th Batch Loss: 1.1013 Batch Accuracy: 68.5640\n",
            "440th Batch Loss: 0.9082 Batch Accuracy: 68.4375\n",
            "460th Batch Loss: 1.1152 Batch Accuracy: 68.5971\n",
            "480th Batch Loss: 0.7909 Batch Accuracy: 68.7207\n",
            "500th Batch Loss: 0.9469 Batch Accuracy: 68.7906\n",
            "520th Batch Loss: 0.9360 Batch Accuracy: 68.7800\n",
            "540th Batch Loss: 1.1652 Batch Accuracy: 68.7703\n",
            "560th Batch Loss: 1.2197 Batch Accuracy: 68.7305\n",
            "580th Batch Loss: 0.7143 Batch Accuracy: 68.7365\n",
            "600th Batch Loss: 0.8086 Batch Accuracy: 68.7448\n",
            "620th Batch Loss: 0.6074 Batch Accuracy: 68.7424\n",
            "640th Batch Loss: 0.5049 Batch Accuracy: 68.8110\n",
            "660th Batch Loss: 0.8318 Batch Accuracy: 68.8920\n",
            "Epoch [6/10] Loss: 0.9100 Epoch Accuracy: 68.9120\n",
            "0\n",
            "Validation Accuracy: 75.52%\n",
            "20th Batch Loss: 0.7418 Batch Accuracy: 70.6250\n",
            "40th Batch Loss: 1.4840 Batch Accuracy: 69.6484\n",
            "60th Batch Loss: 0.7705 Batch Accuracy: 70.5469\n",
            "80th Batch Loss: 0.6258 Batch Accuracy: 70.0195\n",
            "100th Batch Loss: 0.8358 Batch Accuracy: 70.3906\n",
            "120th Batch Loss: 0.6639 Batch Accuracy: 70.8854\n",
            "140th Batch Loss: 0.7767 Batch Accuracy: 71.0826\n",
            "160th Batch Loss: 0.6305 Batch Accuracy: 70.9961\n",
            "180th Batch Loss: 1.0142 Batch Accuracy: 70.6858\n",
            "200th Batch Loss: 0.7404 Batch Accuracy: 70.5938\n",
            "220th Batch Loss: 0.7672 Batch Accuracy: 70.5327\n",
            "240th Batch Loss: 0.7890 Batch Accuracy: 70.6120\n",
            "260th Batch Loss: 0.9751 Batch Accuracy: 70.6130\n",
            "280th Batch Loss: 0.7717 Batch Accuracy: 70.5580\n",
            "300th Batch Loss: 0.8663 Batch Accuracy: 70.6250\n",
            "320th Batch Loss: 0.9310 Batch Accuracy: 70.3857\n",
            "340th Batch Loss: 0.6337 Batch Accuracy: 70.2987\n",
            "360th Batch Loss: 0.6931 Batch Accuracy: 70.3385\n",
            "380th Batch Loss: 0.9208 Batch Accuracy: 70.4030\n",
            "400th Batch Loss: 0.7110 Batch Accuracy: 70.3398\n",
            "420th Batch Loss: 1.3337 Batch Accuracy: 70.2009\n",
            "440th Batch Loss: 0.6073 Batch Accuracy: 70.3054\n",
            "460th Batch Loss: 0.9842 Batch Accuracy: 70.2378\n",
            "480th Batch Loss: 0.8673 Batch Accuracy: 70.2637\n",
            "500th Batch Loss: 1.1647 Batch Accuracy: 70.3281\n",
            "520th Batch Loss: 1.0746 Batch Accuracy: 70.4026\n",
            "540th Batch Loss: 0.7385 Batch Accuracy: 70.3762\n",
            "560th Batch Loss: 0.6234 Batch Accuracy: 70.3348\n",
            "580th Batch Loss: 0.7643 Batch Accuracy: 70.3664\n",
            "600th Batch Loss: 0.7511 Batch Accuracy: 70.3438\n",
            "620th Batch Loss: 0.8992 Batch Accuracy: 70.3906\n",
            "640th Batch Loss: 0.6489 Batch Accuracy: 70.3491\n",
            "660th Batch Loss: 0.7276 Batch Accuracy: 70.3267\n",
            "Epoch [7/10] Loss: 0.8278 Epoch Accuracy: 70.3542\n",
            "0\n",
            "Validation Accuracy: 75.75%\n",
            "20th Batch Loss: 0.7316 Batch Accuracy: 70.7812\n",
            "40th Batch Loss: 0.6982 Batch Accuracy: 70.8594\n",
            "60th Batch Loss: 0.5626 Batch Accuracy: 71.1458\n",
            "80th Batch Loss: 0.4667 Batch Accuracy: 71.5820\n",
            "100th Batch Loss: 0.8804 Batch Accuracy: 71.2656\n",
            "120th Batch Loss: 0.6534 Batch Accuracy: 70.9635\n",
            "140th Batch Loss: 0.5317 Batch Accuracy: 71.6183\n",
            "160th Batch Loss: 0.8102 Batch Accuracy: 71.2891\n",
            "180th Batch Loss: 0.8095 Batch Accuracy: 71.7188\n",
            "200th Batch Loss: 1.1716 Batch Accuracy: 71.4062\n",
            "220th Batch Loss: 1.0176 Batch Accuracy: 71.3849\n",
            "240th Batch Loss: 0.6607 Batch Accuracy: 71.4128\n",
            "260th Batch Loss: 0.5670 Batch Accuracy: 71.4002\n",
            "280th Batch Loss: 0.5597 Batch Accuracy: 71.5960\n",
            "300th Batch Loss: 0.6140 Batch Accuracy: 71.4062\n",
            "320th Batch Loss: 0.5425 Batch Accuracy: 71.3135\n",
            "340th Batch Loss: 0.6023 Batch Accuracy: 71.1121\n",
            "360th Batch Loss: 1.1912 Batch Accuracy: 71.1241\n",
            "380th Batch Loss: 0.5265 Batch Accuracy: 71.1349\n",
            "400th Batch Loss: 0.8628 Batch Accuracy: 71.0352\n",
            "420th Batch Loss: 0.8601 Batch Accuracy: 71.1458\n",
            "440th Batch Loss: 0.7436 Batch Accuracy: 71.1470\n",
            "460th Batch Loss: 0.5816 Batch Accuracy: 71.3587\n",
            "480th Batch Loss: 0.4758 Batch Accuracy: 71.4616\n",
            "500th Batch Loss: 0.9379 Batch Accuracy: 71.4781\n",
            "520th Batch Loss: 0.7436 Batch Accuracy: 71.4724\n",
            "540th Batch Loss: 0.5436 Batch Accuracy: 71.5104\n",
            "560th Batch Loss: 0.7033 Batch Accuracy: 71.5262\n",
            "580th Batch Loss: 0.9033 Batch Accuracy: 71.4898\n",
            "600th Batch Loss: 0.6843 Batch Accuracy: 71.5026\n",
            "620th Batch Loss: 1.0389 Batch Accuracy: 71.4567\n",
            "640th Batch Loss: 0.9284 Batch Accuracy: 71.4331\n",
            "660th Batch Loss: 0.7968 Batch Accuracy: 71.4323\n",
            "Epoch [8/10] Loss: 0.5494 Epoch Accuracy: 71.4329\n",
            "0\n",
            "Validation Accuracy: 76.77%\n",
            "20th Batch Loss: 1.2923 Batch Accuracy: 70.0000\n",
            "40th Batch Loss: 0.7104 Batch Accuracy: 70.9375\n",
            "60th Batch Loss: 0.7720 Batch Accuracy: 72.1354\n",
            "80th Batch Loss: 0.7424 Batch Accuracy: 71.4062\n",
            "100th Batch Loss: 0.7602 Batch Accuracy: 71.2812\n",
            "120th Batch Loss: 0.5572 Batch Accuracy: 71.8490\n",
            "140th Batch Loss: 0.6246 Batch Accuracy: 72.0424\n",
            "160th Batch Loss: 0.8238 Batch Accuracy: 71.7676\n",
            "180th Batch Loss: 0.6138 Batch Accuracy: 71.8924\n",
            "200th Batch Loss: 0.7075 Batch Accuracy: 71.7891\n",
            "220th Batch Loss: 0.7690 Batch Accuracy: 71.8182\n",
            "240th Batch Loss: 0.8240 Batch Accuracy: 72.0768\n",
            "260th Batch Loss: 0.6698 Batch Accuracy: 72.0853\n",
            "280th Batch Loss: 1.0466 Batch Accuracy: 72.0312\n",
            "300th Batch Loss: 0.7619 Batch Accuracy: 71.9531\n",
            "320th Batch Loss: 0.7187 Batch Accuracy: 71.9873\n",
            "340th Batch Loss: 0.4767 Batch Accuracy: 72.0588\n",
            "360th Batch Loss: 0.8470 Batch Accuracy: 72.2439\n",
            "380th Batch Loss: 0.6196 Batch Accuracy: 72.2039\n",
            "400th Batch Loss: 0.6380 Batch Accuracy: 72.2422\n",
            "420th Batch Loss: 0.6224 Batch Accuracy: 72.1949\n",
            "440th Batch Loss: 0.6266 Batch Accuracy: 72.2727\n",
            "460th Batch Loss: 0.8063 Batch Accuracy: 72.3845\n",
            "480th Batch Loss: 0.4935 Batch Accuracy: 72.4121\n",
            "500th Batch Loss: 0.6483 Batch Accuracy: 72.4156\n",
            "520th Batch Loss: 0.5944 Batch Accuracy: 72.3768\n",
            "540th Batch Loss: 0.6944 Batch Accuracy: 72.4103\n",
            "560th Batch Loss: 0.6812 Batch Accuracy: 72.3661\n",
            "580th Batch Loss: 0.7816 Batch Accuracy: 72.3734\n",
            "600th Batch Loss: 0.6075 Batch Accuracy: 72.3620\n",
            "620th Batch Loss: 0.6573 Batch Accuracy: 72.3992\n",
            "640th Batch Loss: 0.8729 Batch Accuracy: 72.3340\n",
            "660th Batch Loss: 0.5979 Batch Accuracy: 72.3390\n",
            "Epoch [9/10] Loss: 0.7043 Epoch Accuracy: 72.4306\n",
            "0\n",
            "Validation Accuracy: 77.38%\n",
            "20th Batch Loss: 0.7778 Batch Accuracy: 71.2500\n",
            "40th Batch Loss: 0.5602 Batch Accuracy: 72.5000\n",
            "60th Batch Loss: 0.9010 Batch Accuracy: 72.3438\n",
            "80th Batch Loss: 0.9729 Batch Accuracy: 72.0117\n",
            "100th Batch Loss: 0.8176 Batch Accuracy: 72.2500\n",
            "120th Batch Loss: 1.1228 Batch Accuracy: 71.8229\n",
            "140th Batch Loss: 1.0480 Batch Accuracy: 72.1429\n",
            "160th Batch Loss: 0.5268 Batch Accuracy: 72.2070\n",
            "180th Batch Loss: 0.6497 Batch Accuracy: 72.3524\n",
            "200th Batch Loss: 0.9378 Batch Accuracy: 72.7422\n",
            "220th Batch Loss: 0.5869 Batch Accuracy: 72.8835\n",
            "240th Batch Loss: 0.9335 Batch Accuracy: 72.9232\n",
            "260th Batch Loss: 0.9804 Batch Accuracy: 73.1130\n",
            "280th Batch Loss: 0.6199 Batch Accuracy: 73.0915\n",
            "300th Batch Loss: 0.6539 Batch Accuracy: 73.3698\n",
            "320th Batch Loss: 0.7252 Batch Accuracy: 73.5059\n",
            "340th Batch Loss: 1.0725 Batch Accuracy: 73.4145\n",
            "360th Batch Loss: 0.8170 Batch Accuracy: 73.3464\n",
            "380th Batch Loss: 0.5765 Batch Accuracy: 73.3635\n",
            "400th Batch Loss: 0.5674 Batch Accuracy: 73.4453\n",
            "420th Batch Loss: 0.9838 Batch Accuracy: 73.3966\n",
            "440th Batch Loss: 0.9522 Batch Accuracy: 73.3026\n",
            "460th Batch Loss: 0.6840 Batch Accuracy: 73.2914\n",
            "480th Batch Loss: 0.5733 Batch Accuracy: 73.2552\n",
            "500th Batch Loss: 0.6350 Batch Accuracy: 73.3719\n",
            "520th Batch Loss: 1.0217 Batch Accuracy: 73.4946\n",
            "540th Batch Loss: 0.5500 Batch Accuracy: 73.5793\n",
            "560th Batch Loss: 0.7203 Batch Accuracy: 73.5993\n",
            "580th Batch Loss: 0.5384 Batch Accuracy: 73.6153\n",
            "600th Batch Loss: 0.8204 Batch Accuracy: 73.6589\n",
            "620th Batch Loss: 0.6109 Batch Accuracy: 73.7727\n",
            "640th Batch Loss: 0.7098 Batch Accuracy: 73.7671\n",
            "660th Batch Loss: 0.7563 Batch Accuracy: 73.7405\n",
            "Epoch [10/10] Loss: 0.6687 Epoch Accuracy: 73.7269\n",
            "0\n",
            "Validation Accuracy: 77.98%\n",
            "Fold 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 1.8699 Batch Accuracy: 17.2656\n",
            "40th Batch Loss: 2.0061 Batch Accuracy: 23.2031\n",
            "60th Batch Loss: 1.8542 Batch Accuracy: 25.6510\n",
            "80th Batch Loss: 1.8772 Batch Accuracy: 26.8164\n",
            "100th Batch Loss: 1.7554 Batch Accuracy: 28.6094\n",
            "120th Batch Loss: 1.6898 Batch Accuracy: 29.5573\n",
            "140th Batch Loss: 1.7195 Batch Accuracy: 30.5804\n",
            "160th Batch Loss: 1.3718 Batch Accuracy: 31.3574\n",
            "180th Batch Loss: 1.6789 Batch Accuracy: 32.3872\n",
            "200th Batch Loss: 1.6408 Batch Accuracy: 33.0938\n",
            "220th Batch Loss: 1.6157 Batch Accuracy: 34.0909\n",
            "240th Batch Loss: 1.3278 Batch Accuracy: 34.4596\n",
            "260th Batch Loss: 1.5297 Batch Accuracy: 35.0962\n",
            "280th Batch Loss: 1.2825 Batch Accuracy: 35.6864\n",
            "300th Batch Loss: 1.6645 Batch Accuracy: 36.3021\n",
            "320th Batch Loss: 1.6629 Batch Accuracy: 36.7236\n",
            "340th Batch Loss: 1.2101 Batch Accuracy: 37.3300\n",
            "360th Batch Loss: 1.2473 Batch Accuracy: 37.8472\n",
            "380th Batch Loss: 1.4889 Batch Accuracy: 38.1003\n",
            "400th Batch Loss: 1.1911 Batch Accuracy: 38.5273\n",
            "420th Batch Loss: 1.5563 Batch Accuracy: 38.7686\n",
            "440th Batch Loss: 1.3734 Batch Accuracy: 39.1087\n",
            "460th Batch Loss: 1.3898 Batch Accuracy: 39.5312\n",
            "480th Batch Loss: 1.2696 Batch Accuracy: 39.8340\n",
            "500th Batch Loss: 1.4267 Batch Accuracy: 40.2125\n",
            "520th Batch Loss: 1.3442 Batch Accuracy: 40.6340\n",
            "540th Batch Loss: 1.4713 Batch Accuracy: 41.0069\n",
            "560th Batch Loss: 1.2707 Batch Accuracy: 41.4174\n",
            "580th Batch Loss: 1.1449 Batch Accuracy: 41.7915\n",
            "600th Batch Loss: 1.4004 Batch Accuracy: 42.0599\n",
            "620th Batch Loss: 1.5121 Batch Accuracy: 42.3715\n",
            "640th Batch Loss: 1.7036 Batch Accuracy: 42.6978\n",
            "660th Batch Loss: 1.0849 Batch Accuracy: 43.0587\n",
            "Epoch [1/10] Loss: 1.0737 Epoch Accuracy: 43.3194\n",
            "0\n",
            "Validation Accuracy: 57.81%\n",
            "20th Batch Loss: 1.8173 Batch Accuracy: 47.1875\n",
            "40th Batch Loss: 1.8741 Batch Accuracy: 46.4062\n",
            "60th Batch Loss: 1.0466 Batch Accuracy: 47.8906\n",
            "80th Batch Loss: 1.0965 Batch Accuracy: 49.4922\n",
            "100th Batch Loss: 1.4633 Batch Accuracy: 49.4531\n",
            "120th Batch Loss: 1.1153 Batch Accuracy: 49.8698\n",
            "140th Batch Loss: 1.5961 Batch Accuracy: 50.4688\n",
            "160th Batch Loss: 1.0964 Batch Accuracy: 50.8398\n",
            "180th Batch Loss: 1.0369 Batch Accuracy: 51.0764\n",
            "200th Batch Loss: 1.4127 Batch Accuracy: 51.4531\n",
            "220th Batch Loss: 1.0326 Batch Accuracy: 51.9318\n",
            "240th Batch Loss: 1.4370 Batch Accuracy: 52.1875\n",
            "260th Batch Loss: 1.1605 Batch Accuracy: 52.4399\n",
            "280th Batch Loss: 1.1006 Batch Accuracy: 52.5279\n",
            "300th Batch Loss: 1.2249 Batch Accuracy: 52.7135\n",
            "320th Batch Loss: 1.0952 Batch Accuracy: 53.0322\n",
            "340th Batch Loss: 1.1016 Batch Accuracy: 53.3456\n",
            "360th Batch Loss: 1.2861 Batch Accuracy: 53.5330\n",
            "380th Batch Loss: 0.8181 Batch Accuracy: 53.7171\n",
            "400th Batch Loss: 1.3026 Batch Accuracy: 53.8984\n",
            "420th Batch Loss: 1.1978 Batch Accuracy: 53.8356\n",
            "440th Batch Loss: 1.1076 Batch Accuracy: 53.9240\n",
            "460th Batch Loss: 1.1152 Batch Accuracy: 54.0014\n",
            "480th Batch Loss: 1.2791 Batch Accuracy: 53.9974\n",
            "500th Batch Loss: 1.0216 Batch Accuracy: 54.2750\n",
            "520th Batch Loss: 1.3148 Batch Accuracy: 54.2308\n",
            "540th Batch Loss: 0.8601 Batch Accuracy: 54.4358\n",
            "560th Batch Loss: 1.1715 Batch Accuracy: 54.5647\n",
            "580th Batch Loss: 1.1779 Batch Accuracy: 54.7144\n",
            "600th Batch Loss: 0.9963 Batch Accuracy: 54.8984\n",
            "620th Batch Loss: 1.4551 Batch Accuracy: 54.9017\n",
            "640th Batch Loss: 1.1285 Batch Accuracy: 55.1025\n",
            "660th Batch Loss: 1.2651 Batch Accuracy: 55.2604\n",
            "Epoch [2/10] Loss: 1.2214 Epoch Accuracy: 55.3588\n",
            "0\n",
            "Validation Accuracy: 61.31%\n",
            "20th Batch Loss: 1.1325 Batch Accuracy: 60.0000\n",
            "40th Batch Loss: 1.1947 Batch Accuracy: 57.4609\n",
            "60th Batch Loss: 1.4458 Batch Accuracy: 58.5677\n",
            "80th Batch Loss: 1.8854 Batch Accuracy: 58.8281\n",
            "100th Batch Loss: 1.1505 Batch Accuracy: 58.8750\n",
            "120th Batch Loss: 1.3902 Batch Accuracy: 59.3359\n",
            "140th Batch Loss: 1.2615 Batch Accuracy: 59.5089\n",
            "160th Batch Loss: 1.0050 Batch Accuracy: 59.4727\n",
            "180th Batch Loss: 1.1723 Batch Accuracy: 59.5747\n",
            "200th Batch Loss: 1.5552 Batch Accuracy: 59.6328\n",
            "220th Batch Loss: 1.4931 Batch Accuracy: 60.0852\n",
            "240th Batch Loss: 1.0767 Batch Accuracy: 60.0781\n",
            "260th Batch Loss: 0.9937 Batch Accuracy: 60.2404\n",
            "280th Batch Loss: 1.0284 Batch Accuracy: 60.5134\n",
            "300th Batch Loss: 0.9080 Batch Accuracy: 60.6354\n",
            "320th Batch Loss: 1.1117 Batch Accuracy: 60.5664\n",
            "340th Batch Loss: 1.0058 Batch Accuracy: 60.6572\n",
            "360th Batch Loss: 1.0505 Batch Accuracy: 60.6163\n",
            "380th Batch Loss: 0.9058 Batch Accuracy: 60.6867\n",
            "400th Batch Loss: 0.9608 Batch Accuracy: 60.7070\n",
            "420th Batch Loss: 0.9723 Batch Accuracy: 60.7552\n",
            "440th Batch Loss: 1.2088 Batch Accuracy: 60.9197\n",
            "460th Batch Loss: 0.8134 Batch Accuracy: 60.8152\n",
            "480th Batch Loss: 0.9995 Batch Accuracy: 60.9082\n",
            "500th Batch Loss: 1.1688 Batch Accuracy: 60.9937\n",
            "520th Batch Loss: 1.6534 Batch Accuracy: 60.9555\n",
            "540th Batch Loss: 0.9888 Batch Accuracy: 60.9635\n",
            "560th Batch Loss: 0.8857 Batch Accuracy: 60.9598\n",
            "580th Batch Loss: 1.0164 Batch Accuracy: 61.0695\n",
            "600th Batch Loss: 1.1727 Batch Accuracy: 61.0833\n",
            "620th Batch Loss: 1.0417 Batch Accuracy: 61.0459\n",
            "640th Batch Loss: 0.9113 Batch Accuracy: 61.2109\n",
            "660th Batch Loss: 1.1031 Batch Accuracy: 61.2429\n",
            "Epoch [3/10] Loss: 1.1349 Epoch Accuracy: 61.3588\n",
            "0\n",
            "Validation Accuracy: 68.19%\n",
            "20th Batch Loss: 1.2725 Batch Accuracy: 62.5781\n",
            "40th Batch Loss: 0.9527 Batch Accuracy: 63.8281\n",
            "60th Batch Loss: 1.5694 Batch Accuracy: 62.9427\n",
            "80th Batch Loss: 1.0357 Batch Accuracy: 62.9297\n",
            "100th Batch Loss: 0.8494 Batch Accuracy: 63.4844\n",
            "120th Batch Loss: 0.9956 Batch Accuracy: 64.0885\n",
            "140th Batch Loss: 0.6377 Batch Accuracy: 64.1071\n",
            "160th Batch Loss: 0.7193 Batch Accuracy: 64.0332\n",
            "180th Batch Loss: 1.0595 Batch Accuracy: 64.2274\n",
            "200th Batch Loss: 0.8702 Batch Accuracy: 64.2969\n",
            "220th Batch Loss: 1.1376 Batch Accuracy: 63.9134\n",
            "240th Batch Loss: 0.6814 Batch Accuracy: 64.1602\n",
            "260th Batch Loss: 1.3510 Batch Accuracy: 64.0385\n",
            "280th Batch Loss: 0.9049 Batch Accuracy: 63.9955\n",
            "300th Batch Loss: 1.2292 Batch Accuracy: 63.9062\n",
            "320th Batch Loss: 1.0227 Batch Accuracy: 63.8574\n",
            "340th Batch Loss: 1.0140 Batch Accuracy: 63.8925\n",
            "360th Batch Loss: 0.9754 Batch Accuracy: 64.0408\n",
            "380th Batch Loss: 0.8453 Batch Accuracy: 64.0913\n",
            "400th Batch Loss: 0.9846 Batch Accuracy: 64.1758\n",
            "420th Batch Loss: 1.2102 Batch Accuracy: 64.1592\n",
            "440th Batch Loss: 0.8159 Batch Accuracy: 64.2401\n",
            "460th Batch Loss: 1.2247 Batch Accuracy: 64.2833\n",
            "480th Batch Loss: 1.0259 Batch Accuracy: 64.5085\n",
            "500th Batch Loss: 1.0832 Batch Accuracy: 64.5594\n",
            "520th Batch Loss: 0.8341 Batch Accuracy: 64.6695\n",
            "540th Batch Loss: 1.1774 Batch Accuracy: 64.6557\n",
            "560th Batch Loss: 1.1339 Batch Accuracy: 64.5703\n",
            "580th Batch Loss: 0.8357 Batch Accuracy: 64.6121\n",
            "600th Batch Loss: 1.0556 Batch Accuracy: 64.7500\n",
            "620th Batch Loss: 0.9016 Batch Accuracy: 64.6976\n",
            "640th Batch Loss: 0.9783 Batch Accuracy: 64.7705\n",
            "660th Batch Loss: 1.3111 Batch Accuracy: 64.7206\n",
            "Epoch [4/10] Loss: 0.8082 Epoch Accuracy: 64.7083\n",
            "0\n",
            "Validation Accuracy: 70.29%\n",
            "20th Batch Loss: 0.7399 Batch Accuracy: 65.8594\n",
            "40th Batch Loss: 1.1841 Batch Accuracy: 66.7578\n",
            "60th Batch Loss: 1.0957 Batch Accuracy: 66.6146\n",
            "80th Batch Loss: 1.2520 Batch Accuracy: 66.5039\n",
            "100th Batch Loss: 0.7191 Batch Accuracy: 66.3125\n",
            "120th Batch Loss: 0.8637 Batch Accuracy: 66.6536\n",
            "140th Batch Loss: 1.1327 Batch Accuracy: 66.5067\n",
            "160th Batch Loss: 1.0227 Batch Accuracy: 66.6211\n",
            "180th Batch Loss: 1.1657 Batch Accuracy: 66.5799\n",
            "200th Batch Loss: 1.0613 Batch Accuracy: 66.6016\n",
            "220th Batch Loss: 0.9256 Batch Accuracy: 66.4347\n",
            "240th Batch Loss: 0.8234 Batch Accuracy: 66.5755\n",
            "260th Batch Loss: 0.8146 Batch Accuracy: 66.6346\n",
            "280th Batch Loss: 0.9721 Batch Accuracy: 66.6629\n",
            "300th Batch Loss: 1.1608 Batch Accuracy: 66.8073\n",
            "320th Batch Loss: 1.1243 Batch Accuracy: 66.7578\n",
            "340th Batch Loss: 0.7909 Batch Accuracy: 66.8015\n",
            "360th Batch Loss: 1.4573 Batch Accuracy: 66.6753\n",
            "380th Batch Loss: 0.9048 Batch Accuracy: 66.4885\n",
            "400th Batch Loss: 1.2184 Batch Accuracy: 66.5703\n",
            "420th Batch Loss: 0.6638 Batch Accuracy: 66.5774\n",
            "440th Batch Loss: 0.8214 Batch Accuracy: 66.6477\n",
            "460th Batch Loss: 1.1179 Batch Accuracy: 66.7765\n",
            "480th Batch Loss: 1.1074 Batch Accuracy: 66.8034\n",
            "500th Batch Loss: 1.1563 Batch Accuracy: 66.7344\n",
            "520th Batch Loss: 0.8310 Batch Accuracy: 66.7097\n",
            "540th Batch Loss: 0.9045 Batch Accuracy: 66.7448\n",
            "560th Batch Loss: 0.5893 Batch Accuracy: 66.7941\n",
            "580th Batch Loss: 0.7446 Batch Accuracy: 66.9154\n",
            "600th Batch Loss: 0.6943 Batch Accuracy: 67.0573\n",
            "620th Batch Loss: 1.1776 Batch Accuracy: 67.0086\n",
            "640th Batch Loss: 0.9136 Batch Accuracy: 66.9409\n",
            "660th Batch Loss: 0.7745 Batch Accuracy: 66.8963\n",
            "Epoch [5/10] Loss: 0.9766 Epoch Accuracy: 66.9236\n",
            "0\n",
            "Validation Accuracy: 73.15%\n",
            "20th Batch Loss: 0.8804 Batch Accuracy: 66.2500\n",
            "40th Batch Loss: 0.6625 Batch Accuracy: 68.5938\n",
            "60th Batch Loss: 0.9647 Batch Accuracy: 68.5677\n",
            "80th Batch Loss: 1.1140 Batch Accuracy: 68.4180\n",
            "100th Batch Loss: 0.7299 Batch Accuracy: 68.3438\n",
            "120th Batch Loss: 1.0366 Batch Accuracy: 68.0208\n",
            "140th Batch Loss: 0.7309 Batch Accuracy: 68.5268\n",
            "160th Batch Loss: 0.7762 Batch Accuracy: 68.7793\n",
            "180th Batch Loss: 0.8281 Batch Accuracy: 68.8021\n",
            "200th Batch Loss: 0.8263 Batch Accuracy: 68.6875\n",
            "220th Batch Loss: 0.6807 Batch Accuracy: 68.9134\n",
            "240th Batch Loss: 0.8225 Batch Accuracy: 69.0104\n",
            "260th Batch Loss: 0.9603 Batch Accuracy: 68.8341\n",
            "280th Batch Loss: 1.0423 Batch Accuracy: 68.7835\n",
            "300th Batch Loss: 0.9833 Batch Accuracy: 68.6979\n",
            "320th Batch Loss: 0.8272 Batch Accuracy: 68.8574\n",
            "340th Batch Loss: 0.6497 Batch Accuracy: 68.8649\n",
            "360th Batch Loss: 0.8896 Batch Accuracy: 68.8325\n",
            "380th Batch Loss: 0.5261 Batch Accuracy: 68.8322\n",
            "400th Batch Loss: 0.6606 Batch Accuracy: 68.9336\n",
            "420th Batch Loss: 0.5749 Batch Accuracy: 68.9658\n",
            "440th Batch Loss: 0.9095 Batch Accuracy: 68.8814\n",
            "460th Batch Loss: 0.8867 Batch Accuracy: 68.9640\n",
            "480th Batch Loss: 1.1844 Batch Accuracy: 68.8900\n",
            "500th Batch Loss: 0.6313 Batch Accuracy: 68.9062\n",
            "520th Batch Loss: 1.1757 Batch Accuracy: 68.9423\n",
            "540th Batch Loss: 0.7229 Batch Accuracy: 68.9410\n",
            "560th Batch Loss: 0.6246 Batch Accuracy: 68.8979\n",
            "580th Batch Loss: 0.8635 Batch Accuracy: 68.8712\n",
            "600th Batch Loss: 0.7838 Batch Accuracy: 68.8802\n",
            "620th Batch Loss: 0.8850 Batch Accuracy: 68.8710\n",
            "640th Batch Loss: 0.8554 Batch Accuracy: 68.8989\n",
            "660th Batch Loss: 0.7448 Batch Accuracy: 68.8849\n",
            "Epoch [6/10] Loss: 0.7121 Epoch Accuracy: 68.7917\n",
            "1\n",
            "Validation Accuracy: 71.90%\n",
            "20th Batch Loss: 0.7314 Batch Accuracy: 67.8125\n",
            "40th Batch Loss: 1.0996 Batch Accuracy: 68.6719\n",
            "60th Batch Loss: 1.1718 Batch Accuracy: 69.0104\n",
            "80th Batch Loss: 1.0084 Batch Accuracy: 69.1406\n",
            "100th Batch Loss: 0.6733 Batch Accuracy: 69.4531\n",
            "120th Batch Loss: 1.3420 Batch Accuracy: 69.6094\n",
            "140th Batch Loss: 0.5496 Batch Accuracy: 69.8326\n",
            "160th Batch Loss: 0.8271 Batch Accuracy: 69.7266\n",
            "180th Batch Loss: 1.0611 Batch Accuracy: 69.6267\n",
            "200th Batch Loss: 0.6336 Batch Accuracy: 69.4609\n",
            "220th Batch Loss: 0.8096 Batch Accuracy: 69.4105\n",
            "240th Batch Loss: 0.5733 Batch Accuracy: 69.5247\n",
            "260th Batch Loss: 1.2576 Batch Accuracy: 69.2608\n",
            "280th Batch Loss: 0.6667 Batch Accuracy: 69.4141\n",
            "300th Batch Loss: 0.7819 Batch Accuracy: 69.4323\n",
            "320th Batch Loss: 0.6175 Batch Accuracy: 69.6484\n",
            "340th Batch Loss: 0.5509 Batch Accuracy: 69.7702\n",
            "360th Batch Loss: 0.5798 Batch Accuracy: 69.9957\n",
            "380th Batch Loss: 0.9776 Batch Accuracy: 70.1398\n",
            "400th Batch Loss: 0.9345 Batch Accuracy: 70.1836\n",
            "420th Batch Loss: 0.4610 Batch Accuracy: 70.2381\n",
            "440th Batch Loss: 0.6737 Batch Accuracy: 70.3054\n",
            "460th Batch Loss: 0.6377 Batch Accuracy: 70.3397\n",
            "480th Batch Loss: 0.7210 Batch Accuracy: 70.4199\n",
            "500th Batch Loss: 0.8302 Batch Accuracy: 70.5031\n",
            "520th Batch Loss: 0.7556 Batch Accuracy: 70.4958\n",
            "540th Batch Loss: 0.5371 Batch Accuracy: 70.5035\n",
            "560th Batch Loss: 0.5681 Batch Accuracy: 70.5776\n",
            "580th Batch Loss: 0.7579 Batch Accuracy: 70.6142\n",
            "600th Batch Loss: 0.7028 Batch Accuracy: 70.5078\n",
            "620th Batch Loss: 0.7252 Batch Accuracy: 70.4612\n",
            "640th Batch Loss: 1.0384 Batch Accuracy: 70.4468\n",
            "660th Batch Loss: 0.6806 Batch Accuracy: 70.4522\n",
            "Epoch [7/10] Loss: 0.7941 Epoch Accuracy: 70.4444\n",
            "0\n",
            "Validation Accuracy: 73.65%\n",
            "20th Batch Loss: 0.8651 Batch Accuracy: 70.3906\n",
            "40th Batch Loss: 0.6331 Batch Accuracy: 70.8203\n",
            "60th Batch Loss: 1.0340 Batch Accuracy: 71.1458\n",
            "80th Batch Loss: 0.8641 Batch Accuracy: 70.9375\n",
            "100th Batch Loss: 0.8384 Batch Accuracy: 71.3906\n",
            "120th Batch Loss: 0.6934 Batch Accuracy: 71.3802\n",
            "140th Batch Loss: 0.8844 Batch Accuracy: 71.6741\n",
            "160th Batch Loss: 0.6386 Batch Accuracy: 71.7969\n",
            "180th Batch Loss: 0.5911 Batch Accuracy: 71.5191\n",
            "200th Batch Loss: 0.6933 Batch Accuracy: 71.6719\n",
            "220th Batch Loss: 1.0797 Batch Accuracy: 71.6335\n",
            "240th Batch Loss: 1.0634 Batch Accuracy: 71.8164\n",
            "260th Batch Loss: 0.7633 Batch Accuracy: 71.8630\n",
            "280th Batch Loss: 0.5674 Batch Accuracy: 71.9420\n",
            "300th Batch Loss: 0.6291 Batch Accuracy: 71.9531\n",
            "320th Batch Loss: 0.8983 Batch Accuracy: 71.8066\n",
            "340th Batch Loss: 0.5724 Batch Accuracy: 71.8612\n",
            "360th Batch Loss: 0.8025 Batch Accuracy: 71.6536\n",
            "380th Batch Loss: 0.6370 Batch Accuracy: 71.4803\n",
            "400th Batch Loss: 0.9695 Batch Accuracy: 71.6250\n",
            "420th Batch Loss: 0.6863 Batch Accuracy: 71.6629\n",
            "440th Batch Loss: 0.8614 Batch Accuracy: 71.8182\n",
            "460th Batch Loss: 0.5420 Batch Accuracy: 71.8105\n",
            "480th Batch Loss: 0.7667 Batch Accuracy: 71.8457\n",
            "500th Batch Loss: 0.4923 Batch Accuracy: 71.8281\n",
            "520th Batch Loss: 0.6994 Batch Accuracy: 71.7097\n",
            "540th Batch Loss: 0.8687 Batch Accuracy: 71.6753\n",
            "560th Batch Loss: 0.8856 Batch Accuracy: 71.6853\n",
            "580th Batch Loss: 0.5177 Batch Accuracy: 71.6972\n",
            "600th Batch Loss: 0.8006 Batch Accuracy: 71.5651\n",
            "620th Batch Loss: 0.6781 Batch Accuracy: 71.6381\n",
            "640th Batch Loss: 0.8162 Batch Accuracy: 71.6064\n",
            "660th Batch Loss: 0.8306 Batch Accuracy: 71.5388\n",
            "Epoch [8/10] Loss: 1.0188 Epoch Accuracy: 71.5903\n",
            "0\n",
            "Validation Accuracy: 77.21%\n",
            "20th Batch Loss: 0.6357 Batch Accuracy: 73.9062\n",
            "40th Batch Loss: 0.7133 Batch Accuracy: 72.5391\n",
            "60th Batch Loss: 0.9127 Batch Accuracy: 71.5104\n",
            "80th Batch Loss: 0.8421 Batch Accuracy: 71.8945\n",
            "100th Batch Loss: 0.5809 Batch Accuracy: 73.3125\n",
            "120th Batch Loss: 0.6266 Batch Accuracy: 72.8906\n",
            "140th Batch Loss: 0.7938 Batch Accuracy: 72.9241\n",
            "160th Batch Loss: 0.9584 Batch Accuracy: 72.8223\n",
            "180th Batch Loss: 0.6215 Batch Accuracy: 72.7778\n",
            "200th Batch Loss: 0.5566 Batch Accuracy: 73.0703\n",
            "220th Batch Loss: 1.3269 Batch Accuracy: 73.0114\n",
            "240th Batch Loss: 0.6280 Batch Accuracy: 73.1315\n",
            "260th Batch Loss: 0.7562 Batch Accuracy: 73.0409\n",
            "280th Batch Loss: 0.9383 Batch Accuracy: 73.0636\n",
            "300th Batch Loss: 0.5387 Batch Accuracy: 73.0417\n",
            "320th Batch Loss: 0.6883 Batch Accuracy: 72.8076\n",
            "340th Batch Loss: 0.6724 Batch Accuracy: 72.8722\n",
            "360th Batch Loss: 0.7693 Batch Accuracy: 72.9601\n",
            "380th Batch Loss: 0.8896 Batch Accuracy: 72.8084\n",
            "400th Batch Loss: 0.9049 Batch Accuracy: 72.6484\n",
            "420th Batch Loss: 0.6969 Batch Accuracy: 72.6339\n",
            "440th Batch Loss: 0.7923 Batch Accuracy: 72.7060\n",
            "460th Batch Loss: 0.6256 Batch Accuracy: 72.7514\n",
            "480th Batch Loss: 0.6260 Batch Accuracy: 72.7734\n",
            "500th Batch Loss: 0.5559 Batch Accuracy: 72.7594\n",
            "520th Batch Loss: 0.6030 Batch Accuracy: 72.8335\n",
            "540th Batch Loss: 0.7633 Batch Accuracy: 72.9022\n",
            "560th Batch Loss: 1.0977 Batch Accuracy: 72.8599\n",
            "580th Batch Loss: 0.7142 Batch Accuracy: 72.7963\n",
            "600th Batch Loss: 0.8825 Batch Accuracy: 72.7995\n",
            "620th Batch Loss: 0.7371 Batch Accuracy: 72.8352\n",
            "640th Batch Loss: 0.7332 Batch Accuracy: 72.9321\n",
            "660th Batch Loss: 1.2938 Batch Accuracy: 72.9451\n",
            "Epoch [9/10] Loss: 0.9064 Epoch Accuracy: 72.9398\n",
            "0\n",
            "Validation Accuracy: 77.33%\n",
            "20th Batch Loss: 0.9772 Batch Accuracy: 75.4688\n",
            "40th Batch Loss: 0.9029 Batch Accuracy: 73.9844\n",
            "60th Batch Loss: 0.6410 Batch Accuracy: 74.6615\n",
            "80th Batch Loss: 1.0206 Batch Accuracy: 73.1055\n",
            "100th Batch Loss: 0.8493 Batch Accuracy: 72.4219\n",
            "120th Batch Loss: 0.8923 Batch Accuracy: 72.2917\n",
            "140th Batch Loss: 0.6938 Batch Accuracy: 72.8348\n",
            "160th Batch Loss: 0.9438 Batch Accuracy: 72.8027\n",
            "180th Batch Loss: 0.8811 Batch Accuracy: 72.9080\n",
            "200th Batch Loss: 0.7063 Batch Accuracy: 72.7500\n",
            "220th Batch Loss: 0.5219 Batch Accuracy: 72.9688\n",
            "240th Batch Loss: 0.6069 Batch Accuracy: 72.9167\n",
            "260th Batch Loss: 0.5827 Batch Accuracy: 73.1851\n",
            "280th Batch Loss: 0.8232 Batch Accuracy: 73.0859\n",
            "300th Batch Loss: 0.7791 Batch Accuracy: 73.0990\n",
            "320th Batch Loss: 0.8131 Batch Accuracy: 73.1982\n",
            "340th Batch Loss: 0.7990 Batch Accuracy: 72.9733\n",
            "360th Batch Loss: 1.1526 Batch Accuracy: 72.9688\n",
            "380th Batch Loss: 0.7792 Batch Accuracy: 73.1373\n",
            "400th Batch Loss: 1.0682 Batch Accuracy: 73.0625\n",
            "420th Batch Loss: 0.5442 Batch Accuracy: 72.9725\n",
            "440th Batch Loss: 0.8611 Batch Accuracy: 72.9901\n",
            "460th Batch Loss: 0.4712 Batch Accuracy: 72.8702\n",
            "480th Batch Loss: 0.7653 Batch Accuracy: 72.9818\n",
            "500th Batch Loss: 0.6585 Batch Accuracy: 73.0625\n",
            "520th Batch Loss: 0.8746 Batch Accuracy: 73.1250\n",
            "540th Batch Loss: 0.7882 Batch Accuracy: 73.2089\n",
            "560th Batch Loss: 0.9012 Batch Accuracy: 73.1529\n",
            "580th Batch Loss: 0.8846 Batch Accuracy: 73.1034\n",
            "600th Batch Loss: 0.4681 Batch Accuracy: 73.1641\n",
            "620th Batch Loss: 0.5916 Batch Accuracy: 73.2031\n",
            "640th Batch Loss: 0.7341 Batch Accuracy: 73.2812\n",
            "660th Batch Loss: 1.0755 Batch Accuracy: 73.3594\n",
            "Epoch [10/10] Loss: 0.9752 Epoch Accuracy: 73.3056\n",
            "1\n",
            "Validation Accuracy: 75.25%\n",
            "Fold 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 2.1188 Batch Accuracy: 19.6094\n",
            "40th Batch Loss: 2.2026 Batch Accuracy: 23.4375\n",
            "60th Batch Loss: 2.0386 Batch Accuracy: 26.6927\n",
            "80th Batch Loss: 1.7685 Batch Accuracy: 28.3008\n",
            "100th Batch Loss: 1.7875 Batch Accuracy: 29.8906\n",
            "120th Batch Loss: 1.7373 Batch Accuracy: 31.1589\n",
            "140th Batch Loss: 1.6983 Batch Accuracy: 32.2879\n",
            "160th Batch Loss: 1.6247 Batch Accuracy: 33.3398\n",
            "180th Batch Loss: 1.7404 Batch Accuracy: 33.9670\n",
            "200th Batch Loss: 1.4221 Batch Accuracy: 34.5234\n",
            "220th Batch Loss: 1.9029 Batch Accuracy: 35.2202\n",
            "240th Batch Loss: 1.4830 Batch Accuracy: 35.8268\n",
            "260th Batch Loss: 1.3839 Batch Accuracy: 36.1959\n",
            "280th Batch Loss: 1.6980 Batch Accuracy: 36.9364\n",
            "300th Batch Loss: 1.7003 Batch Accuracy: 37.4792\n",
            "320th Batch Loss: 1.3932 Batch Accuracy: 38.0127\n",
            "340th Batch Loss: 1.5975 Batch Accuracy: 38.5018\n",
            "360th Batch Loss: 1.3461 Batch Accuracy: 38.7587\n",
            "380th Batch Loss: 1.7119 Batch Accuracy: 39.2270\n",
            "400th Batch Loss: 1.8222 Batch Accuracy: 39.7656\n",
            "420th Batch Loss: 1.3702 Batch Accuracy: 40.0335\n",
            "440th Batch Loss: 1.6342 Batch Accuracy: 40.4616\n",
            "460th Batch Loss: 1.3382 Batch Accuracy: 40.9035\n",
            "480th Batch Loss: 1.1388 Batch Accuracy: 41.2044\n",
            "500th Batch Loss: 1.5221 Batch Accuracy: 41.6219\n",
            "520th Batch Loss: 1.5930 Batch Accuracy: 42.0553\n",
            "540th Batch Loss: 1.3801 Batch Accuracy: 42.3409\n",
            "560th Batch Loss: 1.1973 Batch Accuracy: 42.6535\n",
            "580th Batch Loss: 0.9394 Batch Accuracy: 42.9741\n",
            "600th Batch Loss: 1.1619 Batch Accuracy: 43.3307\n",
            "620th Batch Loss: 1.0104 Batch Accuracy: 43.5660\n",
            "640th Batch Loss: 1.4835 Batch Accuracy: 43.8452\n",
            "660th Batch Loss: 1.4864 Batch Accuracy: 44.0885\n",
            "Epoch [1/10] Loss: 1.3519 Epoch Accuracy: 44.3171\n",
            "0\n",
            "Validation Accuracy: 57.88%\n",
            "20th Batch Loss: 1.2648 Batch Accuracy: 46.4844\n",
            "40th Batch Loss: 1.4250 Batch Accuracy: 48.5938\n",
            "60th Batch Loss: 1.2932 Batch Accuracy: 48.3073\n",
            "80th Batch Loss: 1.3793 Batch Accuracy: 48.7891\n",
            "100th Batch Loss: 1.5678 Batch Accuracy: 49.0938\n",
            "120th Batch Loss: 1.0176 Batch Accuracy: 49.5964\n",
            "140th Batch Loss: 1.1681 Batch Accuracy: 50.5580\n",
            "160th Batch Loss: 1.0650 Batch Accuracy: 51.2500\n",
            "180th Batch Loss: 1.5647 Batch Accuracy: 51.5365\n",
            "200th Batch Loss: 1.4502 Batch Accuracy: 51.8984\n",
            "220th Batch Loss: 1.0347 Batch Accuracy: 52.1733\n",
            "240th Batch Loss: 1.5011 Batch Accuracy: 52.1875\n",
            "260th Batch Loss: 1.2948 Batch Accuracy: 52.2055\n",
            "280th Batch Loss: 1.1683 Batch Accuracy: 52.6172\n",
            "300th Batch Loss: 1.0517 Batch Accuracy: 53.1875\n",
            "320th Batch Loss: 1.3709 Batch Accuracy: 53.3887\n",
            "340th Batch Loss: 0.9097 Batch Accuracy: 53.5570\n",
            "360th Batch Loss: 0.9552 Batch Accuracy: 53.7847\n",
            "380th Batch Loss: 1.1513 Batch Accuracy: 54.0378\n",
            "400th Batch Loss: 1.1560 Batch Accuracy: 54.1406\n",
            "420th Batch Loss: 0.9182 Batch Accuracy: 54.2894\n",
            "440th Batch Loss: 1.1112 Batch Accuracy: 54.4602\n",
            "460th Batch Loss: 1.4437 Batch Accuracy: 54.6060\n",
            "480th Batch Loss: 1.0639 Batch Accuracy: 54.7070\n",
            "500th Batch Loss: 1.2452 Batch Accuracy: 54.6219\n",
            "520th Batch Loss: 1.1904 Batch Accuracy: 54.7446\n",
            "540th Batch Loss: 1.2415 Batch Accuracy: 54.9884\n",
            "560th Batch Loss: 1.1323 Batch Accuracy: 55.1535\n",
            "580th Batch Loss: 1.1880 Batch Accuracy: 55.2802\n",
            "600th Batch Loss: 1.0656 Batch Accuracy: 55.4089\n",
            "620th Batch Loss: 1.1276 Batch Accuracy: 55.4612\n",
            "640th Batch Loss: 1.3443 Batch Accuracy: 55.5737\n",
            "660th Batch Loss: 1.0756 Batch Accuracy: 55.7315\n",
            "Epoch [2/10] Loss: 0.8472 Epoch Accuracy: 55.8542\n",
            "0\n",
            "Validation Accuracy: 66.85%\n",
            "20th Batch Loss: 0.7178 Batch Accuracy: 62.7344\n",
            "40th Batch Loss: 1.1519 Batch Accuracy: 62.0703\n",
            "60th Batch Loss: 0.9304 Batch Accuracy: 62.0052\n",
            "80th Batch Loss: 0.7397 Batch Accuracy: 62.4805\n",
            "100th Batch Loss: 1.4282 Batch Accuracy: 61.9844\n",
            "120th Batch Loss: 0.8268 Batch Accuracy: 62.0312\n",
            "140th Batch Loss: 1.1113 Batch Accuracy: 62.0536\n",
            "160th Batch Loss: 0.7593 Batch Accuracy: 62.1582\n",
            "180th Batch Loss: 1.1546 Batch Accuracy: 62.0747\n",
            "200th Batch Loss: 0.8066 Batch Accuracy: 62.0156\n",
            "220th Batch Loss: 0.8171 Batch Accuracy: 62.1236\n",
            "240th Batch Loss: 1.3203 Batch Accuracy: 61.7578\n",
            "260th Batch Loss: 0.9814 Batch Accuracy: 61.5325\n",
            "280th Batch Loss: 1.2874 Batch Accuracy: 61.6797\n",
            "300th Batch Loss: 1.0436 Batch Accuracy: 61.6458\n",
            "320th Batch Loss: 0.7674 Batch Accuracy: 61.8018\n",
            "340th Batch Loss: 0.9984 Batch Accuracy: 61.8015\n",
            "360th Batch Loss: 1.2236 Batch Accuracy: 61.9748\n",
            "380th Batch Loss: 0.7828 Batch Accuracy: 62.0806\n",
            "400th Batch Loss: 0.7040 Batch Accuracy: 62.1211\n",
            "420th Batch Loss: 1.2252 Batch Accuracy: 62.0982\n",
            "440th Batch Loss: 0.9055 Batch Accuracy: 62.2266\n",
            "460th Batch Loss: 0.9687 Batch Accuracy: 62.3641\n",
            "480th Batch Loss: 0.9150 Batch Accuracy: 62.3177\n",
            "500th Batch Loss: 1.2555 Batch Accuracy: 62.3656\n",
            "520th Batch Loss: 0.9955 Batch Accuracy: 62.4099\n",
            "540th Batch Loss: 1.3735 Batch Accuracy: 62.5203\n",
            "560th Batch Loss: 0.8916 Batch Accuracy: 62.5558\n",
            "580th Batch Loss: 1.1249 Batch Accuracy: 62.5485\n",
            "600th Batch Loss: 0.9489 Batch Accuracy: 62.5677\n",
            "620th Batch Loss: 0.8787 Batch Accuracy: 62.6537\n",
            "640th Batch Loss: 1.2447 Batch Accuracy: 62.6782\n",
            "660th Batch Loss: 1.0944 Batch Accuracy: 62.8527\n",
            "Epoch [3/10] Loss: 0.6769 Epoch Accuracy: 62.8866\n",
            "0\n",
            "Validation Accuracy: 68.92%\n",
            "20th Batch Loss: 1.3892 Batch Accuracy: 64.1406\n",
            "40th Batch Loss: 1.0637 Batch Accuracy: 63.7109\n",
            "60th Batch Loss: 0.8100 Batch Accuracy: 63.2031\n",
            "80th Batch Loss: 1.3179 Batch Accuracy: 63.3789\n",
            "100th Batch Loss: 0.9863 Batch Accuracy: 63.1875\n",
            "120th Batch Loss: 1.1594 Batch Accuracy: 63.7370\n",
            "140th Batch Loss: 0.8380 Batch Accuracy: 63.9732\n",
            "160th Batch Loss: 0.9337 Batch Accuracy: 63.9844\n",
            "180th Batch Loss: 1.0618 Batch Accuracy: 64.0972\n",
            "200th Batch Loss: 0.9379 Batch Accuracy: 64.1250\n",
            "220th Batch Loss: 1.0022 Batch Accuracy: 64.1690\n",
            "240th Batch Loss: 0.7234 Batch Accuracy: 64.1732\n",
            "260th Batch Loss: 1.0301 Batch Accuracy: 64.3089\n",
            "280th Batch Loss: 0.9477 Batch Accuracy: 64.2020\n",
            "300th Batch Loss: 1.1940 Batch Accuracy: 64.4427\n",
            "320th Batch Loss: 1.1070 Batch Accuracy: 64.4922\n",
            "340th Batch Loss: 1.2849 Batch Accuracy: 64.6048\n",
            "360th Batch Loss: 1.1514 Batch Accuracy: 64.6441\n",
            "380th Batch Loss: 0.9668 Batch Accuracy: 64.6423\n",
            "400th Batch Loss: 1.3748 Batch Accuracy: 64.7109\n",
            "420th Batch Loss: 0.7642 Batch Accuracy: 64.8624\n",
            "440th Batch Loss: 0.7871 Batch Accuracy: 64.6413\n",
            "460th Batch Loss: 0.8642 Batch Accuracy: 64.6264\n",
            "480th Batch Loss: 0.8096 Batch Accuracy: 64.6517\n",
            "500th Batch Loss: 1.1294 Batch Accuracy: 64.7531\n",
            "520th Batch Loss: 0.8969 Batch Accuracy: 64.8377\n",
            "540th Batch Loss: 1.0211 Batch Accuracy: 64.9334\n",
            "560th Batch Loss: 0.8447 Batch Accuracy: 65.0223\n",
            "580th Batch Loss: 0.8831 Batch Accuracy: 65.0647\n",
            "600th Batch Loss: 1.1742 Batch Accuracy: 65.0964\n",
            "620th Batch Loss: 0.7145 Batch Accuracy: 65.1512\n",
            "640th Batch Loss: 0.7866 Batch Accuracy: 65.2515\n",
            "660th Batch Loss: 1.1918 Batch Accuracy: 65.2202\n",
            "Epoch [4/10] Loss: 1.0953 Epoch Accuracy: 65.1968\n",
            "0\n",
            "Validation Accuracy: 73.02%\n",
            "20th Batch Loss: 0.9483 Batch Accuracy: 67.4219\n",
            "40th Batch Loss: 0.7449 Batch Accuracy: 67.8516\n",
            "60th Batch Loss: 0.8789 Batch Accuracy: 67.1615\n",
            "80th Batch Loss: 0.7308 Batch Accuracy: 67.5195\n",
            "100th Batch Loss: 0.8501 Batch Accuracy: 68.0469\n",
            "120th Batch Loss: 1.1194 Batch Accuracy: 67.4870\n",
            "140th Batch Loss: 0.9836 Batch Accuracy: 66.8304\n",
            "160th Batch Loss: 0.7566 Batch Accuracy: 66.9043\n",
            "180th Batch Loss: 0.9889 Batch Accuracy: 67.0052\n",
            "200th Batch Loss: 1.2544 Batch Accuracy: 67.2109\n",
            "220th Batch Loss: 0.8186 Batch Accuracy: 67.0099\n",
            "240th Batch Loss: 1.0054 Batch Accuracy: 67.0378\n",
            "260th Batch Loss: 1.0165 Batch Accuracy: 66.8750\n",
            "280th Batch Loss: 0.7549 Batch Accuracy: 67.0536\n",
            "300th Batch Loss: 0.7134 Batch Accuracy: 67.0521\n",
            "320th Batch Loss: 0.9723 Batch Accuracy: 66.8604\n",
            "340th Batch Loss: 0.9482 Batch Accuracy: 66.8566\n",
            "360th Batch Loss: 1.2884 Batch Accuracy: 66.9141\n",
            "380th Batch Loss: 0.7741 Batch Accuracy: 66.7681\n",
            "400th Batch Loss: 0.9269 Batch Accuracy: 66.9062\n",
            "420th Batch Loss: 0.7383 Batch Accuracy: 66.8341\n",
            "440th Batch Loss: 0.8981 Batch Accuracy: 66.7152\n",
            "460th Batch Loss: 1.0982 Batch Accuracy: 66.7289\n",
            "480th Batch Loss: 0.8003 Batch Accuracy: 66.7643\n",
            "500th Batch Loss: 1.0095 Batch Accuracy: 66.7281\n",
            "520th Batch Loss: 0.6856 Batch Accuracy: 66.7668\n",
            "540th Batch Loss: 0.8079 Batch Accuracy: 66.8519\n",
            "560th Batch Loss: 0.8901 Batch Accuracy: 66.9420\n",
            "580th Batch Loss: 1.3279 Batch Accuracy: 66.9450\n",
            "600th Batch Loss: 1.0166 Batch Accuracy: 67.0104\n",
            "620th Batch Loss: 0.8402 Batch Accuracy: 67.1673\n",
            "640th Batch Loss: 0.8426 Batch Accuracy: 67.2168\n",
            "660th Batch Loss: 0.7658 Batch Accuracy: 67.3035\n",
            "Epoch [5/10] Loss: 0.6862 Epoch Accuracy: 67.2986\n",
            "0\n",
            "Validation Accuracy: 73.56%\n",
            "20th Batch Loss: 1.1041 Batch Accuracy: 68.5938\n",
            "40th Batch Loss: 0.8232 Batch Accuracy: 68.6328\n",
            "60th Batch Loss: 0.7577 Batch Accuracy: 68.7760\n",
            "80th Batch Loss: 0.7077 Batch Accuracy: 68.8086\n",
            "100th Batch Loss: 0.8690 Batch Accuracy: 69.1250\n",
            "120th Batch Loss: 0.9952 Batch Accuracy: 69.3620\n",
            "140th Batch Loss: 1.3405 Batch Accuracy: 69.4978\n",
            "160th Batch Loss: 0.8724 Batch Accuracy: 69.2188\n",
            "180th Batch Loss: 0.8876 Batch Accuracy: 69.4184\n",
            "200th Batch Loss: 0.6618 Batch Accuracy: 69.5469\n",
            "220th Batch Loss: 0.5581 Batch Accuracy: 69.7869\n",
            "240th Batch Loss: 0.7490 Batch Accuracy: 69.5052\n",
            "260th Batch Loss: 0.9714 Batch Accuracy: 69.3870\n",
            "280th Batch Loss: 0.6984 Batch Accuracy: 69.4643\n",
            "300th Batch Loss: 0.8273 Batch Accuracy: 69.4062\n",
            "320th Batch Loss: 0.8240 Batch Accuracy: 69.2236\n",
            "340th Batch Loss: 0.8811 Batch Accuracy: 69.1544\n",
            "360th Batch Loss: 0.6908 Batch Accuracy: 69.3316\n",
            "380th Batch Loss: 0.7349 Batch Accuracy: 69.3339\n",
            "400th Batch Loss: 0.7436 Batch Accuracy: 69.3281\n",
            "420th Batch Loss: 0.8384 Batch Accuracy: 69.2485\n",
            "440th Batch Loss: 1.0732 Batch Accuracy: 69.1655\n",
            "460th Batch Loss: 1.0819 Batch Accuracy: 69.1508\n",
            "480th Batch Loss: 0.6423 Batch Accuracy: 69.0755\n",
            "500th Batch Loss: 0.5936 Batch Accuracy: 69.1000\n",
            "520th Batch Loss: 0.9263 Batch Accuracy: 69.1346\n",
            "540th Batch Loss: 1.0566 Batch Accuracy: 69.1348\n",
            "560th Batch Loss: 0.5365 Batch Accuracy: 69.2020\n",
            "580th Batch Loss: 1.2500 Batch Accuracy: 69.2726\n",
            "600th Batch Loss: 0.6330 Batch Accuracy: 69.3776\n",
            "620th Batch Loss: 0.8265 Batch Accuracy: 69.3196\n",
            "640th Batch Loss: 0.6862 Batch Accuracy: 69.3726\n",
            "660th Batch Loss: 0.7602 Batch Accuracy: 69.4342\n",
            "Epoch [6/10] Loss: 0.8207 Epoch Accuracy: 69.3773\n",
            "0\n",
            "Validation Accuracy: 75.77%\n",
            "20th Batch Loss: 0.4358 Batch Accuracy: 70.5469\n",
            "40th Batch Loss: 0.5002 Batch Accuracy: 70.4297\n",
            "60th Batch Loss: 1.0305 Batch Accuracy: 70.1823\n",
            "80th Batch Loss: 1.3627 Batch Accuracy: 70.1758\n",
            "100th Batch Loss: 0.7536 Batch Accuracy: 70.0000\n",
            "120th Batch Loss: 0.7018 Batch Accuracy: 70.1562\n",
            "140th Batch Loss: 0.7858 Batch Accuracy: 69.9107\n",
            "160th Batch Loss: 0.9580 Batch Accuracy: 69.9414\n",
            "180th Batch Loss: 0.7028 Batch Accuracy: 70.2778\n",
            "200th Batch Loss: 0.7811 Batch Accuracy: 70.4766\n",
            "220th Batch Loss: 1.1754 Batch Accuracy: 70.2060\n",
            "240th Batch Loss: 0.8942 Batch Accuracy: 70.3776\n",
            "260th Batch Loss: 1.2637 Batch Accuracy: 70.3065\n",
            "280th Batch Loss: 0.7595 Batch Accuracy: 70.3739\n",
            "300th Batch Loss: 0.9978 Batch Accuracy: 70.3490\n",
            "320th Batch Loss: 1.0791 Batch Accuracy: 70.2344\n",
            "340th Batch Loss: 0.9276 Batch Accuracy: 70.1930\n",
            "360th Batch Loss: 0.9096 Batch Accuracy: 70.1389\n",
            "380th Batch Loss: 0.7166 Batch Accuracy: 70.0863\n",
            "400th Batch Loss: 0.7829 Batch Accuracy: 70.1055\n",
            "420th Batch Loss: 0.6292 Batch Accuracy: 70.2269\n",
            "440th Batch Loss: 0.9444 Batch Accuracy: 70.2450\n",
            "460th Batch Loss: 0.5869 Batch Accuracy: 70.4348\n",
            "480th Batch Loss: 0.6153 Batch Accuracy: 70.4525\n",
            "500th Batch Loss: 0.6580 Batch Accuracy: 70.4813\n",
            "520th Batch Loss: 0.7717 Batch Accuracy: 70.4718\n",
            "540th Batch Loss: 1.2680 Batch Accuracy: 70.4630\n",
            "560th Batch Loss: 1.0418 Batch Accuracy: 70.3627\n",
            "580th Batch Loss: 1.0023 Batch Accuracy: 70.3718\n",
            "600th Batch Loss: 1.0715 Batch Accuracy: 70.2708\n",
            "620th Batch Loss: 1.1157 Batch Accuracy: 70.2898\n",
            "640th Batch Loss: 1.0379 Batch Accuracy: 70.3271\n",
            "660th Batch Loss: 0.7189 Batch Accuracy: 70.2888\n",
            "Epoch [7/10] Loss: 0.9508 Epoch Accuracy: 70.2986\n",
            "1\n",
            "Validation Accuracy: 74.69%\n",
            "20th Batch Loss: 0.8769 Batch Accuracy: 70.5469\n",
            "40th Batch Loss: 1.1587 Batch Accuracy: 70.1953\n",
            "60th Batch Loss: 0.6033 Batch Accuracy: 71.0156\n",
            "80th Batch Loss: 0.8211 Batch Accuracy: 71.4844\n",
            "100th Batch Loss: 0.8944 Batch Accuracy: 72.1875\n",
            "120th Batch Loss: 0.8539 Batch Accuracy: 72.1224\n",
            "140th Batch Loss: 1.2823 Batch Accuracy: 71.9420\n",
            "160th Batch Loss: 0.6312 Batch Accuracy: 72.1680\n",
            "180th Batch Loss: 0.8530 Batch Accuracy: 72.3524\n",
            "200th Batch Loss: 0.7607 Batch Accuracy: 72.5000\n",
            "220th Batch Loss: 1.0824 Batch Accuracy: 72.6349\n",
            "240th Batch Loss: 0.8772 Batch Accuracy: 72.6888\n",
            "260th Batch Loss: 0.7698 Batch Accuracy: 72.4159\n",
            "280th Batch Loss: 0.9602 Batch Accuracy: 72.2042\n",
            "300th Batch Loss: 0.7007 Batch Accuracy: 72.1771\n",
            "320th Batch Loss: 0.8857 Batch Accuracy: 72.0508\n",
            "340th Batch Loss: 0.5480 Batch Accuracy: 71.9439\n",
            "360th Batch Loss: 0.6196 Batch Accuracy: 71.9575\n",
            "380th Batch Loss: 0.8401 Batch Accuracy: 71.8873\n",
            "400th Batch Loss: 1.2736 Batch Accuracy: 71.8633\n",
            "420th Batch Loss: 0.7246 Batch Accuracy: 71.9196\n",
            "440th Batch Loss: 0.9841 Batch Accuracy: 71.8999\n",
            "460th Batch Loss: 0.8371 Batch Accuracy: 71.9124\n",
            "480th Batch Loss: 0.6411 Batch Accuracy: 72.0475\n",
            "500th Batch Loss: 0.6704 Batch Accuracy: 71.9469\n",
            "520th Batch Loss: 0.9783 Batch Accuracy: 71.9291\n",
            "540th Batch Loss: 0.9092 Batch Accuracy: 71.9068\n",
            "560th Batch Loss: 0.9182 Batch Accuracy: 71.9392\n",
            "580th Batch Loss: 0.8669 Batch Accuracy: 71.8373\n",
            "600th Batch Loss: 0.7290 Batch Accuracy: 71.7891\n",
            "620th Batch Loss: 0.6407 Batch Accuracy: 71.7818\n",
            "640th Batch Loss: 0.8686 Batch Accuracy: 71.7798\n",
            "660th Batch Loss: 0.8594 Batch Accuracy: 71.7306\n",
            "Epoch [8/10] Loss: 0.6455 Epoch Accuracy: 71.7269\n",
            "0\n",
            "Validation Accuracy: 78.12%\n",
            "20th Batch Loss: 0.8574 Batch Accuracy: 72.8906\n",
            "40th Batch Loss: 0.5477 Batch Accuracy: 72.9297\n",
            "60th Batch Loss: 0.7615 Batch Accuracy: 73.0469\n",
            "80th Batch Loss: 0.7874 Batch Accuracy: 73.3789\n",
            "100th Batch Loss: 0.8760 Batch Accuracy: 72.9531\n",
            "120th Batch Loss: 0.6334 Batch Accuracy: 72.8516\n",
            "140th Batch Loss: 0.5045 Batch Accuracy: 72.8683\n",
            "160th Batch Loss: 0.7235 Batch Accuracy: 72.7832\n",
            "180th Batch Loss: 1.0522 Batch Accuracy: 72.9080\n",
            "200th Batch Loss: 0.8851 Batch Accuracy: 72.7891\n",
            "220th Batch Loss: 0.6916 Batch Accuracy: 72.7628\n",
            "240th Batch Loss: 0.9058 Batch Accuracy: 72.7799\n",
            "260th Batch Loss: 0.8186 Batch Accuracy: 72.8786\n",
            "280th Batch Loss: 1.1183 Batch Accuracy: 72.8181\n",
            "300th Batch Loss: 0.9591 Batch Accuracy: 72.7344\n",
            "320th Batch Loss: 0.8407 Batch Accuracy: 72.6514\n",
            "340th Batch Loss: 0.7548 Batch Accuracy: 72.4127\n",
            "360th Batch Loss: 0.8392 Batch Accuracy: 72.4696\n",
            "380th Batch Loss: 0.6096 Batch Accuracy: 72.6192\n",
            "400th Batch Loss: 0.9166 Batch Accuracy: 72.6250\n",
            "420th Batch Loss: 0.9137 Batch Accuracy: 72.5930\n",
            "440th Batch Loss: 0.7027 Batch Accuracy: 72.6385\n",
            "460th Batch Loss: 0.9243 Batch Accuracy: 72.5374\n",
            "480th Batch Loss: 0.9248 Batch Accuracy: 72.6172\n",
            "500th Batch Loss: 0.5896 Batch Accuracy: 72.6188\n",
            "520th Batch Loss: 0.5258 Batch Accuracy: 72.6893\n",
            "540th Batch Loss: 0.6722 Batch Accuracy: 72.8328\n",
            "560th Batch Loss: 1.0405 Batch Accuracy: 72.7818\n",
            "580th Batch Loss: 1.0730 Batch Accuracy: 72.7317\n",
            "600th Batch Loss: 0.7706 Batch Accuracy: 72.8385\n",
            "620th Batch Loss: 0.7112 Batch Accuracy: 72.9007\n",
            "640th Batch Loss: 0.5038 Batch Accuracy: 72.9272\n",
            "660th Batch Loss: 0.9685 Batch Accuracy: 72.9285\n",
            "Epoch [9/10] Loss: 0.7185 Epoch Accuracy: 72.9769\n",
            "1\n",
            "Validation Accuracy: 77.21%\n",
            "20th Batch Loss: 0.7835 Batch Accuracy: 73.9844\n",
            "40th Batch Loss: 0.6226 Batch Accuracy: 75.0391\n",
            "60th Batch Loss: 0.4378 Batch Accuracy: 75.0000\n",
            "80th Batch Loss: 1.0773 Batch Accuracy: 73.9453\n",
            "100th Batch Loss: 0.7227 Batch Accuracy: 74.5625\n",
            "120th Batch Loss: 1.1648 Batch Accuracy: 74.6615\n",
            "140th Batch Loss: 1.0372 Batch Accuracy: 74.3192\n",
            "160th Batch Loss: 0.6562 Batch Accuracy: 74.5117\n",
            "180th Batch Loss: 0.7612 Batch Accuracy: 74.6267\n",
            "200th Batch Loss: 0.9336 Batch Accuracy: 74.4219\n",
            "220th Batch Loss: 0.7661 Batch Accuracy: 74.1477\n",
            "240th Batch Loss: 1.1532 Batch Accuracy: 74.3945\n",
            "260th Batch Loss: 0.7587 Batch Accuracy: 74.3029\n",
            "280th Batch Loss: 1.1173 Batch Accuracy: 74.3806\n",
            "300th Batch Loss: 0.6489 Batch Accuracy: 74.4010\n",
            "320th Batch Loss: 0.8780 Batch Accuracy: 74.4678\n",
            "340th Batch Loss: 0.7619 Batch Accuracy: 74.5496\n",
            "360th Batch Loss: 0.7910 Batch Accuracy: 74.3576\n",
            "380th Batch Loss: 0.6075 Batch Accuracy: 74.3462\n",
            "400th Batch Loss: 0.4577 Batch Accuracy: 74.3008\n",
            "420th Batch Loss: 0.7781 Batch Accuracy: 74.2597\n",
            "440th Batch Loss: 0.4880 Batch Accuracy: 74.1726\n",
            "460th Batch Loss: 0.8061 Batch Accuracy: 74.1236\n",
            "480th Batch Loss: 0.7809 Batch Accuracy: 74.2155\n",
            "500th Batch Loss: 0.6810 Batch Accuracy: 74.1594\n",
            "520th Batch Loss: 0.7371 Batch Accuracy: 74.1076\n",
            "540th Batch Loss: 0.8940 Batch Accuracy: 74.1117\n",
            "560th Batch Loss: 0.7454 Batch Accuracy: 74.0765\n",
            "580th Batch Loss: 0.8502 Batch Accuracy: 74.1002\n",
            "600th Batch Loss: 0.6300 Batch Accuracy: 74.0990\n",
            "620th Batch Loss: 0.6353 Batch Accuracy: 74.0449\n",
            "640th Batch Loss: 1.1905 Batch Accuracy: 73.9429\n",
            "660th Batch Loss: 0.6147 Batch Accuracy: 73.9583\n",
            "Epoch [10/10] Loss: 0.7786 Epoch Accuracy: 73.9329\n",
            "0\n",
            "Validation Accuracy: 78.50%\n",
            "Fold 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20th Batch Loss: 1.9922 Batch Accuracy: 18.9844\n",
            "40th Batch Loss: 1.8407 Batch Accuracy: 23.8281\n",
            "60th Batch Loss: 1.5453 Batch Accuracy: 26.5885\n",
            "80th Batch Loss: 1.8907 Batch Accuracy: 28.7500\n",
            "100th Batch Loss: 1.6059 Batch Accuracy: 30.2344\n",
            "120th Batch Loss: 1.7699 Batch Accuracy: 30.7292\n",
            "140th Batch Loss: 1.6722 Batch Accuracy: 31.5625\n",
            "160th Batch Loss: 1.7009 Batch Accuracy: 32.0117\n",
            "180th Batch Loss: 1.5569 Batch Accuracy: 32.9167\n",
            "200th Batch Loss: 1.7286 Batch Accuracy: 33.5625\n",
            "220th Batch Loss: 1.6428 Batch Accuracy: 34.0909\n",
            "240th Batch Loss: 1.5875 Batch Accuracy: 34.7526\n",
            "260th Batch Loss: 1.5934 Batch Accuracy: 35.3005\n",
            "280th Batch Loss: 1.5733 Batch Accuracy: 35.9431\n",
            "300th Batch Loss: 1.2503 Batch Accuracy: 36.4271\n",
            "320th Batch Loss: 1.2818 Batch Accuracy: 37.1680\n",
            "340th Batch Loss: 1.2581 Batch Accuracy: 37.7803\n",
            "360th Batch Loss: 1.3260 Batch Accuracy: 38.4071\n",
            "380th Batch Loss: 1.3808 Batch Accuracy: 38.9309\n",
            "400th Batch Loss: 1.5208 Batch Accuracy: 39.4141\n",
            "420th Batch Loss: 1.5732 Batch Accuracy: 39.8735\n",
            "440th Batch Loss: 1.5638 Batch Accuracy: 40.2734\n",
            "460th Batch Loss: 1.1410 Batch Accuracy: 40.6624\n",
            "480th Batch Loss: 1.4304 Batch Accuracy: 41.0645\n",
            "500th Batch Loss: 1.0134 Batch Accuracy: 41.5031\n",
            "520th Batch Loss: 1.1885 Batch Accuracy: 41.8419\n",
            "540th Batch Loss: 1.2094 Batch Accuracy: 42.2135\n",
            "560th Batch Loss: 1.2368 Batch Accuracy: 42.4721\n",
            "580th Batch Loss: 1.5813 Batch Accuracy: 42.7829\n",
            "600th Batch Loss: 1.3835 Batch Accuracy: 43.1172\n",
            "620th Batch Loss: 1.1187 Batch Accuracy: 43.5509\n",
            "640th Batch Loss: 1.7225 Batch Accuracy: 43.8306\n",
            "660th Batch Loss: 1.3161 Batch Accuracy: 43.9844\n",
            "Epoch [1/10] Loss: 1.3510 Epoch Accuracy: 44.1574\n",
            "0\n",
            "Validation Accuracy: 55.08%\n",
            "20th Batch Loss: 1.2897 Batch Accuracy: 42.5781\n",
            "40th Batch Loss: 1.4093 Batch Accuracy: 45.3516\n",
            "60th Batch Loss: 1.4777 Batch Accuracy: 44.6094\n",
            "80th Batch Loss: 1.2507 Batch Accuracy: 44.7070\n",
            "100th Batch Loss: 1.2656 Batch Accuracy: 46.0312\n",
            "120th Batch Loss: 1.1331 Batch Accuracy: 47.5130\n",
            "140th Batch Loss: 1.2954 Batch Accuracy: 48.2031\n",
            "160th Batch Loss: 1.1411 Batch Accuracy: 48.8672\n",
            "180th Batch Loss: 1.3356 Batch Accuracy: 49.3490\n",
            "200th Batch Loss: 1.0131 Batch Accuracy: 49.8594\n",
            "220th Batch Loss: 1.2622 Batch Accuracy: 50.4190\n",
            "240th Batch Loss: 1.2897 Batch Accuracy: 50.7617\n",
            "260th Batch Loss: 0.9126 Batch Accuracy: 50.9736\n",
            "280th Batch Loss: 1.1294 Batch Accuracy: 51.3393\n",
            "300th Batch Loss: 0.9952 Batch Accuracy: 51.5104\n",
            "320th Batch Loss: 0.9100 Batch Accuracy: 51.8359\n",
            "340th Batch Loss: 1.0666 Batch Accuracy: 52.1415\n",
            "360th Batch Loss: 1.1787 Batch Accuracy: 52.3655\n",
            "380th Batch Loss: 1.4227 Batch Accuracy: 52.6110\n",
            "400th Batch Loss: 0.8883 Batch Accuracy: 52.9102\n",
            "420th Batch Loss: 1.0328 Batch Accuracy: 53.1994\n",
            "440th Batch Loss: 1.2702 Batch Accuracy: 53.4233\n",
            "460th Batch Loss: 1.4376 Batch Accuracy: 53.6277\n",
            "480th Batch Loss: 1.2374 Batch Accuracy: 53.8184\n",
            "500th Batch Loss: 1.2842 Batch Accuracy: 53.9125\n",
            "520th Batch Loss: 0.7198 Batch Accuracy: 54.1136\n",
            "540th Batch Loss: 0.9119 Batch Accuracy: 54.3316\n",
            "560th Batch Loss: 1.0889 Batch Accuracy: 54.4420\n",
            "580th Batch Loss: 0.9573 Batch Accuracy: 54.5205\n",
            "600th Batch Loss: 1.1822 Batch Accuracy: 54.5339\n",
            "620th Batch Loss: 1.1084 Batch Accuracy: 54.5741\n",
            "640th Batch Loss: 1.1642 Batch Accuracy: 54.6533\n",
            "660th Batch Loss: 1.2405 Batch Accuracy: 54.8201\n",
            "Epoch [2/10] Loss: 1.2834 Epoch Accuracy: 54.8634\n",
            "0\n",
            "Validation Accuracy: 66.38%\n",
            "20th Batch Loss: 1.3179 Batch Accuracy: 62.2656\n",
            "40th Batch Loss: 1.2464 Batch Accuracy: 61.2891\n",
            "60th Batch Loss: 1.1125 Batch Accuracy: 61.8490\n",
            "80th Batch Loss: 0.9730 Batch Accuracy: 61.1719\n",
            "100th Batch Loss: 1.1137 Batch Accuracy: 61.0156\n",
            "120th Batch Loss: 1.1060 Batch Accuracy: 61.2500\n",
            "140th Batch Loss: 0.9438 Batch Accuracy: 61.5179\n",
            "160th Batch Loss: 1.3314 Batch Accuracy: 61.1328\n",
            "180th Batch Loss: 0.8163 Batch Accuracy: 61.0938\n",
            "200th Batch Loss: 0.9195 Batch Accuracy: 61.2969\n",
            "220th Batch Loss: 1.0649 Batch Accuracy: 61.3565\n",
            "240th Batch Loss: 1.1073 Batch Accuracy: 61.1328\n",
            "260th Batch Loss: 1.4358 Batch Accuracy: 61.0397\n",
            "280th Batch Loss: 0.9124 Batch Accuracy: 60.9096\n",
            "300th Batch Loss: 1.2942 Batch Accuracy: 60.6927\n",
            "320th Batch Loss: 0.8999 Batch Accuracy: 60.8643\n",
            "340th Batch Loss: 0.8140 Batch Accuracy: 61.0432\n",
            "360th Batch Loss: 0.8298 Batch Accuracy: 60.9332\n",
            "380th Batch Loss: 0.8347 Batch Accuracy: 60.9786\n",
            "400th Batch Loss: 1.0727 Batch Accuracy: 60.9414\n",
            "420th Batch Loss: 0.9948 Batch Accuracy: 60.8668\n",
            "440th Batch Loss: 0.9751 Batch Accuracy: 60.9517\n",
            "460th Batch Loss: 1.2259 Batch Accuracy: 61.0632\n",
            "480th Batch Loss: 0.8206 Batch Accuracy: 61.2695\n",
            "500th Batch Loss: 1.2779 Batch Accuracy: 61.2844\n",
            "520th Batch Loss: 0.9162 Batch Accuracy: 61.1869\n",
            "540th Batch Loss: 1.0068 Batch Accuracy: 61.2124\n",
            "560th Batch Loss: 1.2970 Batch Accuracy: 61.3114\n",
            "580th Batch Loss: 0.9676 Batch Accuracy: 61.4116\n",
            "600th Batch Loss: 0.9375 Batch Accuracy: 61.4505\n",
            "620th Batch Loss: 1.2709 Batch Accuracy: 61.4945\n",
            "640th Batch Loss: 0.9184 Batch Accuracy: 61.5527\n",
            "660th Batch Loss: 1.0365 Batch Accuracy: 61.6146\n",
            "Epoch [3/10] Loss: 0.8262 Epoch Accuracy: 61.6667\n",
            "0\n",
            "Validation Accuracy: 69.92%\n",
            "20th Batch Loss: 1.1309 Batch Accuracy: 63.3594\n",
            "40th Batch Loss: 1.0570 Batch Accuracy: 63.5547\n",
            "60th Batch Loss: 1.0840 Batch Accuracy: 63.9583\n",
            "80th Batch Loss: 0.7116 Batch Accuracy: 63.5352\n",
            "100th Batch Loss: 1.2819 Batch Accuracy: 63.1406\n",
            "120th Batch Loss: 0.6819 Batch Accuracy: 63.3464\n",
            "140th Batch Loss: 0.7414 Batch Accuracy: 63.3929\n",
            "160th Batch Loss: 0.9109 Batch Accuracy: 63.0176\n",
            "180th Batch Loss: 0.9326 Batch Accuracy: 63.3073\n",
            "200th Batch Loss: 0.9906 Batch Accuracy: 63.5625\n",
            "220th Batch Loss: 0.8226 Batch Accuracy: 63.8707\n",
            "240th Batch Loss: 0.6629 Batch Accuracy: 63.9648\n",
            "260th Batch Loss: 0.7091 Batch Accuracy: 64.0505\n",
            "280th Batch Loss: 1.3604 Batch Accuracy: 63.9174\n",
            "300th Batch Loss: 0.8609 Batch Accuracy: 63.9427\n",
            "320th Batch Loss: 0.8698 Batch Accuracy: 63.8135\n",
            "340th Batch Loss: 1.1352 Batch Accuracy: 63.7316\n",
            "360th Batch Loss: 1.1907 Batch Accuracy: 63.7587\n",
            "380th Batch Loss: 0.7676 Batch Accuracy: 63.9021\n",
            "400th Batch Loss: 0.9328 Batch Accuracy: 63.9922\n",
            "420th Batch Loss: 1.2693 Batch Accuracy: 63.9695\n",
            "440th Batch Loss: 1.5312 Batch Accuracy: 64.0554\n",
            "460th Batch Loss: 0.9885 Batch Accuracy: 64.1440\n",
            "480th Batch Loss: 1.1148 Batch Accuracy: 64.1602\n",
            "500th Batch Loss: 0.8387 Batch Accuracy: 64.2687\n",
            "520th Batch Loss: 1.0896 Batch Accuracy: 64.3149\n",
            "540th Batch Loss: 1.1877 Batch Accuracy: 64.4039\n",
            "560th Batch Loss: 0.9470 Batch Accuracy: 64.5201\n",
            "580th Batch Loss: 0.6391 Batch Accuracy: 64.6228\n",
            "600th Batch Loss: 1.3188 Batch Accuracy: 64.5807\n",
            "620th Batch Loss: 1.0085 Batch Accuracy: 64.6069\n",
            "640th Batch Loss: 1.2253 Batch Accuracy: 64.6240\n",
            "660th Batch Loss: 1.1159 Batch Accuracy: 64.6283\n",
            "Epoch [4/10] Loss: 0.9295 Epoch Accuracy: 64.6319\n",
            "0\n",
            "Validation Accuracy: 70.52%\n",
            "20th Batch Loss: 0.9024 Batch Accuracy: 66.5625\n",
            "40th Batch Loss: 1.0285 Batch Accuracy: 65.0391\n",
            "60th Batch Loss: 1.4661 Batch Accuracy: 65.0781\n",
            "80th Batch Loss: 0.8089 Batch Accuracy: 64.9609\n",
            "100th Batch Loss: 0.6295 Batch Accuracy: 65.6562\n",
            "120th Batch Loss: 0.9091 Batch Accuracy: 65.4557\n",
            "140th Batch Loss: 0.7457 Batch Accuracy: 65.6585\n",
            "160th Batch Loss: 0.9180 Batch Accuracy: 65.8398\n",
            "180th Batch Loss: 1.5051 Batch Accuracy: 65.9896\n",
            "200th Batch Loss: 1.2644 Batch Accuracy: 65.4766\n",
            "220th Batch Loss: 1.1113 Batch Accuracy: 65.6250\n",
            "240th Batch Loss: 0.7112 Batch Accuracy: 65.7617\n",
            "260th Batch Loss: 0.7950 Batch Accuracy: 65.8654\n",
            "280th Batch Loss: 0.5101 Batch Accuracy: 66.0268\n",
            "300th Batch Loss: 0.8387 Batch Accuracy: 66.1615\n",
            "320th Batch Loss: 1.0475 Batch Accuracy: 66.2695\n",
            "340th Batch Loss: 0.9780 Batch Accuracy: 66.2822\n",
            "360th Batch Loss: 0.7648 Batch Accuracy: 66.3108\n",
            "380th Batch Loss: 0.8482 Batch Accuracy: 66.3281\n",
            "400th Batch Loss: 0.9908 Batch Accuracy: 66.1992\n",
            "420th Batch Loss: 1.2362 Batch Accuracy: 66.1868\n",
            "440th Batch Loss: 0.8969 Batch Accuracy: 66.3494\n",
            "460th Batch Loss: 0.8563 Batch Accuracy: 66.3281\n",
            "480th Batch Loss: 0.7442 Batch Accuracy: 66.2305\n",
            "500th Batch Loss: 0.9271 Batch Accuracy: 66.4281\n",
            "520th Batch Loss: 0.6975 Batch Accuracy: 66.5505\n",
            "540th Batch Loss: 0.7774 Batch Accuracy: 66.6406\n",
            "560th Batch Loss: 0.5990 Batch Accuracy: 66.7411\n",
            "580th Batch Loss: 1.0025 Batch Accuracy: 66.8265\n",
            "600th Batch Loss: 0.7687 Batch Accuracy: 66.8828\n",
            "620th Batch Loss: 0.5332 Batch Accuracy: 66.8800\n",
            "640th Batch Loss: 0.9903 Batch Accuracy: 66.8677\n",
            "660th Batch Loss: 0.9890 Batch Accuracy: 66.8868\n",
            "Epoch [5/10] Loss: 0.6953 Epoch Accuracy: 66.9028\n",
            "0\n",
            "Validation Accuracy: 74.08%\n",
            "20th Batch Loss: 0.9249 Batch Accuracy: 66.8750\n",
            "40th Batch Loss: 0.8337 Batch Accuracy: 67.3047\n",
            "60th Batch Loss: 0.6848 Batch Accuracy: 67.5521\n",
            "80th Batch Loss: 0.7342 Batch Accuracy: 67.9102\n",
            "100th Batch Loss: 1.0499 Batch Accuracy: 67.6406\n",
            "120th Batch Loss: 0.7536 Batch Accuracy: 68.0729\n",
            "140th Batch Loss: 0.6095 Batch Accuracy: 68.1138\n",
            "160th Batch Loss: 0.9331 Batch Accuracy: 68.5156\n",
            "180th Batch Loss: 0.9131 Batch Accuracy: 68.8628\n",
            "200th Batch Loss: 1.1547 Batch Accuracy: 68.8203\n",
            "220th Batch Loss: 0.7976 Batch Accuracy: 68.8849\n",
            "240th Batch Loss: 0.7685 Batch Accuracy: 69.0625\n",
            "260th Batch Loss: 0.9337 Batch Accuracy: 69.3029\n",
            "280th Batch Loss: 0.6031 Batch Accuracy: 69.2969\n",
            "300th Batch Loss: 1.2383 Batch Accuracy: 69.1771\n",
            "320th Batch Loss: 0.6458 Batch Accuracy: 69.2090\n",
            "340th Batch Loss: 0.7714 Batch Accuracy: 69.0809\n",
            "360th Batch Loss: 1.2306 Batch Accuracy: 69.1059\n",
            "380th Batch Loss: 0.6223 Batch Accuracy: 69.1694\n",
            "400th Batch Loss: 1.0793 Batch Accuracy: 69.1172\n",
            "420th Batch Loss: 0.8403 Batch Accuracy: 69.1518\n",
            "440th Batch Loss: 0.8583 Batch Accuracy: 69.1903\n",
            "460th Batch Loss: 1.0415 Batch Accuracy: 69.3376\n",
            "480th Batch Loss: 0.9986 Batch Accuracy: 69.4401\n",
            "500th Batch Loss: 0.6959 Batch Accuracy: 69.4469\n",
            "520th Batch Loss: 0.8567 Batch Accuracy: 69.3750\n",
            "540th Batch Loss: 0.5269 Batch Accuracy: 69.4097\n",
            "560th Batch Loss: 0.7935 Batch Accuracy: 69.4224\n",
            "580th Batch Loss: 0.6851 Batch Accuracy: 69.5232\n",
            "600th Batch Loss: 0.7991 Batch Accuracy: 69.5312\n",
            "620th Batch Loss: 0.5848 Batch Accuracy: 69.5060\n",
            "640th Batch Loss: 0.8371 Batch Accuracy: 69.5166\n",
            "660th Batch Loss: 0.7371 Batch Accuracy: 69.4744\n",
            "Epoch [6/10] Loss: 0.9663 Epoch Accuracy: 69.4815\n",
            "0\n",
            "Validation Accuracy: 75.75%\n",
            "20th Batch Loss: 0.6826 Batch Accuracy: 70.9375\n",
            "40th Batch Loss: 0.7896 Batch Accuracy: 69.4922\n",
            "60th Batch Loss: 0.8152 Batch Accuracy: 69.5833\n",
            "80th Batch Loss: 0.6225 Batch Accuracy: 69.5312\n",
            "100th Batch Loss: 0.5995 Batch Accuracy: 69.9062\n",
            "120th Batch Loss: 0.6875 Batch Accuracy: 69.9740\n",
            "140th Batch Loss: 1.0908 Batch Accuracy: 70.2902\n",
            "160th Batch Loss: 0.6894 Batch Accuracy: 70.5371\n",
            "180th Batch Loss: 0.6769 Batch Accuracy: 70.6597\n",
            "200th Batch Loss: 0.8896 Batch Accuracy: 70.3750\n",
            "220th Batch Loss: 0.7339 Batch Accuracy: 69.9645\n",
            "240th Batch Loss: 0.7947 Batch Accuracy: 69.7786\n",
            "260th Batch Loss: 0.6492 Batch Accuracy: 69.5673\n",
            "280th Batch Loss: 0.7638 Batch Accuracy: 69.8493\n",
            "300th Batch Loss: 1.0660 Batch Accuracy: 69.9271\n",
            "320th Batch Loss: 0.6916 Batch Accuracy: 70.0537\n",
            "340th Batch Loss: 0.9203 Batch Accuracy: 70.2252\n",
            "360th Batch Loss: 0.5301 Batch Accuracy: 70.3819\n",
            "380th Batch Loss: 1.1593 Batch Accuracy: 70.3372\n",
            "400th Batch Loss: 0.5763 Batch Accuracy: 70.3789\n",
            "420th Batch Loss: 0.9568 Batch Accuracy: 70.3757\n",
            "440th Batch Loss: 0.7263 Batch Accuracy: 70.4723\n",
            "460th Batch Loss: 1.1485 Batch Accuracy: 70.3906\n",
            "480th Batch Loss: 1.1231 Batch Accuracy: 70.2995\n",
            "500th Batch Loss: 0.6583 Batch Accuracy: 70.3500\n",
            "520th Batch Loss: 0.7471 Batch Accuracy: 70.4718\n",
            "540th Batch Loss: 0.9835 Batch Accuracy: 70.4051\n",
            "560th Batch Loss: 1.1134 Batch Accuracy: 70.4269\n",
            "580th Batch Loss: 0.6487 Batch Accuracy: 70.4364\n",
            "600th Batch Loss: 1.1218 Batch Accuracy: 70.3620\n",
            "620th Batch Loss: 0.6823 Batch Accuracy: 70.3125\n",
            "640th Batch Loss: 1.1023 Batch Accuracy: 70.3198\n",
            "660th Batch Loss: 0.8798 Batch Accuracy: 70.3456\n",
            "Epoch [7/10] Loss: 0.7032 Epoch Accuracy: 70.3935\n",
            "0\n",
            "Validation Accuracy: 76.88%\n",
            "20th Batch Loss: 0.7178 Batch Accuracy: 73.8281\n",
            "40th Batch Loss: 0.5661 Batch Accuracy: 71.8750\n",
            "60th Batch Loss: 0.6112 Batch Accuracy: 72.4740\n",
            "80th Batch Loss: 1.3267 Batch Accuracy: 72.7734\n",
            "100th Batch Loss: 0.8838 Batch Accuracy: 72.0938\n",
            "120th Batch Loss: 0.6321 Batch Accuracy: 71.7188\n",
            "140th Batch Loss: 0.6839 Batch Accuracy: 71.3839\n",
            "160th Batch Loss: 0.7616 Batch Accuracy: 71.3281\n",
            "180th Batch Loss: 0.8041 Batch Accuracy: 71.0764\n",
            "200th Batch Loss: 0.5007 Batch Accuracy: 71.2422\n",
            "220th Batch Loss: 1.0228 Batch Accuracy: 71.3707\n",
            "240th Batch Loss: 0.4977 Batch Accuracy: 71.6406\n",
            "260th Batch Loss: 0.7540 Batch Accuracy: 71.5505\n",
            "280th Batch Loss: 0.6679 Batch Accuracy: 71.3839\n",
            "300th Batch Loss: 0.8398 Batch Accuracy: 71.3958\n",
            "320th Batch Loss: 0.5939 Batch Accuracy: 71.4746\n",
            "340th Batch Loss: 0.5814 Batch Accuracy: 71.3511\n",
            "360th Batch Loss: 0.6238 Batch Accuracy: 71.4887\n",
            "380th Batch Loss: 0.7526 Batch Accuracy: 71.4803\n",
            "400th Batch Loss: 0.8673 Batch Accuracy: 71.4648\n",
            "420th Batch Loss: 0.6197 Batch Accuracy: 71.4695\n",
            "440th Batch Loss: 1.0612 Batch Accuracy: 71.4702\n",
            "460th Batch Loss: 0.6390 Batch Accuracy: 71.5014\n",
            "480th Batch Loss: 0.9587 Batch Accuracy: 71.5137\n",
            "500th Batch Loss: 1.0160 Batch Accuracy: 71.5625\n",
            "520th Batch Loss: 0.8444 Batch Accuracy: 71.5986\n",
            "540th Batch Loss: 0.6162 Batch Accuracy: 71.6493\n",
            "560th Batch Loss: 0.5559 Batch Accuracy: 71.5151\n",
            "580th Batch Loss: 0.8308 Batch Accuracy: 71.5867\n",
            "600th Batch Loss: 0.6245 Batch Accuracy: 71.6432\n",
            "620th Batch Loss: 0.8878 Batch Accuracy: 71.6482\n",
            "640th Batch Loss: 1.3463 Batch Accuracy: 71.5332\n",
            "660th Batch Loss: 0.7927 Batch Accuracy: 71.5223\n",
            "Epoch [8/10] Loss: 0.7365 Epoch Accuracy: 71.5116\n",
            "1\n",
            "Validation Accuracy: 75.83%\n",
            "20th Batch Loss: 0.8394 Batch Accuracy: 72.8125\n",
            "40th Batch Loss: 0.7671 Batch Accuracy: 72.7734\n",
            "60th Batch Loss: 0.7560 Batch Accuracy: 72.4219\n",
            "80th Batch Loss: 0.9660 Batch Accuracy: 72.3047\n",
            "100th Batch Loss: 0.5491 Batch Accuracy: 72.3906\n",
            "120th Batch Loss: 0.7523 Batch Accuracy: 72.7734\n",
            "140th Batch Loss: 0.7357 Batch Accuracy: 73.0580\n",
            "160th Batch Loss: 1.1008 Batch Accuracy: 72.8223\n",
            "180th Batch Loss: 1.0392 Batch Accuracy: 72.5087\n",
            "200th Batch Loss: 0.7007 Batch Accuracy: 72.5000\n",
            "220th Batch Loss: 0.8255 Batch Accuracy: 72.3224\n",
            "240th Batch Loss: 0.7878 Batch Accuracy: 72.5130\n",
            "260th Batch Loss: 0.7768 Batch Accuracy: 72.6202\n",
            "280th Batch Loss: 1.0136 Batch Accuracy: 72.6283\n",
            "300th Batch Loss: 1.3375 Batch Accuracy: 72.6771\n",
            "320th Batch Loss: 0.5877 Batch Accuracy: 72.5342\n",
            "340th Batch Loss: 0.7702 Batch Accuracy: 72.5414\n",
            "360th Batch Loss: 0.7044 Batch Accuracy: 72.3394\n",
            "380th Batch Loss: 0.8828 Batch Accuracy: 72.2286\n",
            "400th Batch Loss: 1.0077 Batch Accuracy: 72.3203\n",
            "420th Batch Loss: 0.8199 Batch Accuracy: 72.2693\n",
            "440th Batch Loss: 0.5977 Batch Accuracy: 72.2976\n",
            "460th Batch Loss: 0.8352 Batch Accuracy: 72.4117\n",
            "480th Batch Loss: 0.7118 Batch Accuracy: 72.4382\n",
            "500th Batch Loss: 0.8072 Batch Accuracy: 72.4000\n",
            "520th Batch Loss: 0.7374 Batch Accuracy: 72.3978\n",
            "540th Batch Loss: 0.5945 Batch Accuracy: 72.5231\n",
            "560th Batch Loss: 0.7465 Batch Accuracy: 72.6172\n",
            "580th Batch Loss: 0.7494 Batch Accuracy: 72.6562\n",
            "600th Batch Loss: 0.4397 Batch Accuracy: 72.7370\n",
            "620th Batch Loss: 0.5289 Batch Accuracy: 72.7797\n",
            "640th Batch Loss: 0.4721 Batch Accuracy: 72.8076\n",
            "660th Batch Loss: 0.5920 Batch Accuracy: 72.8433\n",
            "Epoch [9/10] Loss: 0.9807 Epoch Accuracy: 72.7778\n",
            "0\n",
            "Validation Accuracy: 77.52%\n",
            "20th Batch Loss: 0.7522 Batch Accuracy: 72.1094\n",
            "40th Batch Loss: 0.3748 Batch Accuracy: 73.5938\n",
            "60th Batch Loss: 1.0852 Batch Accuracy: 72.9688\n",
            "80th Batch Loss: 0.6667 Batch Accuracy: 73.2422\n",
            "100th Batch Loss: 0.5549 Batch Accuracy: 73.4688\n",
            "120th Batch Loss: 0.9656 Batch Accuracy: 73.0208\n",
            "140th Batch Loss: 1.0000 Batch Accuracy: 73.0357\n",
            "160th Batch Loss: 0.7822 Batch Accuracy: 73.0664\n",
            "180th Batch Loss: 0.9021 Batch Accuracy: 72.9861\n",
            "200th Batch Loss: 0.5443 Batch Accuracy: 72.8828\n",
            "220th Batch Loss: 0.8998 Batch Accuracy: 73.0895\n",
            "240th Batch Loss: 0.9022 Batch Accuracy: 73.0729\n",
            "260th Batch Loss: 0.5131 Batch Accuracy: 73.1070\n",
            "280th Batch Loss: 0.4438 Batch Accuracy: 73.2366\n",
            "300th Batch Loss: 0.5976 Batch Accuracy: 73.3698\n",
            "320th Batch Loss: 0.9801 Batch Accuracy: 73.1982\n",
            "340th Batch Loss: 1.0733 Batch Accuracy: 73.3364\n",
            "360th Batch Loss: 0.4791 Batch Accuracy: 73.2682\n",
            "380th Batch Loss: 0.7206 Batch Accuracy: 73.2977\n",
            "400th Batch Loss: 0.6938 Batch Accuracy: 73.3867\n",
            "420th Batch Loss: 1.1021 Batch Accuracy: 73.2589\n",
            "440th Batch Loss: 0.7130 Batch Accuracy: 73.1286\n",
            "460th Batch Loss: 0.8678 Batch Accuracy: 73.0197\n",
            "480th Batch Loss: 0.8262 Batch Accuracy: 73.0111\n",
            "500th Batch Loss: 0.9144 Batch Accuracy: 73.0844\n",
            "520th Batch Loss: 0.5396 Batch Accuracy: 73.1160\n",
            "540th Batch Loss: 0.5639 Batch Accuracy: 73.1395\n",
            "560th Batch Loss: 0.4810 Batch Accuracy: 73.1696\n",
            "580th Batch Loss: 0.5179 Batch Accuracy: 73.0819\n",
            "600th Batch Loss: 0.7513 Batch Accuracy: 73.1068\n",
            "620th Batch Loss: 0.7554 Batch Accuracy: 73.1275\n",
            "640th Batch Loss: 0.8513 Batch Accuracy: 73.0347\n",
            "660th Batch Loss: 0.9140 Batch Accuracy: 73.0445\n",
            "Epoch [10/10] Loss: 0.6877 Epoch Accuracy: 73.0556\n",
            "0\n",
            "Validation Accuracy: 78.58%\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "dropout_prob = 0.30\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "model = None\n",
        "\n",
        "# call function\n",
        "# Main loop for k-fold cross-validation\n",
        "for fold, (train_fold_indices, val_fold_indices) in enumerate(skf.split(train_idx, stratified_train_labels)):\n",
        "    print(f'Fold {fold + 1}/{num_folds}')\n",
        "    mean, std = calc_mean_std(Subset(stratified_train_set, train_fold_indices))\n",
        "\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_fold_indices)\n",
        "    val_sampler = torch.utils.data.SubsetRandomSampler(val_fold_indices)\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=stratified_train_set,\n",
        "        batch_size=batch_size,\n",
        "        sampler=train_sampler,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        dataset=stratified_train_set,\n",
        "        batch_size=batch_size,\n",
        "        sampler=val_sampler,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "\n",
        "    model = Net(dropout_prob).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    adam_train_and_validate(model, train_loader, val_loader, criterion, optimizer, mean, std, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "VNt0Y4EYrQKb",
        "outputId": "ff833c27-0fce-4a0e-e66b-5c1765827cf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 80.82%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test model\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=stratified_test_set,\n",
        "    batch_size=batch_size,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=g)\n",
        "\n",
        "test(model, test_loader, mean, std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3ZaKDEXTxj8"
      },
      "source": [
        "## Population Based - Genetic Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCu7BxovUDNX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht8q-yrGUKpa"
      },
      "source": [
        "## Proposed - Adaptive Baldwinian-Lamarckian Memetic Algorithm\n",
        "## Self-regularizing Adam-guided Adaptive-SL-PSO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsUwHE1WWi3y"
      },
      "source": [
        "### Imports, Preprocessing & Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9bsK83NWIEi",
        "outputId": "fa897d73-34e7-4b20-904f-7927b53a1b3a"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from deap import base\n",
        "from deap import benchmarks\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "from numba import jit, cuda\n",
        "from numpy import genfromtxt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "iSQnlEWTWs2e"
      },
      "outputs": [],
      "source": [
        "def neighbour_search(pop, individual, neighbours=10):\n",
        "    distances = {}\n",
        "    for i, part in enumerate(pop):\n",
        "        # print(f\"part: {len(part)} individual: {len(individual)}\")\n",
        "        if individual != part:\n",
        "            distance = np.linalg.norm(np.array(part)-np.array(individual))\n",
        "            if len(distances) < neighbours:\n",
        "                distances[i] = distance\n",
        "            else:\n",
        "                copy = distances.copy()\n",
        "                for e in copy.keys():\n",
        "                    if distances[e] > distance:\n",
        "                        distances[i] = distance\n",
        "                        del distances[e]\n",
        "                        break\n",
        "    neighbourhood = [pop[i] for i in distances.keys()]\n",
        "    sort_population(neighbourhood, potential=True)\n",
        "    return neighbourhood\n",
        "\n",
        "# Ali's functions for question 3 ----------------------------------------------------------------\n",
        "def sort_population(population, potential=False):\n",
        "    if potential:\n",
        "        population.sort(key=lambda x: x.potential[0])\n",
        "    else:\n",
        "        population.sort(key=lambda x: x.fitness.values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "oNGdG_SbxR9f"
      },
      "outputs": [],
      "source": [
        "# Function to freeze all but the last layer\n",
        "def freeze_all_but_last(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'fc2' not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "# Extract weights from the last layer\n",
        "def extract_weights_biases(layer):\n",
        "    return [p.data.numpy() for p in layer.parameters()]\n",
        "\n",
        "def generate_particle(dimension):\n",
        "    part = creator.Particle([random.uniform(-1, 1) for _ in range(dimension)])\n",
        "    part.speed = [random.uniform(-1, 1) for _ in range(dimension)]\n",
        "    part.smin = -1\n",
        "    part.smax = 1\n",
        "    return part\n",
        "\n",
        "# Define the fitness function\n",
        "def evaluate_particle(model, particle, inputs, labels, potential=True, best=False):\n",
        "    weights = np.asarray(particle)\n",
        "    print(weights)\n",
        "    new_weights = torch.from_numpy(weights[:weights_len].reshape(weights_dim))\n",
        "    new_biases = torch.from_numpy(weights[weights_len:dimension])\n",
        "    print(new_weights)\n",
        "    print(new_biases)\n",
        "    new_weights = new_weights.to(device)\n",
        "    new_biases = new_biases.to(device)\n",
        "    model.fc2.weight = torch.nn.Parameter(new_weights)\n",
        "    model.fc2.bias = torch.nn.Parameter(new_biases)\n",
        "\n",
        "    inputs, labels = torch.FloatTensor(inputs), torch.Tensor(labels)\n",
        "    inputs, labels, model = inputs.to(device), labels.to(device), model.to(device)\n",
        "    outputs = model(inputs)  # input and predict based on images\n",
        "    loss = criterion(outputs, labels)\n",
        "    optimizer.zero_grad()  # clear gradients for next train\n",
        "    if potential:\n",
        "            loss.backward()  # backpropagation, compute gradients\n",
        "            optimizer.step()  # apply gradients\n",
        "            outputs = model(inputs)  # input and predict based on images\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad() # clear gradients for next train\n",
        "            if best:\n",
        "                    demonstrator_weights = model.fc2.weight.data.to('cpu').reshape(-1)\n",
        "                    demosntrator_biases = model.fc2.bias.data.to('cpu').reshape(-1)\n",
        "                    return (loss.item(),), np.concatenate((demonstrator_weights, demosntrator_biases))\n",
        "    return loss.item(),\n",
        "\n",
        "# social learning in a neighbourhood of size\n",
        "def behaviour_learning(inputs, labels, model, gamma, gbest, part, pop, epsilon, mu, neighbours=10):\n",
        "    i = pop.index(part)\n",
        "    neighbour_pop = neighbour_search(pop, part, neighbours)\n",
        "\n",
        "    index = None\n",
        "    for j in range(len(neighbour_pop)-1, -1, -1):\n",
        "        if neighbour_pop[j].potential[0] <= part.potential[0]:\n",
        "            index = j\n",
        "            break\n",
        "        \n",
        "\n",
        "    demonstrator = None\n",
        "\n",
        "    if (i != 0) & (index is not None):\n",
        "        if index != 0:\n",
        "                k = math.floor(random.randrange(0, index))\n",
        "                demonstrator = neighbour_pop[k]\n",
        "        else:\n",
        "                k = index\n",
        "                demonstrator = neighbour_pop[k]\n",
        "    else:\n",
        "        best_fitness, best_self = evaluate_particle(model, part, inputs, labels, potential=True, best=True)\n",
        "        demonstrator = creator.Particle(best_self)\n",
        "        demonstrator.fitness.values = best_fitness\n",
        "        demonstrator.smin = -1\n",
        "        demonstrator.smax = 1\n",
        "\n",
        "    r1 = (random.uniform(0, 1) for _ in range(len(part)))\n",
        "    r2 = (random.uniform(0, 1) for _ in range(len(part)))\n",
        "    r3 = (random.uniform(0, 1) for _ in range(len(part)))\n",
        "    ones = [1] * len(part)\n",
        "    one_minus_gamma = np.asarray(ones) - gamma\n",
        "\n",
        "    v_r0 = list(map(operator.mul, r1, part.speed))\n",
        "    v_r1 = list(map(operator.mul, r2, map(operator.sub, demonstrator, part))) # local best\n",
        "    v_r2 = list(map(operator.mul,r3, map(operator.mul, [epsilon*x for x in mu], part))) # global best\n",
        "\n",
        "    sl_speed = list(map(operator.add, v_r1, v_r2))\n",
        "    exploitation_speed = gamma*np.asarray(gbest)\n",
        "    exploration_speed = list(map(operator.mul, one_minus_gamma, sl_speed))\n",
        "\n",
        "    part.speed = list(map(operator.add, v_r0 , map(operator.add, exploitation_speed, exploration_speed)))\n",
        "    part[:] = list(map(operator.add, part, part.speed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "iYQDf9FXWFBN"
      },
      "outputs": [],
      "source": [
        "def pso_optimize(model, toolbox, pop, inputs, labels, g):\n",
        "    interval        = 10\n",
        "    iterations      = 100\n",
        "    neighbours = 10\n",
        "    beta = 0.01\n",
        "    alpha = 0.5\n",
        "\n",
        "    m = populationSize + math.floor(dimension/10)\n",
        "    epsilon = beta * (dimension/populationSize)\n",
        "\n",
        "    print(pop[0])\n",
        "\n",
        "    gbest = None\n",
        "    preliminary_best = None\n",
        "\n",
        "    # eval current fitness\n",
        "    for part in pop:\n",
        "        part.fitness.values = toolbox.evaluate(model, part, inputs, labels, potential=False) #actually only one fitness value\n",
        "    print(pop[0])\n",
        "    # Begin the evolution\n",
        "    #for g in range(iterations):\n",
        "\n",
        "\n",
        "    # A new Search\n",
        "    #print(\"-- Search %i --\" % g)\n",
        "\n",
        "    # find the global best - lamarckian search party lead - gradient descent\n",
        "    sort_population(pop, potential=False)\n",
        "    gbest = pop[0]\n",
        "    print(pop[0])\n",
        "    print(f'current global best: {gbest.fitness.values[0]}, {pop[0].fitness.values[0]}')\n",
        "    \n",
        "    # eval potential (after one step of Adam descent) of all search parties\n",
        "    for part in pop:\n",
        "        part.potential = toolbox.evaluate(model, part, inputs, labels, potential=True) #actually only one fitness value\n",
        "    print(pop[0])\n",
        "    # sort the the baldwinian search participants by their potential, leave the current global best as the lamarckian search-lead\n",
        "    sort_population(pop[1:], potential=True)\n",
        "    print(pop[0])\n",
        "    # parameter setting - variable\n",
        "    mu = [sum(np.asarray(pop)[:,x])/populationSize for x in range(dimension)]\n",
        "    gamma = 1/(1+math.exp(3 - 6*1-gbest.potential[0]/gbest.fitness.values[0]))\n",
        "    i = 0\n",
        "\n",
        "    # evolve the local-search-groups via SL-PSO algorithm\n",
        "    for part in pop[1:]:\n",
        "        i = i + 1\n",
        "        learn_prob = (1 - (i-1)/m)**(alpha*math.log(math.ceil(dimension/m)))\n",
        "        if random.random() < learn_prob:\n",
        "            toolbox.learn(inputs, labels, model, gamma, gbest, part, pop[1:], epsilon, mu, neighbours)\n",
        "\n",
        "\n",
        "        # save the best at current step, so we can maintain consistency & update global best after this swarm is updated\n",
        "        if (not preliminary_best) or preliminary_best.fitness < part.fitness:\n",
        "            preliminary_best = creator.Particle(part)\n",
        "            preliminary_best.fitness.values = part.fitness.values\n",
        "\n",
        "    # evolve the lamarckian lead\n",
        "    new_fitness, evolved_gbest = toolbox.evaluate(model, pop[0], inputs, labels, potential=True, best=True) #actually only one fitness value\n",
        "    pop[0] = creator.Particle(evolved_gbest)\n",
        "    pop[0].fitness.values = new_fitness\n",
        "    pop[0].smin = -1\n",
        "    pop[0].smax = 1\n",
        "    gbest = pop[0]\n",
        "    print(pop[0])\n",
        "    print(f'gbest after evolution: {gbest.fitness.values[0]}, pop[0] after evolution: {pop[0].fitness.values[0]}, preliminary_best after evolution: {preliminary_best.fitness.values[0]}')\n",
        "\n",
        "    # update the global best to the best particle post-evolution\n",
        "    if gbest.fitness < preliminary_best.fitness:\n",
        "        print(\"reassigning global best to the preliminary best\")\n",
        "        gbest = creator.Particle(preliminary_best)\n",
        "        gbest.fitness.values = preliminary_best.fitness.values\n",
        "        \n",
        "    # set weights to best individual\n",
        "    weights = np.asarray(gbest)\n",
        "    new_weights = torch.from_numpy(weights[:weights_len].reshape(weights_dim)).float().to(device)\n",
        "    new_biases = torch.from_numpy(weights[weights_len:dimension]).float().to(device)\n",
        "    #model.fc2.weight = torch.nn.Parameter(new_weights)\n",
        "    #model.fc2.bias = torch.nn.Parameter(new_biases)\n",
        "\n",
        "    # Gather all the fitnesses in one list and print the stats\n",
        "    # print every interval\n",
        "    fits.append(gbest.fitness.values[0])\n",
        "    if g%interval==0: # interval\n",
        "        logbook.record(gen=g, evals=len(pop), **stats.compile(pop))\n",
        "        print(logbook.stream)\n",
        "        length = len(pop)\n",
        "        mean = sum(fits) / length\n",
        "        sum2 = sum(x*x for x in fits)\n",
        "        std = abs(sum2 / length - mean**2)**0.5\n",
        "\n",
        "\n",
        "        print(\"  Min %s\" % min(fits))\n",
        "        print(\"  Max %s\" % max(fits))\n",
        "        print(\"  Avg %s\" % mean)\n",
        "        print(\"  Std %s\" % std)\n",
        "        plt.plot(fits)\n",
        "    return new_weights, new_biases, pop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "CYuX057YY6zk"
      },
      "outputs": [],
      "source": [
        "# function for training and evaluating the model\n",
        "def memetic_train_and_validate(model, toolbox, pop, train_loader, test_loader, criterion, optimizer, mean, std, epochs=30):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # early stopping parameters\n",
        "    early_stopping_patience = 3  # number of epochs to wait for improvement before stopping\n",
        "    early_stopping_counter = 0    # counter for epochs without improvement\n",
        "    best_accuracy = 0             # track the best accuracy\n",
        "    batch_curve = []\n",
        "\n",
        "    # train\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        i = 0\n",
        "\n",
        "        for inputs, train_load_labels in train_loader:\n",
        "            print(pop[0])\n",
        "            inputs = train_transform(inputs, mean, std)\n",
        "            #inputs, train_load_labels = inputs.to(device), train_load_labels.to(device)\n",
        "            i += 1\n",
        "            new_weights, new_biases, pop = pso_optimize(model, toolbox, pop, inputs, train_load_labels, i)\n",
        "            #model.fc2.weight = torch.nn.Parameter(new_weights)\n",
        "            #model.fc2.biases = torch.nn.Parameter(new_biases)\n",
        "            inputs, train_load_labels = inputs.to(device), train_load_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, train_load_labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            running_loss += loss.item()\n",
        "            total += train_load_labels.size(0)\n",
        "            correct += (predicted == train_load_labels).sum().item()\n",
        "            #if i % 20 == 0:\n",
        "            batch_accuracy = 100 * correct / total\n",
        "            batch_curve.append(batch_accuracy)\n",
        "            print(f'{i}th Batch Loss: {loss.item():.4f} Batch Accuracy: {batch_accuracy:.4f}')\n",
        "\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}] Loss: {loss.item():.4f} Epoch Accuracy: {epoch_accuracy:.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # validate\n",
        "        with torch.no_grad():\n",
        "            for inputs, test_load_labels in test_loader:\n",
        "                #inputs = normalize(inputs, mean, std)\n",
        "                inputs, test_load_labels = inputs.to(device), test_load_labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += test_load_labels.size(0)\n",
        "                correct += (predicted == test_load_labels).sum().item()\n",
        "\n",
        "        validation_accuracy = 100 * correct / total\n",
        "\n",
        "        # check if the current validation accuracy is better than the best recorded accuracy\n",
        "        if validation_accuracy > best_accuracy:\n",
        "            best_accuracy = validation_accuracy\n",
        "            early_stopping_counter = 0  # Reset the counter\n",
        "            # save the model checkpoint\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, filename=f\"best_model_epoch_{epoch+1}.pth.tar\")\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        print(early_stopping_counter)\n",
        "        # check if early stopping should be triggered\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "        print(f'Validation Accuracy: {validation_accuracy:.2f}%')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxRz7BbNWwHv",
        "outputId": "3f671a29-41b1-409e-e675-aee498fa6fdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_checkpoint = torch.load(\"best_model_epoch_10.pth.tar\")  # Replace X with the epoch number\n",
        "model = Net(0)\n",
        "model.load_state_dict(best_checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "0mmabBgCg2ae",
        "outputId": "331aa192-b5dc-462e-992d-2cfe415fda3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Particle' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alishihab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.015447951662233894, 0.6420502228750358, 0.5567303837978361, -0.8776270588942141, 0.3566924094757409, 0.9130911499866972, 0.9566409505980515, 0.6818461748766209, 0.9433386332433895, 0.46962716899201173, 0.9702230251990405, 0.6288549758460922, -0.8597314836996488, -0.8812726702609044, 0.4404244896401599, 0.851005881965587, -0.9491609447302198, 0.007895597745647187, -0.43374690226305934, 0.9401591876207187, -0.9302262160074852, -0.10484579559685847, 0.3971973856780646, 0.5870052198033604, -0.17395404732468434, 0.5242917621479606, 0.00531875301682927, -0.40136195473710523, 0.3493658207095298, -0.12878590129601664, 0.5624790497881151, 0.2040663073994966, 0.19274181298142934, -0.17893361592293622, 0.3453522871432875, -0.10797176251580898, 0.9906228862114677, 0.09603197615465198, 0.5372144422971656, 0.32868339017147186, 0.13656310679652806, 0.4045540003949486, -0.6807413827878144, 0.045219743616111696, 0.5504346502064228, -0.03882280338798938, 0.19876048546164382, 0.45514467359222843, -0.9054892711702318, -0.20536425652617885, -0.4386357074643257, -0.39212022434123095, -0.27478339787317974, 0.005920786720396531, -0.7426756429777126, -0.7550286640579444, 0.48387098640011894, -0.37629538991195677, -0.1975731931633633, 0.5233015590219534, 0.7531330996282435, 0.1277809159315335, -0.15817424612964603, -0.018674747607088538, 0.42873928130678784, 0.9889733718774001, 0.12048436033116827, -0.41859356547982696, -0.19360312005525682, 0.28287016935510945, -0.8495804131180713, -0.28818298688829236, 0.6514656156173608, 0.4041438442268708, -0.736417617083748, 0.07745995392451599, 0.8354984719962506, 0.44261153253595675, -0.15557497092884698, -0.45281342739013297, 0.7032112694339181, 0.6137540509986885, 0.708466759560876, -0.7005670165157818, -0.3414845395389239, 0.9718242196383522, 0.4192943537231131, 0.7302096305110763, 0.026103990304829683, 0.4955334762417307, 0.7201444008154347, -0.5359322147037733, -0.9452375831807449, -0.8091267539702127, -0.44427448820546145, 0.7124072347799422, -0.6321307709192165, -0.9069060560989959, 0.4450998814779701, 0.71302080574123, 0.684711602960761, -0.1798464876581496, -0.5937764129892307, -0.3562039043289753, 0.6862999045608562, -0.8163795130129441, 0.04168481344994124, -0.7902647485029313, -0.7546431893806331, 0.15824175988168254, -0.21005822692106646, 0.6122882141307551, -0.1625477193187963, 0.07157528874050123, -0.3695152255523857, -0.32247079842078974, -0.39380901042219896, 0.8653488175767827, -0.37044894529585304, -0.1401460171758444, 0.0363122030553813, 0.8440810866510537, 0.4813999911067157, 0.585859203647672, 0.2601654407257339, 0.23211087188883317, -0.21779141693677428, -0.7962708375899463, 0.7996145950546325, 0.8165592429221353, 0.45586979874067257, 0.9975383060275136, -0.33638817731181714, -0.17714437929976246, 0.08368705457630221, 0.6947459466763448, -0.7937482614110323, 0.5456635257227429, -0.7623246037264881, 0.4760361454185009, -0.49314437294087243, -0.5808601282844204, -0.5785579732325412, -0.3754240859326703, -0.2257590771192577, -0.8284943296764442, -0.9894538862330744, -0.5463044205131402, -0.41963153065464365, 0.49980506435535643, 0.1724123431101776, -0.718126079476344, -0.37547696894241134, -0.0793014795550302, -0.32420218738652307, -0.5699863277346293, -0.08730642003365574, -0.7635201273616552, -0.4266976326559597, 0.780389093374825, 0.045896568986279584, -0.5447289399993511, -0.842687199032149, 0.8981900213657281, -0.3598954910119019, 0.7225466066172361, 0.13534523957743705, 0.1475758212059448, 0.2986870080522215, 0.7186622405933618, -0.4769165623138314, 0.696549683350947, -0.33837493067841273, 0.38166624089715406, -0.5952991651242112, -0.8110627362747174, 0.8669947884715481, -0.8707531672214619, 0.8756171043836263, -0.41913168570646575, -0.22532819053856867, 0.3328050229364512, -0.9969799384164024, -0.16449578590108005, -0.44212421683721215, 0.7508586951042067, 0.5690833091141911, -0.03987187774661449, -0.8383527152746284, 0.8835075436833972, -0.8386987223333897, -0.2524994159399996, -0.01238341525264608, -0.3364091504339093, -0.079653990416648, -0.889673979411312, 0.5161354917649839, -0.9190873345739841, -0.5272544798219472, -0.9913730684997879, 0.7032884364485117, 0.3192769942378262, 0.5694268656629848, -0.8261778958754571, -0.486005257302756, 0.8546851891119751, 0.47309597546976323, 0.7093517773635682, -0.5801467138274403, 0.4702155185098489, 0.9514334032295693, -0.5189728890298604, -0.21475261505994547, 0.056308115762343514, -0.7314257408066447, -0.20527134272321024, -0.9774236343685057, 0.1570741028256808, -0.7922290988943241, 0.8918865798720734, 0.42261276434011363, -0.4276000835796021, 0.9362966115681568, -0.7977215005080949, -0.7241938724566095, -0.34890315194228094, -0.8977642115507207, 0.06740215137499495, -0.5793390370011984, 0.39052285873600723, -0.41056749779683077, -0.7824409614317316, 0.9924466616878245, -0.9616479970236658, -0.6500416701218974, -0.49922211599270505, 0.26500321653915493, 0.13595449766744716, 0.37324394336467726, 0.2128662367354477, 0.18074792012803176, -0.4825975039047534, 0.14466065120709537, -0.8638736102358389, -0.3293334671340036, -0.7948736692236678, 0.32698361217432637, -0.6911235656610519, -0.4402813668595347, 0.6022595279553877, -0.704379868555961, -0.45224172821786124, 0.6796295559949921, 0.883235470044792, 0.7531793627238703, -0.7479986596615447, 0.21727535948667764, -0.8671464593671596, 0.21769262295461034, 0.9509523996817899, 0.18794740204785998, -0.28596454972611385, 0.2849407656132441, 0.3926011018613913, -0.5041685824158941, 0.43842415924875744, 0.4675042635125133, 0.6888855158780647, -0.6641179052992847, -0.09167534083568274, 0.92176589779644, -0.19105668077203575, 0.19124143849120134, -0.04055598561906382, -0.5033729079087801, 0.08300341666456101, -0.6624629727700035, -0.8168502807345019, 0.9309054681784295, 0.5806631449412152, 0.03298895615745523, 0.5643673094154358, 0.217810337259708, -0.5505094897039491, -0.8540104040028067, -0.7685910839677517, -0.13258215063277823, -0.30362900834150475, -0.05378881419668735, 0.3285816822631267, 0.467278467585011, -0.44993266674357213, -0.7456498996377583, -0.11925725838724666, -0.9860854343766701, -0.8699647038146276, -0.43921095609375627, 0.5556653354883543, 0.7227925617163602, 0.35914609954483967, 0.11261726351389534, -0.4797622508617503, 0.3042758622261368, 0.23978202454995157, -0.7623632849448387, 0.7178355053561629, 0.6339807542783966, 0.0022457626155709676, 0.7251538269314524, 0.16349282376386642, 0.8055524844295356, 0.3719563896639142, 0.17595508403191906, 0.5817897892721573, 0.5749637200589686, 0.17244891264242423, -0.884581762034683, 0.7329829631468274, -0.683301953668751, -0.9936276468451553, -0.12808884899073436, -0.24455806032176475, 0.0008482805529659299, -0.9533317579859768, -0.5022680316643986, 0.7451499512579103, -0.2570734353435695, -0.4586574672387336, -0.9661107355051279, 0.44305658371875367, 0.1928947699849588, -0.5760207325457072, 0.7657154231115131, 0.23633675162661172, -0.48560299413042896, 0.2446897166558133, -0.18908355082845785, 0.14677594022629692, -0.9294301587251244, 0.637629919958896, -0.16836226843535296, -0.8213334221497717, 0.10552469638011508, 0.060003901598524756, -0.5348323944544333, 0.7183414936444681, 0.911321802540846, 0.6637135992929943, 0.3148857132454579, 0.7138180889919776, -0.39981475416428114, -0.926847299614382, 0.7299559747539017, 0.4881906152508324, -0.6900039790586048, -0.9318541323414109, -0.8994371488735695, -0.22379819560007475, 0.7820766689150249, -0.07094560921740567, 0.960991073114428, -0.6638318335774713, 0.472956971258937, -0.2667931823409526, 0.8088238796703375, 0.4733456366521567, 0.8125903167197421, -0.5894616926904845, -0.34078412139785175, -0.2506041449863208, -0.8583973751185925, -0.09485443602219656, 0.38989857728563515, 0.6140580876203452, -0.9134540320615412, 0.5048821387834457, 0.9458182977913527, 0.6747117545684773, 0.5162901771763182, -0.09726635537123096, -0.22436142345836751, 0.1619588295544705, 0.860300032928754, 0.288536055059625, 0.27964279017924043, 0.9229282786386042, 0.9144126659458762, -0.8700405597924497, -0.30571086946443327, 0.5996983671700875, 0.5207189019848375, -0.6708791533849567, -0.8720687247723677, -0.02481316326974836, -0.9494195668882872, 0.9429641584021435, 0.5174093881078541, -0.8229542216569299, -0.7158078558816829, -0.5985001295757617, -0.8438046043691041, -0.7616182480615954, 0.6985885625236481, -0.2205783712582572, -0.5355825463707307, 0.2390774476917381, 0.16407208356082825, -0.5938613393946777, -0.553966573693141, -0.7861825777403526, -0.484379116145204, -0.6867438581637391, 0.10102008780110783, 0.28176361865180666, 0.5366187864541301, 0.9675097022405608, -0.7783313168966082, -0.49324087086506996, 0.884645495049782, -0.3127601166752316, -0.04579256306747581, -0.2053672731980607, 0.14369614840052813, 0.1526447586917421, 0.593501430956318, 0.212124969826186, -0.23150820474420364, -0.7095948139751662, -0.7905365324322018, 0.930720589429995, -0.12921430272774348, -0.7264543544856421, -0.6048485542338522, 0.13154076152053795, -0.09278777697374263, 0.6642835067486421, 0.5170540681463158, -0.28329829978500354, -0.7890275069721717, -0.007725041250493003, 0.27684113655793463, 0.036275191825078545, 0.6096934693574996, -0.8078736906598991, -0.11034735795896156, -0.7750233088121794, 0.25759859596369883, 0.2628085137916729, -0.8696079038245446, 0.28639886067939946, -0.6739593008304234, -0.3251001859297533, 0.8315124624219563, -0.3949064746254405, 0.35656440945950796, -0.5922302530813601, 0.942087399678079, -0.7966819033843464, 0.7631083292293972, -0.7804658058478857, -0.8468738948310652, 0.794492522907795, 0.7545448459981576, -0.5558382318141759, 0.795922827879582, 0.05140643036219439, -0.048781683033250145, 0.3220676487939802, -0.6622996614205838, -0.5363821616497677, -0.40349983825996194, -0.11915009094318108, 0.6042294272627824, -0.2674819109737925, -0.4151397232606133, -0.8176887620081472, -0.7470532631582048, 0.13513745915364073, 0.3967683406695073, -0.48295528663219, -0.2265453882874462, 0.7145879987716102, 0.41745327733390325, 0.8287246937968142, -0.7726201655812113, -0.8450055605577593, 0.3185580658023732, 0.3013789274407559, -0.3355615119632509, 0.34535014748659076, -0.8989508860938531, -0.28633869380242993, -0.6282514772993828, 0.6960099908825701, -0.6372696518193333, -0.7640155006448324, -0.10320559484713043, 0.6773184903941691, -0.9690482780718086, 0.3474251699729094, 0.5201950030477693, -0.6932339433302519, -0.7440004192092027, 0.48975120319766674, 0.22598211425614267, 0.8288593324734777, 0.9153217305721684, 0.03565332255292275, -0.4736884197465172, 0.2184692955878358, -0.9527030261837421, -0.4218357745538279, -0.8164844743771686, -0.34035885448487013, -0.9759355389621114, -0.49017217106504796, -0.4159672944554689, 0.26288775025875144, 0.4804950991946124, 0.4490092151886538, -0.7389909285083034, 0.4663815325619245, 0.47689265031100314, 0.9515411145703327, 0.662247139151948, -0.5130557353205218, 0.28017884761716383, 0.16136465784869136, 0.2285407152849559, -0.693389451305451, 0.030914825460316653, 0.9410903115494085, 0.20551465224517096, -0.49757442601457913, -0.6027723770664826, 0.07519424887799842, 0.6157480814212561, 0.19555061124291817, 0.8742312439636262, -0.28911797045386467, -0.459539816003562, -0.026233452696938553, -0.05795946721148271, -0.43540557493984444, -0.7211327463944048, 0.022893852465550202, -0.3017858494207857, 0.8051430839133005, 0.6626335202275866, 0.31338383316867024, -0.34476997500880757, -0.8287577326831836, 0.3190621496201107, -0.6448102360375374, -0.15987108209720335, -0.9834931914642961, 0.5997764559395549, 0.1740473792578494, 0.9401087506656871, 0.527799026512006, -0.5167849297224434, -0.5578055449509784, -0.16044027390578397, 0.5486907786113435, -0.8285124113256137, 0.0325542648090098, -0.3706004919541228, -0.44892635068972764, -0.3078646037196777, 0.2123112023564595, 0.5166231642094827, -0.4089535430472315, 0.04808602237494797, 0.8364152288041482, 0.2925800449835845, 0.31364848305302995, 0.902494470779919, 0.5271157929348076, 0.08164700232558264, -0.4844752585452514, 0.4862799278672232, -0.12983217281420556, 0.9577303620751247, -0.5452179281398879, -0.7627796043810469, -0.6245495760786013, 0.10919599888590881, 0.25195950113150656, 0.25086542137696366, -0.8939140576898159, -0.7892834614813407, -0.7094904380521461, -0.4960336182148233, 0.1502528624365953, -0.9136373865945688, 0.41401526069684746, -0.5779768778523686, 0.7863986586352463, -0.9291175592987009, 0.4220672595779915, 0.015129924792772309, 0.9034417919244941, 0.12758923639235809, 0.03925220736886148, -0.4864597156185897, -0.9825050208471422, -0.4633412868760973, -0.8255919617156282, 0.5012060048154514, -0.8928988350769194, 0.9833384278698221, 0.3742464125179208, 0.5750496942455448, 0.3411363831872922, -0.442227133538895, -0.5984913674508836, -0.8304853330326254, 0.26781108401951426, 0.44066271461924567, -0.08380268383901557, -0.7036737982610317, -0.9810246030162255, 0.3630421364135463, 0.535562478267946, -0.05580894398611225, -0.446835651775662, -0.24587349951984594, -0.8387307029306434, -0.3239966171060449, 0.7992432137698595, 0.3948088135920309, 0.40131844184042453, -0.846762633654361, 0.1618671412684216, 0.34216131676911954, -0.002105282382289575, 0.3150475743245993, 0.12531526375130442, 0.05247689254159127, 0.5623129537867018, 0.709975246697153, 0.5159685662978557, -0.9477040806590802, 0.6099577490284231, 0.20799463650832895, -0.7853185670138894, -0.1647545947066209, 0.5393506226559921, 0.11721985461638984, -0.08869771356783551, 0.5619755245824043, -0.6697404872080397, 0.2881351068339657, 0.8856606539093765, 0.553952975088041, -0.9924453225561805, 0.28781651319816026, 0.6274860752007503, -0.8166762358642539, -0.9269441958294491, -0.17750258609776837, 0.943434680982363, 0.6232059652473114, 0.2021388309561638, -0.3754982438221761, 0.994807493089906, 0.6607517256805806, -0.7280780539196527, 0.10972141578291006, 0.1298222501473909, -0.8273968923884436, -0.499731505923543, 0.8736436094122686, -0.6233680837603297, -0.37458780339770126, 0.7102327699530677, 0.18678732719320745, 0.32927158467544837, -0.5903513527170412, 0.8976168939435281, 0.7020012219617926, 0.5768294356601491, -0.032884164407363414, -0.9513313315350149, 0.38570832616794637, 0.754749979808524, 0.39697423697504175, -0.6848810168806136, -0.20101947706701684, 0.6451778949357543, 0.8220307811756897, 0.07458874277996896, 0.12397451361904288, 0.48938839778648235, 0.023801478777112806, -0.48102261577812566, -0.7720745956103339, 0.4827016079687563, -0.5351185256825517, -0.9653173197809006, 0.5809047214975216, 0.618288359219791, -0.23375168068969399, 0.6524668549979071, -0.610267377797612, -0.5429697546343524, 0.5831030449198771, -0.6227535490722496, 0.6088754071201858, -0.21311973432801778, 0.10135230442115928, -0.009917914404665273, -0.3069549496615036, -0.16493013712710924, -0.379050334578777, -0.6083064376361402, -0.26101196813289285, -0.8863935515737478, -0.15442305047089278, -0.3438644577125938, -0.6523723164759871, -0.9870939589931935, 0.06330449775621982, -0.8630797891799131, -0.22061870499569136, 0.3118684324175498, 0.9441564239112423, 0.40185645610447396, 0.4936935480491942, -0.8997593242828856, 0.9052892176928136, 0.5436199671241573, 0.14794308519796084, -0.6482449509846255, 0.8035371427635098, 0.3760576399783959, 0.6032039172481065, 0.6751967572278388, 0.44248621451720327, 0.507392022984078, 0.5065483222520579, -0.8370865333093833, 0.6266272635548493, -0.397157486277401, -0.08223478031502807, 0.2773740512943048, -0.17524704609189423, 0.9492984223417908, 0.46384466221840426, 0.48316993586711443, -0.7819054005818076, -0.2908775927685012, 0.45940629131342003, -0.08969260873470875, 0.2376025652168885, 0.18733233555395423, 0.555928280314026, 0.772075429438366, 0.06001172672833088, -0.8607957934227406, -0.33863183166025146, -0.6947322735913017, 0.17868174257892488, 0.2989467495348712, 0.6302524475939923, 0.28923702144455343, 0.2793210067098122, 0.1326413771342645, 0.5087044305650839, 0.4367833470756082, -0.9841068165583637, -0.44362580512346916, -0.18123833748170948, 0.8398157293035737, -0.012249909571833806, -0.90299767343304, -0.6017827646300724, 0.5678289437221844, 0.4120287555546289, 0.3264928878449924, -0.4694552337049569, -0.8519083692287421, -0.7872003250916422, 0.9313738442916693, -0.5487878586026336, -0.8966700094598747, 0.04150635358691446, -0.3105536693605535, -0.8649265084208446, -0.1615664425241392, -0.2179875453605975, 0.23006155992662647, -0.8066930368972363, 0.3656857432544307, 0.965530749245654, -0.0381248405361756, -0.5952325968734957, 0.672619444388231, -0.7033179998894981, 0.5096453893794766, -0.3616533306864711, 0.14362497166355026, -0.4662964829208991, -0.939573384257391, -0.7186989063448777, 0.3580573097197166, 0.35045567675272626, -0.17117709873489706, -0.25004785383837747, 0.33557222610961057, -0.9138613419715744, 0.3820004383855107, 0.24219000340530883, -0.1080197326503709, 0.3384910528383318, -0.8706987036939702, 0.7389283103953148, -0.5496100315513777, 0.4626018359614974, 0.5904458890170905, 0.7433282209026999, 0.7716362341073886, 0.2878809032762779, -0.7807146935172717, -0.898381180908455, -0.4851426607100293, 0.606147034771533, -0.5500020723692987, -0.5446739231883972, 0.15277197471835247, -0.5323916563231732, 0.6091747016668461, -0.5616897117357234, 0.38494534883344733, -0.9420902804796485, -0.3489507781821044, -0.7650777816937104, 0.7471436936660785, 0.8772866448082905, -0.0750608392940817, 0.17962846057927195, 0.4006780750912262, -0.375793597623431, 0.5933538768802058, -0.11969526034213218, 0.23298893902460405, 0.2933659842061016, 0.645288755300311, 0.8729383874029368, 0.4732631607058133, -0.9841300454365527, 0.950347214252115, -0.6096513118661746, 0.9901191777782465, 0.287245273200766, -0.09193067663287913, -0.4744406681355686, -0.005854683029867358, 0.6324003145247103, -0.42485321071072235, 0.7451398894993857, -0.41834133269722473, 0.020629233413173464, 0.8619187996811486, 0.5285968842327229, 0.3627124134939743, -0.3954603989332328, -0.15006796882090945, -0.09245941962836568, -0.57769466577777, 0.9614189315683728, 0.43112762987071007, 0.754162443573744, 0.8227779487561575, 0.6300888080903349, 0.26136933952256514, -0.441605588725712, -0.4334419260209075, 0.9910272929654984, -0.7166811412275516, 0.8352988488364557, 0.9220548519396898, 0.5807064540477864, 0.20244802055124955, 0.26626757863418193, -0.13896910769620585, 0.059475943609917925, 0.9586013329781395, 0.5887057196602352, -0.3465793846927703, 0.056751733764207835, 0.45116589179048705, -0.3576268048895952, -0.6253375682220816, 0.9285547978907573, 0.45219189816795713, -0.028642506231768783, -0.08057938218285576, 0.6948266048045095, 0.17730049070827802, -0.6055223433984283, -0.6211643478313102, -0.15497303848728872, -0.24673652742105623, -0.21019086887709926, -0.25204179631268264, -0.7129493114896155, -0.0001560812305081427, -0.6338553516867305, -0.39878804867015183, -0.40806815809397, 0.45872275938341955, -0.4615870592859417, -0.7047872518781393, -0.6469370623436355, 0.5427696612552293, -0.7569627750784369, -0.027486851111291344, -0.8036158395327988, -0.9999106352007088, -0.25582289967120553, -0.4537029307005538, 0.5455618204223311, -0.9185654432749568, 0.5468737526400964, 0.8909588484076969, 0.5289111662695831, 0.7756637780545108, 0.9873847847856638, -0.0687049037291887, -0.4441832094755973, 0.17677397455731558, 0.976297054451096, 0.6445646039009885, 0.3314418128969636, -0.1314425477439709, 0.8333589923358697, 0.9469399862421295, -0.6121993786292583, 0.2890702237305822, -0.7111281099127691, -0.3044467427440243, 0.10820832782232892, 0.4263578837994835, 0.9585986261152584, 0.8029514575427508, -0.9544067907398639, -0.4415533130140423, -0.6111092171398924, 0.47728261540815864, 0.5740820441380552, -0.7933568625123772, -0.01154459485727477, 0.3839100946067664, -0.29900036580571765, 0.5675241544753367, 0.964654152152773, 0.46370486370271613, -0.2528294792622532, 0.6313958057981974, 0.8359556718221945, -0.43621370120384384, -0.8454675491517354, -0.9456877240878281, -0.26787563754681853, -0.0028172533146373357, -0.7973991303637216, 0.08682238289382838, 0.3349629561384728, -0.5529953398204381, -0.09783018114539743, -0.5060263170206454, 0.1738860531920534, 0.9328493251872749, -0.4560709377266752, 0.029460317577258532, -0.019427785375005335, 0.994693072746933, -0.9835412089970388, -0.6501629220779623, 0.5752240444815448, -0.6987331414239542, 0.04008536874071367, -0.6326025711564309, -0.7982221696988387, 0.08047839165711945, 0.36542416545822487, 0.07948717445408682, -0.07126216859806345, 0.2836127910052373, -0.10780528272096479, 0.4899757194641965, -0.9035949659423599, 0.5303508745749697, 0.2390464492671629, -0.33293621392438166, -0.4449944925010285, -0.4646328665658721, 0.3626436771473973, 0.3201499431861641, 0.8116045190747181, -0.07721231367329517, 0.8423704483934578, 0.5986141422273701, 0.9620245151465967, -0.09959473541044606, 0.28260458815169565, 0.028826191780616295, -0.7572288817203938, 0.7769605925390946, -0.507650182375849, 0.9651543452951195, 0.11122850440062892, 0.5843122313044915, -0.1016160787877658, 0.6553021896328164, 0.6107566480601456, -0.311272476036468, -0.9418308209549702, 0.9064842222428577, 0.2505422835359832, -0.12207163555993072, -0.7680777111530916, -0.5235063582094972, -0.06652603319483652, -0.19179837483009954, -0.5040893565308713, 0.7844356356594806, -0.6807059091297942, -0.08039979512595874, -0.22615528965113985, 0.28708459858015245, -0.25615819710632803, -0.2697559385467587, 0.7158389052476888, 0.9251040172463492, -0.106846592348248, -0.03100009841429019, -0.7071909704088475, 0.08923282529302168, -0.5459990066943567, 0.33701067876649504, 0.7499239335306096, -0.09866158992329899, -0.35010925095472567, 0.7340055921509328, 0.5770046033857217, 0.7023180668662938, 0.07180249101270864, 0.5916417608794988, -0.5035796735827405, -0.7799835786737173, -0.6325543422882609, -0.6077140032004376, -0.22976953939198208, 0.011932070577535203, -0.6246874156041164, 0.31213390604786984, 0.36123580365724073, -0.9258886887391438, 0.6611591346133037, 0.33192839407508257, 0.22998206585487435, 0.03987743109084585, 0.5801268611071098, 0.5179659636396037, 0.3540257078512361, -0.559929814961933, -0.8908075816932512, -0.16620389069492214, 0.2948213111872804, -0.9269114296407457, 0.5300726131765257, 0.3672299859174921, 0.6580514378190949, -0.33988829504718754, -0.9889834559791362, -0.6558231850451617, -0.7128008999312994, -0.5362119720395446, 0.8133365254271057, 0.8311234466841924, 0.9080146299888439, -0.2449954108346959, 0.5819754015697232, 0.7795187872650071, -0.38946730964050835, -0.0760440115750387, 0.9889449231513885, 0.826561519245635, 0.052412321788918304, 0.3400841611369787, 0.5995407101495553, -0.2161020372400717, -0.5440772619208765, -0.23826283711277352, 0.5256649072319513, 0.5949532646490299, 0.8149520668304124, -0.013566995777681612, -0.6167940248548303, -0.06274721426704999, 0.6916873166991022, 0.498899006992757, -0.7928605548030245, -0.6489691622886493, 0.8276019784263018, -0.5832820191826389, 0.5954672247287796, 0.5376481637595387, 0.5740428298907876, 0.061250465095522566, 0.9728691123671589, 0.024397561335112172, -0.2707032215859875, 0.017264503655633145, 0.008126550048866088, -0.20337758891659274, -0.7460062512941903, -0.9223217170477229, -0.48908489057563, -0.5230526304010068, 0.6171251127642978, 0.538441738455798, -0.8478393020022601, -0.02207904220714041, -0.704813802509141, 0.9776806110435909, 0.4917014311136483, -0.12376920999956398, 0.7504810802762789, 0.5853902788420537, -0.11714496934983898, -0.584987251237131, -0.8772281382174079, 0.003330322858609458, -0.07395926676850628, 0.26125960430549955, 0.5080912814500509, -0.6939671202846278, 0.18755772353841604, 0.9700679503992609, -0.03733528412973475, -0.6943517124206373, -0.6617040967097245, -0.16724702777241163, 0.5755873521633603, -0.5800989074994596, -0.3767521451561837, 0.3105787776275699, 0.6007655595527692, -0.3784818192032813, -0.4514302628851661, -0.8378940157226462, -0.5076863145570938, 0.2395878524266688, -0.9843750846695849, 0.13939299665691407, 0.5229489886654357, -0.855221730772554, 0.3736462805976686, -0.8678770959545152, -0.9417363243077554, 0.9276489705252977, 0.5015726334819026, -0.28507134467657047, 0.7524245884202894, 0.9771425622681444, 0.8734695472244758, -0.0509193342922758, 0.24965700078312403, -0.8219538873526653, -0.5067588674443422, 0.8855885341083134, 0.12882309026197203, -0.6265967959599532, 0.5112505241578191, 0.1580912747442067, -0.913348910738595, -0.5203191881687201, -0.36194346745814565, 0.3795599766945159, -0.5134840734263542, 0.14353831259089933, -0.8766273438405339, -0.2715092405128845, -0.4776079612106874, -0.24837774448506922, -0.5367648959810074, -0.5770892256248248, -0.2189153877613068, 0.7479791685149058, -0.45411107391151484, -0.2110682003546962, 0.12107709400789313, -0.5802229015320355, -0.5813261141890076, -0.3310464592809499, -0.7462623450369754, -0.5622422854808027, -0.10919180528620442, -0.1725483960383869, -0.11599551742020187, -0.661554209048274, -0.6483458053310514, 0.03615023542087403, -0.7816193386086763, 0.418753866886173, 0.24808388437274576, 0.18174160581130394, 0.26479538698709537, -0.8371568801190918, -0.49139638976135913, -0.38756738886467645, 0.7638068266130349, 0.02350568809333442, 0.36708736684391385, -0.6029108822589193, 0.046601138290651045, 0.46022202157312475, 0.37661407226194465, -0.04499039402284977, 0.8457698342937525, -0.7834848793896294, -0.032188380979176534, 0.37566315609475875, 0.10025727831045095, 0.15745924013532298, -0.12761955980864537, -0.7173944756985493, -0.8231794431505046, 0.6776697871071722, -0.0808131297003496, -0.6926951784126947, 0.6023973569211676, -0.7449535176996975, 0.7399888808297039, -0.23077323303629527, -0.9123874865260035, 0.9694004548977928, -0.393624832971015, -0.8189913745272084, 0.22776216714463526, -0.8851634953197332, 0.7781040670185875, -0.7078990792577906, -0.8061433680317291, -0.32080415421693065, 0.89697313077325, 0.313438979258186, 0.6229717267015651, 0.4903311793186873, 0.26294440278154996, 0.661731524643707, -0.9973948543093463, -0.9056951209892823, 0.3757890743468124, 0.8055206093391356, -0.1050629560469345, -0.5637824154940849, -0.5934297232183381, -0.04069392227048563, -0.6952795790993036, 0.1689196129776509, 0.534714194305592, 0.03089995050275607, 0.48242806255690973, 0.8088289743649424, -0.21485460205592455, 0.08008683220423829, -0.7537757173911512, 0.3057143824632149, 0.6814447424746441, 0.6070378855218885, 0.9948692761274609, 0.018330694784324253, -0.9838383220701519, -0.24150509496902473, 0.4779657153786485, 0.9203720707576613, -0.04187351953110885, -0.28301584604426133, 0.8395704193160065, 0.8587873823243812, 0.2719073873399733, 0.13397787730260502, 0.8789488707709165, -0.6820751896092969, 0.18704499651354856, 0.16731889856211257, -0.8822310329816612, -0.07521343356516152, 0.3916603507718328, -0.033684860471854705, 0.552822782126039, 0.6985052694570602, 0.9710450817766239, -0.4395983253593714, -0.23906522757748738, 0.3693174826082526, -0.5169154183858815, -0.7020072120643228, -0.7195728073910777, 0.21965396165736872, -0.3702376080245475, -0.4257688606841292, -0.3335543318539891, 0.4737657304186562, 0.43187412307137585, 0.4536917173375976, -0.1834239342906947, -0.14148067715738644, 0.902498936925173, 0.838793982266425, -0.5255902274110411, 0.5170407102316539, 0.2833938918331189, 0.17173988858503608, -0.15099552824867524, -0.9402009156243685, -0.13399914834112292, 0.15299577107507178, -0.6378147471166502, 0.8670317625606281, 0.3362347109501356, -0.9354046337685014, 0.2033692875186328, 0.6394262040638858, -0.8445205166187233, 0.49919866822638714, 0.7003638589577521, -0.21509810815966057, 0.6525397102156125, -0.8263358885865253, 0.6052009711703834, 0.09643512820838596, 0.2764547601272547, 0.19018508597093753, -0.7523208822403225, -0.07640807872041577, 0.9573840359409611, 0.48059984927471344, -0.22888570919609363, 0.9159024262962681, 0.4445941027510636, -0.5946186749920801, 0.6262224507396852, -0.7025428879238236, 0.1605366247499398, -0.37555610925529925, 0.3692971075719764, 0.785185370180776, 0.20151742536972406, -0.270344566954164, -0.803228837417693, -0.183752846364859, -0.21391890677338465, -0.7084052013856257, 0.9239854174712987, -0.6612010685133043, -0.8449899874195939, 0.7906050804532718, 0.3098738108177308, -0.4715830579522331, -0.467786971848293, 0.8996037908487311, -0.849274868591857, -0.8169922330504631, 0.5238116777560045, -0.8127749033318921, 0.18822946091511605, -0.822095688514463, -0.6123401563283686, 0.8363170826047621, 0.7471025931689808, 0.36418491359016314, -0.8215630406393939, -0.9778763098523553, -0.4873628586036731, -0.29192507328487016, -0.7582862713462499, -0.76237775620674, 0.053234065184679435, -0.7567788777165874, -0.4269225423339018, -0.13481022637006834, 0.6353482954080698, 0.028484945451506594, -0.5350458530078936, 0.8385216678799432, -0.037695131095780265, 0.1491504057891706, -0.6960861793993969, -0.9578405864016046, 0.2687075398572083, 0.539226431199374, -0.6793776529017383, 0.5047060958504319, -0.7293615144855494, -0.8298801085183289, 0.7288239891530301, 0.5491358826596258, -0.7378276824718664, 0.5805512114066245, 0.9942598222201726, -0.1508726217237717, -0.9685073502124302, -0.7614726815456117, 0.4414312463978123, -0.14708724513623128, -0.2798999586312807, -0.8759096115589, -0.5810697800977165, -0.2951490668507444, 0.6862167160300436, -0.41576806161345625, -0.10178951882533238, 0.013788980092745984, -0.20023183420972912, -0.5721517106862601, -0.8083901869041881, -0.37791018914216323, -0.8146477765500277, -0.3734428968837029, 0.647840504940411, 0.06680300497838054, -0.45027265545738193, 0.6734651491555226, -0.27256696498479926, 0.13136429568784047, -0.6275070951090091, -0.11358420089052079, 0.453250324727714, 0.906516244068684, 0.8058595892607212, -0.6105782481572117, -0.8294478736969986, 0.9149365666733587, 0.5763858622453493, 0.20444000912372973, 0.7283427331648455, -0.7132425899366426, -0.1329604292980633, 0.5857842029823104, 0.3231830828394411, -0.299819997228443, -0.9913159444003918, -0.6762953761915467, -0.19096478598038025, -0.2171004770474838, -0.347718384680767, -0.6023300959172457, -0.606556202007408, -0.8398777615366597, 0.07899484428983783, -0.3080192968034354, -0.8973215315264962, -0.6701965876725231, -0.97894339849477, 0.9732030237623182, 0.04649317460615343, 0.13659575489692655, 0.922687813719741, -0.38503139915480666, 0.004973510530159375, -0.2940872475694123, -0.9278903257037172, -0.7895655640660912, -0.941913535924809, 0.6158072178631191, 0.6263696552282854, 0.639995984132014, -0.06374442294056926, -0.7271260296835931, 0.6463773414423255, -0.4133419328853123, -0.7821124136895625, 0.42474369816905533, 0.030512704388335754, 0.3344879354735515, -0.9074097519485194, 0.838881472625497, 0.0020230737812190203, 0.9435545043363149, 0.8304409008326974, 0.43491568458001195, -0.15377418736520254, 0.4783189542620825, 0.20967007528573545, -0.20667340365355424, -0.9967303441981716, -0.6409010097567893, -0.1690786916808107, 0.09855831789634006, 0.5073918228037728, 0.44796360997551155, 0.9248326940072422, 0.3636260612546345, 0.7286005485882312, 0.5685895215686352, 0.2726986310508899, 0.551555516799277, 0.09270794133868465, -0.2514459144621053, 0.44620120648581296, 0.38457681360532403, -0.8632113944265325, 0.6979980572438231, 0.5767783419157098, -0.3699617716703647, 0.491895444268341, 0.24079636266024962, -0.06631213428858618, -0.8661530791230745, 0.7247475689370286, 0.007602576229722491, 0.10864428648972635, -0.04365782225210668, -0.35403598668523406, -0.06006402641241437, -0.029240585915124795, -0.9145319271093622, -0.699264227831494, 0.6537675990127327, -0.35253834548713225, -0.9465899099608435, -0.8269927278134841, 0.8026279897003832, 0.035480218100937266, -0.636258070459762, -0.2201526078473195, -0.4125079227078372, -0.9109415793875133, -0.5741563003196601, 0.6952071082568523, -0.882164616576554, 0.8399502211819307, 0.28062147245764923, -0.2859740976977734, -0.04612727921328186, -0.8276010397613915, 0.199029603418293, -0.9203658796843477, -0.535348990305808, -0.4207621058201252, -0.8101944517392259, 0.10409072677023934, 0.3070997852805406, -0.5362924822161548, -0.7381881765710572, 0.5775394516302896, 0.5768146176855771, 0.8769736856682435, 0.9747871260352075, 0.12568175079592447, 0.09499149876579116, 0.8342479971855248, -0.6061394727336396, -0.660391959899596, 0.1875395892566194, -0.8831819957454983, -0.5386181402187977, -0.9758214563083012, -0.4913006093398302, -0.04791470229799355, 0.8907595946664817, 0.3730329658306537, -0.7943973776918805, -0.3719520325535133, -0.7534882024587848, 0.5725255282344084, 0.15355314033656398, -0.43518940104436865, -0.10039165477738421, 0.6974851862571352, 0.4857888865061353, -0.7719287796074574, -0.546620784114983, -0.9125688405764587, 0.4318066402646261, 0.45981778406643925, -0.16169687353435203, -0.09238695136072983, 0.19580264257537183, 0.8586742590743122, -0.9289892949367093, 0.8059933591451576, 0.1977384125825261, 0.5759503109589634, -0.19996981104151823, -0.426892935785649, 0.6535309680151138, 0.31262686333568057, -0.9681505017011267, -0.158045452812428, -0.9587614538217086, 0.2561241238975016, 0.5995451292219047, 0.6801725496172195, -0.3540868331112237, 0.5268307127178788, 0.9156810206918355, -0.3489092866827852, 0.9218700449070105, -0.6169488039756952, -0.7525585584307961, -0.1005832689752384, -0.04696361518580394, 0.7033411959334017, 0.35501199571891395, 0.9283613302130287, -0.7246502763423943, -0.6464075041779613, 0.7466898749128394, 0.5646755473751686, 0.5260357651217613, -0.942005150955284, 0.4883025175597344, -0.176425006831455, 0.4291491873772848, -0.8900429625544286, -0.28586617011457216, -0.49344573160134364, 0.03941862032967802, -0.43683664389824606, 0.6649863883620659, -0.6584728876096657, 0.8563445942975101, 0.4039319442137297, 0.43703894474964566, 0.6936999447978167, 0.24359069302577696, -0.3400883045949361, -0.6401838520100109, -0.10057830048272765, 0.1336944037901151, 0.46095005173387893, -0.9997875674583272, -0.16067959079260752, -0.9662852280818313, -0.19318047026248442, -0.01455583932242055, 0.04300300913912625, -0.7280448228222178, 0.3380963989266319, -0.20652319706525435, 0.18797908570933775, 0.7805039910843008, -0.4056561623854327, -0.9752962603925883, 0.5412273304679944, 0.257714701038275, -0.1701974363569141, -0.11497517943062974, 0.0158956143993072, 0.9546789243872653, -0.03107850565989434, -0.4263636913081774, -0.9957070478588164, -0.10913602893299834, -0.20641011199091674, -0.20809246340697585, 0.8931072715158783, 0.9861065385560559, -0.14073678695527225, -0.6285476108918049, 0.3208499532387925, 0.6979740943955308, -0.44547104898705503, 0.8131576980104245, -0.44317699712306524, -0.8387039156275908, 0.6428419099096256, -0.8689754113428931, 0.3054381070850918, -0.3441568097816603, -0.5708665801001882, 0.12798229035758824, 0.5551691324147432, -0.25905472535617724, 0.14177828483560684, -0.3557633732795371, -0.9404119737086145, 0.7555715362939468, -0.9824276427364944, -0.08709298746237759, -0.04503091666882586, 0.38563716366171064, 0.5347164686336208, 0.49787406176082816, -0.8704036241039215, -0.4287925218466171, -0.05145493524095257, -0.15331915476839098, 0.3225943019754065, -0.8718288770378357, 0.7769642231247373, 0.880162675892769, -0.6150307820095939, 0.9660688993258744, -0.6873419981792579, -0.11497830174695478, -0.4971290437067206, 0.35206704896793806, -0.20473765479232675, -0.6401506657361073, 0.36956439835900534, -0.02391200734602994, -0.03233186791885023, -0.4193919032010436, -0.025587376002276008, 0.04015593460994471, 0.3274365540313333, -0.2785731750357767, -0.18123549308074716, 0.47837428117300473, -0.2899473603976215, 0.8129551393767604, -0.3755873237790808, -0.5512151160135532, -0.8193055025034752, 0.24393351001653674, -0.34023509196024837, -0.9836211082193635, -0.7539126186482239, -0.04744854382382191, 0.1593989459781171, -0.08476964359523498, -0.1347697606790561, -0.3702833434267072, 0.33917130924772665, -0.8239521703246844, -0.0487507383608905, 0.6445747521997887, 0.5140383834577087, -0.48765523334208294, -0.2196334345781541, -0.6299761040368512, -0.3432924020311352, 0.5662744823023804, -0.5291047463552581, 0.42326786646238324, -0.8293239921660838, -0.14797665102045476, 0.5508412172083319, -0.03367404946273678, 0.2298218475952487, 0.48439508909918727, 0.09423094632364948, -0.19818492347471905, 0.23145659574837252, -0.5638340969883355, 0.9027034326514067, -0.9711186086880721, -0.19947063269260634, 0.384462110926757, 0.5137083319378852, 0.3807931172021972, -0.4209076350306804, 0.7728072650005864, -0.23481907819328662, -0.7108245950363354, -0.7288304147818787, -0.47688464136976894, -0.8371793804090468, -0.14283619931288838, 0.3344544718917779, 0.2553039160535586, 0.3953751389851734, 0.6217708550215795, 0.5745504435736775, 0.6104804903975249, 0.9239702405785051, 0.8385599128683838, -0.43375211504234956, 0.4638392553328956, 0.3696773002526137, 0.8908946026466853, 0.7147651926665, -0.08083658358812706, 0.7751453544311253, -0.7121953634070812, 0.8647227823191008, 0.3267399038633192, -0.2116846609766474, 0.10706321879062419, -0.07584485411811559, 0.8220645694848019, -0.7548098019179048, -0.9959234441850326, 0.3752715557500277, -0.4148174599403467, -0.9876371293794874, 0.816054288472255, -0.8318656564702602, 0.4906153640824684, -0.14588113105496125, -0.9837887826879537, -0.23092263877908747, 0.6916925630594839, 0.5234950016056921, 0.07836003076077236, 0.3907177096301797, 0.06523109925609627, -0.1931422278035808, 0.9971409787718748, 0.20270344485057423, 0.9578747418454989, -0.03416283574956602, 0.8037229943255229, 0.22280268546018123, -0.4960634780245463, 0.5077882086940713, -0.33630770723561576, -0.33021727110974397, 0.8002555634791542, 0.9477069547661876, -0.7765252544976662, 0.9942152027350906, 0.8446477519959334, 0.7587605027107869, -0.16221955674452238, 0.9062250480066942, -0.950497864715041, -0.6295478013602629, -0.7515388676124122, -0.3362489246980298, 0.3448544642438005, -0.7054344613417134, 0.1687684189407075, 0.623634239955384, -0.06520777676549794, -0.7365662008936142, -0.03788140739329249, 0.9510560867492357, 0.19850068493952322, 0.4967918082729481, -0.0953630336245761, -0.5841370787205411, 0.6614160007592578, 0.7792735344279123, -0.8702477751412288, 0.6732818952286792, 0.7453203791646943, -0.8630531952489271, 0.8977514573324521, -0.7627097369794187, -0.5034752096112027, 0.8218131105078434, -0.8076154084333296, -0.020893077700159024, -0.7270001725291042, 0.9878235358189151, -0.9449926520100078, 0.44914313314434096, 0.49681740557504495, 0.538187384244549, 0.67641434948409, -0.12001759918126531, 0.8676061413993281, -0.852473177401391, -0.506082112647551, -0.9419379103271557, 0.4710641875710906, -0.9423779580073319, -0.7646385283165007, 0.30546626703683777, -0.6231462319359777, -0.4907773677531504, -0.8144458622744333, 0.7631049921461404, 0.6959281414996945, -0.12378671634145544, 0.07153331299844767, 0.5958792077143402, 0.15523023259864277, -0.4968683593455725, 0.8141170009504479, -0.6995182236361097, 0.24316963132065683, -0.28207558807988353, -0.9521892067105162, 0.9883959111326155, 0.1859421973466009, -0.7427946402796566, -0.11155986408435292, 0.11803115841809486, -0.7976532473716997, 0.7377286001492009, -0.15181962621919554, -0.6324097398473523, 0.06763696685072595, -0.05144320770096389, 0.6319007680870015, -0.22995205448260703, -0.8805243660365929, -0.8658326770080553, 0.78764946721813, 0.17303498163747433, -0.8668458060772919, -0.35942373930649896, -0.6991422889362622, -0.49953122831050667, 0.11944952828429956, 0.26678391580413896, -0.6256514026362612, -0.085162885026145, -0.9639244740835216, 0.3807351895172104, 0.9237866729600832, -0.4618823176321556, 0.9090161919414914, -0.37041977246824676, -0.21437847053889292, 0.219531015960035, -0.5334530773600785, -0.10264824156356989, -0.9246846072249462, 0.6691610991624979, 0.10619709783031639, -0.11274619150557919, -0.7502774971462483, -0.7683067802377606, -0.2007667059699625, -0.48171291808868455, 0.337100080137136, 0.3440497297153866, -0.898072713556707, 0.8782375571001779, 0.05427322451769667, -0.6392845839276691, -0.7355164587127951, -0.29985922381138863, -0.806992607592548, -0.5038042043282551, -0.7236851221744489, 0.5360873543105256, 0.6727055342534578, -0.9566993059566007, -0.7009625392275185, 0.42098856366787585, 0.3848782892589393, 0.25596229124773573, 0.9900792817478623, 0.12566818139221247, 0.5759784478018874, -0.5724886032870715, 0.13074415507769976, 0.8395095910741326, -0.08612426234795412, -0.2385683110892669, -0.5384612512310631, -0.4645514902107739, -0.2617596549015382, 0.21922678585420496, 0.782653365812003, -0.9564973420961418, -0.5122364606202989, 0.04014248024228584, 0.34747627117347646, 0.12069106847454925, 0.18365451529160115, 0.8307313574840092, 0.4981922104825587, 0.8120361617369842, -0.46693825056934424, -0.5602768425734683, -0.1446677786736874, -0.4099893400061767, -0.10588267154837205, -0.5877069572922144, 0.9094202414853156, -0.09465957324594432, -0.611063590778403, 0.06517030926695533, -0.9952114648183394, -0.949755969746483, 0.6202812773413362, -0.7775991454360636, 0.5997947745702552, 0.2489832726780501, -0.7511091367648168, -0.7764645755252502, 0.0709725906778853, 0.36577569492831263, -0.4443656581551685, 0.22156593115254353, -0.6009336314602143, 0.5755399260990599, -0.8982592151355944, 0.23261660163796205, -0.039634008927400766, -0.023727671039507126, 0.6316716567730665, -0.14082777235661492, 0.26601708604552377, -0.4760653719177035, -0.035957496697064384, 0.5042040035066728, 0.3180890559883358, 0.5120241205067602, -0.07960253509415627, -0.06089216670624853, -0.859146357022333, 0.5606866554668455, -0.7400799980692971, -0.06622961172804165, -0.8765879366739666, -0.6884087026002783, 0.0981433115301471, 0.7065972100065476, 0.5289471764346878, 0.5554614464027041, -0.6685532674428334, 0.23418903203340746, 0.04038709010466146, 0.36679630037019284, 0.6994837324888676, 0.7385099314080226, 0.7360731115218342, 0.603032331966862, -0.9934117070144586, -0.7229686350408797, -0.8631860578522523, -0.7577368136450786, 0.03033342335593603, -0.4351076181765463, 0.01060844069222222, -0.7099497149233709, -0.7973238720076199, 0.5186294960581597, 0.6125726264111739, -0.7303625856000164, -0.14453614844226625, -0.5315463466696249, -0.5935319362810427, 0.1713419655757431, 0.6715206009176611, -0.32901997507192116, -0.6980390165965358, 0.6954606158525736, -0.3228449538404825, 0.29764190628622766, 0.5262474726596731, -0.6566177853489772, 0.6676263195925323, -0.7275154832421824, 0.7897749562716043, 0.2699842243111741, 0.003614682681783332, 0.5302344031901758, -0.6085251239403371, -0.3273928211109145, -0.5906780965905938, -0.09724065017349015, -0.6060254912396716, 0.5481430862323957, 0.1267291058372111, -0.9166733319074332, 0.9244450444410013, -0.9859761668086044, -0.19692647075807246, -0.9434334405171785, -0.7748952322203453, -0.25158693888340844, -0.19039310509526408, 0.964491125400659, -0.26993461536052044, -0.10098403492427677, 0.3639356438915389, -0.6547663217079547, 0.37434702844876333, -0.9039414640251751, -0.1118574564376531, -0.7079019486859854, -0.5347885144918332, 0.7525671121677453, 0.10428496514945129, 0.03886384895886308, -0.8540791808432997, 0.11177811706906393, 0.3667876042686984, 0.7868349117605524, -0.7588833877968986, -0.6743941294962261, -0.03098464551927349, 0.4981439504709364, 0.21472623153263592, -0.2174642313482109, -0.8288683393085301, 0.5623447127598387, 0.7099493066224141, 0.4204807386422724, 0.9669569109911254, -0.3262100923659983, 0.38736910483850884, -0.5627427035171191, -0.361327258828805, -0.8070452583709029, -0.4064643173450968, -0.9033356032540221, 0.6887737402672669, 0.994114423284232, -0.11492371153934444, 0.11626235031209164, -0.7801151775511752, -0.11423940249475328, 0.7521268129576304, -0.0771422466469267, -0.4356255245277292, 0.03208325408047297, 0.3697806351601671, -0.747029612986452, 0.4577297688684123, -0.21649838322259018, 0.22159085900221198, 0.0946803973972481, 0.3312794773710095, -0.4278408553246551, -0.32477597491511134, 0.34400361307983496, 0.7723844145225311, 0.46887905809992714, -0.7103174412724678, -0.5912996086443021, -0.4589301388645508, -0.275494249274324, -0.8572248095289894, -0.059420473576049515, 0.5671362933969408, 0.8206976917559505, 0.12080603641907173, -0.8567911333121241, 0.777672105735624, 0.7576579992911878, -0.34240444803294245, -0.17456284632812324, -0.706721009691057, -0.48836318652268473, -0.9603234825061069, 0.27439326643501594, -0.8494945311588802, 0.7005497705636625, -0.04119678141335714, -0.5636040517697904, -0.3972900768168668, -0.08005187801042069, -0.879767508156855, 0.05583396424015019, -0.7954466513613283, -0.28098343024023187, 0.9091479892567649, -0.6002516479351101, -0.9763670011568786, 0.5743786938443902, -0.8190233803430036, -0.45398509252748065, 0.37859924669654, -0.6577830989999474, -0.16167614044511125, 0.2730263015522296, -0.9427858445139614, -0.7669080039798564, 0.34089897014350545, 0.28709005478339544, 0.8018506307795077, -0.4091504981506098, 0.17308025608318633, -0.09237807908493245, 0.10596312244759276, -0.2129275411537639, 0.25821219232413295, 0.6986901005726294, -0.10758727291719827, -0.4973369828954517, 0.8295873025669527, 0.03038496644721711, -0.35948728192570245, 0.30794162474754283, -0.36337291470361466, -0.5911210393473216, -0.4518443935109375, -0.6889928463321422, -0.3862732576151129, -0.6128415361093815, 0.8678574539689483, 0.47417600735193144, -0.45329024132137796, 0.9019224409361362, -0.8412657505834773, 0.5194031943286879, 0.8409551300742513, 0.8584772037705781, -0.17543128658770768, 0.6262152774477519, -0.3520125117665758, -0.28646960177461134, 0.9348504948221756, -0.5844569427977309, 0.30588079808255597, 0.967791556298623, 0.4346663142175555, -0.169053721539143, -0.879420573776869, 0.04103332125163561, 0.7939208895848244, -0.6374448951406151, 0.8813620109709028, -0.5580977332756192, -0.0707628604497057, 0.02857834364464229, -0.3708433149699659, 0.5711351821891073, 0.5286756971397493, 0.04648014046881177, 0.4528200648854319, 0.9815154605093204, 0.04221808120269133, 0.7413907114803284, -0.6104643711415383, 0.2364631392266825, -0.7196221861601853, -0.5924941124136263, -0.5717453154823566, 0.015116674160592147, -0.6170479460335134, 0.8754997073895792, 0.30238265523897967, 0.17135445960858142, -0.17955168723474313, -0.7074892429726638, -0.2884541535274221, 0.66402081639095, 0.605585142791468, -0.8286436647098792, -0.43707269495418055, -0.5706362847451465, -0.05779532248654551, -0.8991471967313918, -0.5960470300970802, 0.4803117658523681, -0.7260220015198873, -0.1047807468633617, -0.5671677154813588, -0.8244118163743157, -0.6295936352883591, 0.13482689765883538, -0.7440802905928292, 0.7316319147677575, -0.016088318997127482, -0.6726931777575005, 0.24956596322444446, 0.9424393939910858, -0.34112738198281534, -0.06139043596730387, 0.6534226890980954, 0.8023970653083439, -0.9321438871737608, -0.295795512843825, -0.8882544054718668, -0.5615723251011868, -0.8913898887778362, -0.2382643506383335, -0.9640343847113984, 0.2039390702441657, -0.5391945489416488, 0.167719156082472, 0.01582080743625447, -0.21814289745012672, 0.9525090088460144, 0.7609322533727851, -0.7581176927235802, -0.25068104641721756, 0.5255531002923406, 0.9987764125908714, -0.9699091877469652, 0.669410336716382, 0.7111512437094141, 0.1949332952273386, 0.5119544575899391, 0.24869237015490864, -0.11877796501006688, 0.7897588152113368, 0.018318550255218113, -0.3688955823399238, 0.020653586949342717, 0.31399709411893006, -0.1175805688961904, 0.8824321623388147, -0.7853429396192029, 0.8359160186397301, -0.5408595667086216, -0.7574050722530223, 0.10544883935600535, 0.9969474438001675, 0.7536538832392656, 0.35987697507052685, 0.9505244924096743, -0.437016535266606, -0.09046017145518426, -0.2235468546040551, -0.47436315533653417, 0.7119283898342679, -0.9748828050267251, 0.15040209348227918, -0.1716941622976198, -0.9444454860312661, -0.3184292392948187, -0.3983131610896822, 0.7176291932581926, 0.014263848446843364, 0.7153085149836904, 0.947092785311076, 0.654270017846674, -0.5722418046564293, 0.1103742304717985, 0.16263736264181405, -0.4666689095182366, 0.8521346490133987, -0.031294758843630754, 0.16129186385966587, -0.5107939628119764, 0.22630461800814983, -0.06317883658550372, 0.7918993757107893, 0.17504957180899527, -0.3052716616288951, 0.48244564579973104, 0.1567183289683456, 0.7742957961627992, -0.520045757257805, -0.762933752235498, -0.9109136116620731, 0.7966795883714786, 0.2167394413183943, 0.6784212398862677, -0.027632322807357168, -0.23317021044578268, 0.5668687989399055, -0.9239094875599361, 0.8568039657368802, 0.6023890500145228, 0.18707881501746648, 0.1623237382519671, -0.7355510463550261, 0.5712121191553259, 0.673298587270057, 0.5349498950594831, 0.06606947359702553, -0.5871657152809557, -0.7177496831418972, -0.7592193857267402, -0.8583307835034601, -0.3216944046413406, 0.9200962522536802, 0.7599683751658426, 0.4282132251376696, -0.7169453180113996, -0.5739643127238507, -0.1390475673883791, 0.7886716895445711, -0.5572976118181572, 0.3830884740692415, 0.7797246959539326, 0.7869606703361793, 0.7622865352245214, -0.686795162874988, 0.2161624290262414, -0.49813141713640086, -0.5494831012342654, -0.16457425112100244, 0.3675493851138374, 0.1466909125635607, -0.7048416087705092, -0.10608334918419371, 0.9434020255042905, 0.4000150583232316, 0.8907056785658645, -0.6892680569604528, 0.19782382983650248, 0.5647250473194223, -0.3223908890323619, 0.9106687492042453, -0.882766495854399, -0.9307418776311076, 0.5522392551691879, -0.1304655353857036, 0.07059093709650122, -0.28754760639128074, 0.17934513069539637, 0.7706292356124262, 0.5659305656427673, 0.5616489904876143, -0.7812393858925237, 0.7963829313446213, 0.3226365648462286, -0.45942067997817304, 0.9069395337779149, 0.623339887395788, 0.8238549300642446, 0.3435978108883566, 0.2880760352198293, -0.4076361363056302, 0.1450814168495269, -0.4811354132340353, 0.369391089581806, -0.7746273534124968, -0.16025524165376015, 0.39790609831264323, 0.6288013365177783, -0.7725355110154677, -0.5586349568144087, 0.4391494578967101, -0.6918460562733786, -0.13758741432841526, -0.6607574607002158, 0.4460378451830702, 0.6871437538377452, 0.23022385191995198, 0.9480128954206442, -0.7112990332895941, -0.4404683702342702, -0.9352336553368483, -0.4287936347829284, -0.9160192005999861, -0.42871697714070844, -0.9073592198876466, 0.33060487477102374, -0.31587884261571153, -0.630130348608378, -0.18926706836667972, 0.9444771172726869, 0.5663127020652854, 0.4492782840983911, 0.5210878890831365, -0.8473728777265601, 0.05459585984052007, -0.4790630300517378, -0.8993569519703537, -0.7461525246138425, -0.3572868286762294, 0.1859591526544122, 0.979805521026033, -0.047091154730140916, -0.07737504101117154, -0.27868344267473955, -0.5181164669828926, -0.8840185018209448, 0.4633911970504705, 0.2696056146462884, 0.007965018307485527, 0.9806997416335499, -0.6791144952983466, 0.6353861750540801, 0.5868202550303103, -0.2693069521812326, 0.06406297614036438, -0.21073380566116073, -0.3631139907862002, -0.8118916270035055, 0.07417568136684327, 0.6305495098367826, 0.21713987610018326, 0.09262123309295589, 0.4346750509309083, 0.7731048306641355, 0.2161245708920183, -0.945182958991547, 0.6137915822286568, -0.7635056798049442, -0.3970017297129007, -0.8850413487329847, 0.6232784456371423, -0.16175949773171672, -0.3063858225649736, 0.8637870344201366, 0.08190613234614053, 0.24209999145217376, 0.3797683714469109, -0.3148455546899869, 0.8488754344975886, -0.5092763111535876, 0.6943478554476006, 0.24546012349171042, 0.3319801825205906, -0.24247035998995758, 0.2914116306307639, -0.7834640600538665, 0.7712708099835552, -0.42141195799155007, -0.9261889382013553, -0.49298020313067403, -0.8511674773492377, 0.13987057413245552, -0.18503535382267144, 0.342436720625352, 0.6852898251921444, -0.34046470332621914, 0.7490212225414994, -0.40294279673444744, -0.861775007408526, -0.02725033245544517, 0.11372872511699672, -0.9451006138220133, -0.3853804117919619, -0.02038568969576593, 0.6341575198079406, -0.268363159212766, 0.4599930882239063, -0.772360143107961, -0.28685791667239546, 0.3086185128255572, 0.9370822441050515, 0.8193824973282597, -0.20030885472289017, -0.5681095241425915, 0.8631845052596565, 0.5165147942503991, 0.055626118652030376, -0.8806728916111282, -0.9710765705537518, 0.835730175322769, -0.21111123372599394, -0.16809622900365917, 0.9752935337548401, -0.9730911104310667, 0.6005520458873554, 0.5922802263776006, 0.7527707089305848, -0.17321673209418798, 0.8430930743632377, 0.7200499354520846, 0.23036079060222536, 0.01028464680432184, 0.4900328306340742, 0.6047132991284947, 0.17672338994813952, 0.5610837664011097, -0.8347114257726225, -0.14132278825716882, -0.25937451992196636, 0.2972088924298881, -0.22200367783979358, 0.3700225273644686, -0.829685354454061, 0.816022480129891, -0.7965661349562827, 0.8240802078989193, 0.29740754608820175, -0.9011611240093931, -0.9193439905529803, 0.3088676978196663, 0.9742905926927465, -0.5400428501563186, -0.6723458754555169, 0.9518224014511694, 0.7710677407461384, -0.20962794404459717, 0.014275029220489621, 0.49938875219489987, -0.7519257223989064, 0.9481182481572616, -0.5352230331062775, -0.4333292253865966, 0.21918294170091346, -0.10717488361404603, 0.0037157852211628928, 0.9177746462253298, 0.1800661621647841, -0.4412766999435003, 0.1720811083804028, 0.3890314053951529, -0.5330207129772171, 0.6047014191860052, 0.1527264029062554, 0.8828833080588829, 0.39533999439754774, 0.5127978946691543, 0.1918526543995649, -0.5287129952106573, 0.2697302784590734, -0.4693034170067216, -0.4087575277209694, -0.6449898460717858, -0.9844859111262094, 0.9000982932380965, -0.5320716485981289, 0.04323596486535042, -0.45532605376851465, -0.3578715134728456, -0.9972517252577853, 0.9187756710698216, -0.48404060117155523, -0.4572801223083891, -0.0683294793222311, -0.8376391556976122, -0.8151550859441761, -0.9234431815857314, 0.024146868322034054, 0.6021001474294432, 0.19427287544449845, -0.5198922524277425, 0.8509825052653528, -0.8585745346067055, -0.2646669222637643, 0.6961464236567712, -0.4492021883896753, -0.030594442456196802, -0.5811423074369864, -0.36436511453007325, -0.313471994277128, -0.7716094356169418, -0.9703876184664271, -0.09126550263124233, 0.537577339746663, -0.9315878430785622, -0.6256306486054275, 0.30043703381825426, 0.8207586615510924, 0.4471841118015878, -0.7835098785254688, -0.6004365315659455, -0.8821303996103873, -0.44471104736523515, -0.8133222675675387, 0.39391062182742775, -0.017793735074232764, -0.9439083248796138, -0.3708934748017456, -0.6747904832604321, -0.8731535208897618, -0.4887342554402987, 0.5369494708486953, 0.9221592339182523, -0.7343318218488262, -0.8051103751542632, -0.9421141701608045, -0.2774744236550404, -0.47814962387645155, 0.8856161250125669, 0.6973360864816649, 0.6324442693107482, 0.03899666727872497, -0.25008520573489434, -0.8884711527912428, 0.03290110057209539, 0.5020740550131331, 0.06546364872865529, -0.39212031510139456, -0.28222454264766683, -0.10818754790747742, -0.3587528545930905, -0.7757852095366045, 0.5283895563856298, -0.9921087822178947, 0.8870516362687926, 0.8167787117680982, -0.005681264229610017, 0.9183474943819179, -0.3079839980235921, -0.6009790071687218, 0.34011647232772924, -0.19568958859289642, 0.9912355159484991, -0.9663291285634323, 0.37913959651357865, 0.9176065653701573, -0.8126267871878252, -0.9450362122464424, -0.6719586784968501, 0.2454233338142635, -0.36308440475600534, 0.6821038405060336, 0.9485019019951235, -0.6772987890305953, -0.2734865102761157, 0.40893716149867965, 0.5811848216197788, 0.20307579484189198, 0.5009940634091341, 0.9214156968478928, 0.4462877878705396, 0.5197217170338735, -0.7104904962890337, 0.8197659508626989, -0.6049949992891268, 0.8486184547500573, -0.10317854724913023, -0.9043428039901427, 0.783607356286613, 0.35720576898651113, 0.5192527270042924, -0.9036946540830348, 0.3841629177738519, -0.19159647035004435, -0.6362182183061296, -0.25803134301818753, -0.7785227374906256, 0.33920496363168806, -0.976075023129064, -0.37330129651794874, -0.33040610545917626, -0.5855692252530724, -0.29531912330981, 0.4371720540578936, -0.8522793598644272, 0.41013949701930996, -0.18498977220162582, 0.78087923638709, 0.3998452114988331, 0.4946189162997827, 0.7382996383692473, -0.43855109496892575, -0.8279118644335652, -0.09272518804329177, 0.7532764145438788, 0.21871466882746793, -0.948171943279593, 0.2691878001400507, -0.9837683645626001, 0.8590045655079626, -0.38498959514966935, -0.514840623550064, 0.17372942471916497, -0.7541259967020992, -0.7860410815668437, -0.4733800973682598, 0.09989399646646246, 0.5948103646497789, -0.41735158867787114, 0.5703264438181324, -0.6859018146954747, 0.9048434734150324, -0.5011919357433512, -0.2215438747428795, 0.09259193363398022, 0.7058183749000431, -0.07026360615562988, 0.6536795040201535, -0.12687725245745995, 0.46837766964562233, -0.8768698003848727, 0.007969730166231148, -0.7544511090410972, 0.09427392742298157, 0.08258951794761771, -0.7098279917910284, -0.7933194865251925, 0.738000759069787, 0.3577888567626679, 0.5404638689177881, -0.06356164346103643, 0.34489561491764587, 0.09412649557087627, -0.8704528203367143, -0.4688554565128289, -0.2012362855638956, -0.7085218716087911, -0.9354411449360314, 0.6277874767460321, -0.43168977788073803, -0.31615907671635557, -0.3486446580811646, 0.18492311503487713, -0.8100653221288534, 0.9220501380912347, -0.9034963456212528, -0.599550010673606, 0.5022144350079667, -0.9529864611084156, -0.5219131302354507, 0.8484139845784564, -0.6397496687845592, -0.8204841110350005, -0.25551121992971315, -0.8140827271141591, 0.7703542198536939, 0.3368620342297368, -0.27548903854862816, 0.7628014868246584, -0.015356714110587921, -0.9362058966649398, 0.8101888054347604, -0.46059114910998034, 0.3493039471491508, -0.8535663495570975, -0.5332829630873217, 0.5642192220094326, 0.2614948258895111, 0.3279737394321891, -0.590256576449194, 0.35708913293124844, -0.9770568805195314, -0.3868170957638095, -0.9105647055473376, 0.15465582539893785, -0.14938968302012423, -0.7688381699384397, -0.24519239167248852, 0.947445742113812, -0.44179704027224154, 0.8075548133315089, -0.16505919973722616, 0.11702092486938165, 0.08006714501270817, 0.5042556169142025, 0.9400249472576747, -0.3214196742023483, -0.8787613305834183, -0.21561683146746558, -0.23702936972580924, 0.20184869943164285, 0.4483330597362436, -0.2946554224729916, 0.3285300416987238, -0.8869676504854545, -0.25886426752375225, -0.4994508771907302, -0.18762969861495948, -0.832570279693817, 0.8632424908814655, -0.4613021257551677, -0.7375948570503135, 0.38843504346701496, -0.2008799739418321, -0.07608476214789417, 0.9774042564085499, -0.860194943232623, 0.5625259663426949, -0.5079079611297737, 0.9203349106921661, -0.1714358891488772, -0.5592509979217695, -0.6798914508159826, -0.9202963222210878, 0.454350184327589, -0.4718370989575793, -0.5148015927625877, 0.6479511971069782, 0.9770344266962769, 0.9563699447648111, 0.19715527763608254, -0.26676443781838977, 0.1394095363914063, -0.47722618699856834, 0.5384023694594655, 0.7121074277117507, -0.2895235141252357, 0.47894822726170516, 0.347822564369757, -0.5245925800057878, 0.2846406711980569, -0.48251317060899, -0.20634425347855423, -0.9947763852924527, 0.5141456052779065, 0.9150142150002556, 0.6728513679120063, 0.647980283151659, 0.3219020371182595, -0.19047607084569518, 0.1270346499157291, 0.3766392594746264, -0.07075717540533355, -0.13075736257249515, 0.7073946837899401, -0.9355554106026878, -0.5096706480141413, -0.09939248990443761, -0.12037233507330947, 0.20357158090303384, 0.39442827868650854, 0.34230700742264286, 0.993642435346149, -0.8509275225765816, -0.2459902281895523, 0.2929178638009855, 0.5854066477247724, -0.8508021791369043, 0.2090101669595441, -0.21174441360366192, -0.8718017412783476, 0.40920339336798617, -0.5547835126285223, -0.20032038987313117, 0.8732488942861223, 0.485320151178082, -0.1967492752394684, -0.6881946766958766, 0.6762841565133777, 0.04485572207232158, 0.17250183747320302, 0.33934053906251194, 0.2580391988045403, -0.24752162924006527, -0.40570331161328843, 0.8869454491823761, -0.5826716491553381, -0.05973153459964964, 0.8326559575425236, 0.5743807385949873, 0.15120150076739103, 0.5471102377693322, 0.30060937257931575, -0.7370584595777501, -0.32532665398982097, 0.4278592553646341, 0.5034706094907089, 0.9751594524452798, 0.5593316947745854, -0.5901280921286638, -0.6129119032599442, -0.2962830313451652, 0.18957889465768907, 0.22006092491478335, -0.9073971613083107, -0.9181189370623726, 0.7005392010620346, -0.6053897944888875, -0.4081361537868391, -0.4903462302600099, -0.4418896109265904, 0.11532225479713132, -0.8123965760723484, 0.972553791766249, -0.6580143683007265, 0.6855777819684101, 0.4186737843483883, -0.45485157988219393, 0.9578543376334909, -0.4478803807843379, 0.4973321773034547, -0.411192646353679, -0.4282890854673296, -0.11894797913811295, -0.5323433583379384, -0.1701265510657124, 0.7951745463731348, 0.09015332674589471, -0.7681778060528188, -0.6198591038429919, 0.611734946416634, -0.88723586449036, -0.0022142493952450604, -0.48552402052532484, -0.3459033315527591, -0.8910491813166443, 0.1241823509737785, 0.7598531781579387, 0.9439220611457662, 0.3338851620615009, -0.4706537485175353, -0.6320322833169159, -0.6185444507938234, -0.541527443451975, 0.6974916387273158, 0.5044832458876813, -0.8372089365799571, -0.8217775785310375, -0.8455593544000277, 0.018306432167964015, 0.70778134226907, -0.6259870019656366, -0.5504646911061768, -0.37276173242408905, -0.9517247941788245, 0.2685050000970528, -0.7613384869507824, 0.8538700227359177, 0.04031672075807902, -0.4779970867032617, -0.06200752450984304, -0.08228985153159796, -0.7770845685276917, 0.16914651781401036, -0.5820027009496844, -0.398304763889213, -0.09791812065742778, -0.03949620781927665, -0.5608269464321725, 0.03329574095034871, -0.41257320575266654, -0.8269285126848478, 0.8230089792721731, 0.6020295465725691, 0.15861800007536453, -0.6633100192437109, -0.993243984906693, 0.4866186429996524, 0.9336368780514577, 0.8628953252603195, 0.2712443562550697, 0.7868042706501657, -0.6852809614388422, -0.39932035151546863, -0.859357811824194, 0.572266358193066, 0.6136219307840813, -0.9275906773274636, 0.26433280959636907, 0.24377132977087346, -0.34951987233731585, 0.5632506220644204, -0.30441410074965614, 0.16643465910133526, 0.7188506806878243, 0.0964228061374699, -0.644285612031082, 0.6747506498519231, -0.953758139915784, -0.0032394250648297707, -0.9510126650086204, -0.5544695620225388, -0.9719767509450923, 0.17308344703099832, 0.7557653480269686, 0.1380697230320016, -0.16319117041426234, -0.3448138191056844, 0.5903656917838838, -0.1116264510348115, 0.8296616269455155, -0.7682754271340821, 0.044148245386611995, -0.9516425854434674, 0.3796391907437542, 0.17955566497983622, -0.12866328978382535, -0.42230187324902557, 0.6292561777075758, 0.9584877050991885, -0.8120578945547321, -0.403733745986665, -0.7669936582384738, 0.30475391622030146, -0.8859341838683308, -0.031188287521311198, -0.5852162502107512, 0.012350019548977409, -0.925119654187085, 0.3892595384757742, -0.28961025711317223, 0.673637239770378, -0.34377793613892105, 0.4234059229128939, 0.4778516376787545, 0.4859205761404881, -0.5576788330697786, 0.1242788135635986, 0.37056391493981033, -0.9189643872377729, -0.9695802976436623, 0.5585648436479669, -0.07158996021894315, 0.31132266597835834, 0.38175683024640383, -0.28702850013658865, 0.5314291037854171, -0.7974860277334967, 0.5376253613416169, 0.46462181811542624, -0.9956791968375547, -0.91620321446126, 0.5468641059134851, 0.11607804467253158, 0.8747954995183393, 0.9989862283772983, 0.4450423206344012, -0.39075184973993915, -0.3577606256541379, 0.23650306707452073, -0.7492181979367405, -0.9048846264223018, 0.7655529798687177, 0.3934931355175546, -0.39570190638550584, -0.36856534068595903, -0.7602066582388718, -0.6303115293590753, -0.1793973163874385, 0.4096330230301237, -0.32985992480883675, -0.2060528619899542, -0.5425598541948553, -0.8175159170774864, 0.3666097820113252, 0.642817491031439, 0.3661270933393237, 0.38762336847782675, -0.033175283582612236, -0.5310699014914366, 0.34204178242495686, 0.8217431182323129, 0.33547451913129867, 0.4051239679676426, -0.9507021973600052, -0.27855704978228135, -0.5358402452130071, 0.7650541003761118, -0.20473557818528532, -0.44652999383392755, -0.6655097741827574, 0.2502164336891948, -0.5382429046911916, -0.9105554361278207, 0.7033113741129533, -0.9600303830008929, 0.3847503631757694, 0.8145551230711654, 0.1843403072649854, -0.8699773422574208, -0.28930159005534617, 0.9198940428478137, 0.52442428240536, -0.6393969084396804, 0.7251940240899435, 0.29065441677662585, 0.685752166113883, -0.9210401634247212, 0.304229308844425, 0.06720968675723071, -0.8294184300934673, 0.4094184620576342, -0.6479184723551878, 0.40522725544953886, 0.2089366366426193, -0.3134627386773978, 0.6296375282983198, 0.08944004911566017, -0.4550462380254663, 0.2003179265248467, -0.8042370022847287, 0.9799206597044443, -0.6271767198467357, 0.4329396885661927, -0.23852039356035104, -0.9529830938187351, -0.2282496395611655, -0.8391844471846479, 0.22852746133517177, -0.4827977813424069, 0.5823062230955094, 0.015709026238442503, -0.6622044849986302, -0.5482206408843784, -0.6038803764117118, -0.9209737308646524, -0.26023722821464834, 0.37940675221380826, 0.46616653275323183, -0.7998021480026536, 0.9680665551408016, 0.44625138023194855, -0.5706666200713264, 0.8847646519071652, -0.9489472724348285, -0.9043248246097244, 0.8921173306919175, 0.7721573322625426, -0.9626924361857356, 0.5637453159789259, 0.9171413833123216, 0.46393835725388266, -0.493074137680398, -0.8148772495646806, -0.07418604326827105, -0.3269600083350772, 0.5138673409365284, -0.8531361959278139, -0.819427388274387, -0.18709072952285077, -0.5540430163387882, 0.8243471441789718, -0.760249051768358, 0.010843776238233183, 0.07927389902977566, -0.13649804496736961, 0.6628211071054586, -0.5655592382076249, -0.030857148053918593, 0.4518765656054169, 0.35789176349915497, 0.30800802928830406, 0.8301622855741757, -0.8660231155877771, -0.6532347963479574, -0.9327691605305937, 0.10854392326698603, -0.45543975347956334, -0.3326245080182928, -0.6408181599963496, 0.9006997079452281, -0.4240820971226833, -0.27509804614100175, -0.9764229566377878, -0.051885422874259124, -0.1635203997806507, 0.4266331978250091, -0.4477421460306783, 0.35737875045088097, -0.6396439998002466, -0.7884343917125358, 0.2759647108943568, 0.40751153280661945, 0.46825264640123354, 0.3601714896516177, 0.1533136747734931, -0.5471040771440216, -0.4993256270613522, -0.8643663743434657, 0.038504812842879765, -0.1135090747379397, 0.3719283352542657, 0.7562973688364363, -0.6798563713804902, -0.31242259269229455, 0.6458472345771942, -0.7120657383600242, -0.9542521108258786, -0.46099699958177487, 0.14235379747994892, 0.5842501087409546, -0.19127950132733273, 0.5957173696776168, -0.015670580043565918, -0.3080895890193944, -0.9700452716725978, 0.45596831385938774, 0.426202525180164, 0.6790683773948352, 0.455622807666932, -0.8262995813890739, 0.2180182554220993, 0.1909263302652109, -0.9992067063197745, 0.41357850404827734, -0.8175017398451216, -0.2914867264480163, 0.26474507797513924, -0.5923695102343107, -0.2428977933795189, 0.8413076536774187, 0.9055937266317047, 0.34359384772778623, 0.16259851305408723, -0.43259513766093893, -0.011725128328468948, -0.07814401556801687, 0.8777829743435002, -0.4850666109528976, 0.6759947032384004, 0.684349666765296, 0.9558726074980508, 0.8703345859043083, -0.8538919449003011, -0.5217526520865594, 0.12874315776304202, -0.7078293182416577, -0.1843671798277995, -0.11903214771358006, -0.4770568032671487, -0.6828113683076011, 0.9962079650143962, -0.9304633674701874, 0.2968108628111268, 0.19082167242693582, -0.07185555894458262, -0.3334719018115704, 0.5967250486234441, -0.04383876571804124, 0.8627848401544367, 0.6017971536583728, 0.5676933891671068, -0.6393846691311711, 0.5897827575964849, -0.9342671571403709, 0.3874566058518347, 0.5031516436257022, 0.2422363558796512, 0.5640819364089811, -0.10031540733820288, -0.8660425855128868, -0.5685658196567425, 0.8603288116996572, -0.7867779812318885, 0.04231234229958547, -0.36200739975822094, -0.21652061409857204, -0.8015610546006784, 0.6608106083949077, 0.14804444000806427, 0.13121593132951093, -0.26161467574542385, 0.5702515570326476, 0.3179998832800315, 0.1351057134196907, -0.2689113037891635, 0.9921461509028311, 0.40894768523349856, -0.30394965632924786, -0.8817103150974286, -0.9640710070065923, 0.21663504137144862, 0.18823516146723906, 0.39714150395144654, -0.49403412464850316, 0.02191241418841816, 0.2690606200627348, -0.6127132388925545, -0.4215804276755064, -0.3752138944228305, -0.40911386754413837, -0.5537269904645885, 0.5015016956873155, 0.37171050246286885, -0.9590563470781508, 0.9365320685061955, -0.6919613345825444, -0.9382194544030935, -0.1842821433843984, 0.38677408111885825, 0.32805761493437324, 0.9105746009979128, -0.483922133152453, -0.20357346116900144, -0.8506775047971, 0.6671273988687181, 0.5439873199117513, 0.5510655665572006, -0.053233850515810976, -0.010654454895435483, -0.32562791666653945, -0.67990381689306, 0.08886635786886221, -0.5377362938610248, -0.7200361006795457, 0.05561245739826304, 0.7929090361310007, 0.47290405171674, -0.902227589716152, -0.9606055889305483, -0.47556825848039574, 0.5817591967096716, -0.4249982640283425, -0.8325440444673247, 0.9052299918439166, 0.8490081349735368, -0.4976415011533768, 0.18759646269918795, -0.4384169085424552, 0.5149584392572693, 0.3558201389382578, -0.47158620835703413, 0.9270892651718614, 0.6160565278724106, -0.23654704057341225, 0.17948406010270235, -0.09430865413233747, 0.02916383262995792, -0.914767519543042, 0.23948298246501398, 0.0029886160867023115, 0.9901151508184132, 0.18883393559155426, 0.9518507387661541, -0.8005366684243578, -0.24033040892314994, -0.9474592946180209, -0.05144551672368802, 0.8703564132880626, 0.4527562793792279, 0.6893682884728918, -0.6290944477923379, -0.7805799438938616, 0.00768809906498924, -0.28746260657576905, -0.5697115153996644, -0.7187620686650609, -0.5259763243554547, 0.35281933517978836, 0.535362735717458, 0.4324716801453392, 0.10720118646017474, -0.8753854753367816, 0.7151851446676865, -0.5753069474486683, 0.03940442945775757, -0.5864004626182444, -0.8837527111649504, 0.39644913931442893, -0.7575963140921596, -0.5388982301959611, 0.0001147481974039799, -0.66969889355874, -0.43310570382098157, -0.7928200393330289, 0.9088936434233688, 0.5233849909299542, -0.3463026174387067, 0.19809134232457426, 0.02313413099999817, 0.033637160429214275, -0.861317004116642, -0.4084222114669014, 0.3874318507974097, -0.5351768842893421, 0.9322321494322785, 0.7286139615647818, -0.8194954093810749, -0.9011105180368078, 0.6967318942353988, 0.7594745730597987, -0.3228961174759728, -0.40519353956411686, 0.07506700947131462, -0.35261938513633173, 0.1189150214716741, -0.4666037518375812, 0.5537288522849675, 0.22391431690317054, -0.37252388029220906, -0.8458815015672998, -0.06546057617623102, -0.9102778362969481, -0.5459694139928155, 0.7428690802656812, 0.6644203165251386, -0.834324245679855, -0.6497842334143415, -0.5864481862062485, 0.35314835284064383, 0.030045216312591494, 0.18347117479591613, 0.9790983297649536, 0.5576580043448933, -0.8150669321518265, -0.470777660746833, -0.2148649245785823, 0.5853277130461716, -0.2892194882041521, 0.7223734824076742, -0.9492243207213085, 0.014250170982841759, 0.5624354669861009, -0.6268506459293259, -0.8080867226662343, -0.16347198671031937, -0.0551175393401675, -0.04631957825155952, -0.8342201189800897, -0.3931849625028103, 0.554751028620518, -0.5909669419030281, 0.11616058161424858, 0.3835121478152601, 0.4447032970370819, 0.8174098396356284, 0.41993659440503794, -0.568312012634298, -0.9376518435441821, -0.8488286163554053, 0.61363577386223, -0.924048123800941, -0.005971155351782498, 0.5003942457705259, -0.869920646833032, 0.0739296269888221, 0.3918634260863545, 0.8271776580950911, -0.14466077767993357, -0.07486128145178261, -0.16147983902059204, 0.6090349876352759, -0.7182358272587013, 0.3377482834301455, -0.6383864214081478, 0.6915849945690935, 0.5916109902378666, 0.3126441282082195, -0.8215873637375319, 0.40893976270999044, -0.8121418399246139, 0.11353598468971215, -0.1118370279051979, -0.7823142519490385, -0.1919385018244748, 0.5655346476489145, -0.3654628638842803, -0.7389954004126458, 0.8411204463537343, 0.13780947045914482, -0.32787806703550104, 0.7116819753110895, 0.07222494762799747, -0.3344834541176551, -0.09260691987301994, -0.2649652223986394, -0.5271105730198964, 0.7617689911154266, -0.9064862087931613, 0.9081917933903567, -0.635809289201889, -0.9077584276485571, -0.7154324618285766, -0.9142589504268059, 0.5672448194489619, -0.2538736290041528, 0.9894816449256232, -0.31475080114919396, -0.26024505398071063, 0.8400785207973769, 0.23982905762643947, 0.32295749097772153, 0.4345773903164658, -0.3752006930769969, 0.9092156070471507, -0.7389822201471141, 0.2543594569212142, -0.46934685388975184, 0.40928872596172194, -0.4923712462404106, -0.3606664783915208, 0.1646706466872161, -0.9277545316676847, 0.5946919824239749, 0.31115122721534894, 0.5475863822997995, 0.4160681528849217, -0.6521941946220637, -0.4145717163180396, 0.6101495283257419, -0.02083962188061972, -0.9687197226840127, 0.43521777257774286, -0.21164934307357686, 0.8604879904848084, -0.12305739114160485, 0.7107660469892727, -0.573518878689746, -0.6932840444357051, -0.13657060002979593, -0.3854322106076906, 0.9991324090806641, -0.19904575595197938, -0.33940068970658177, 0.9647499113480495, -0.4057869229702884, 0.9031116876213259, -0.5602750565913739, -0.3871734275605472, -0.8242387194521497, -0.7236833993802094, -0.3760909346467771, -0.9004541655592082, -0.0379978187653387, -0.43556294698531706, -0.6464210346476644, 0.21457473670262184, -0.3850520247975988, -0.8265792342000917, 0.8074386469116728, 0.17655296777635132, -0.530797496370764, -0.07211404058918469, 0.843050395947881, -0.5802093481859751, 0.7069454076811732, 0.2630872402639366, 0.9567017877843405, 0.5423084568422594, -0.4966389632347652, 0.9147160253014259, -0.3958698250183956, 0.6218547139357953, -0.3708363523251945, -0.12487902948491314, 0.6610474330372778, -0.9612804166579931, -0.0861151100206412, 0.8745728231723708, 0.9470456178004638, -0.045337527748813944, 0.9631135252753948, -0.37311987948888814, 0.9143653870961395, 0.04495594864328489, 0.2905151219070681, 0.9679464595297322, -0.5866431330602284, -0.7613249704904583, 0.38721364643972955, -0.2759719424210094, -0.2278481956659424, 0.37339154860343804, 0.8763152567940329, 0.45079732516461646, 0.936378484018541, -0.5838272228094097, -0.6629884694791801, -0.6913438672955561, 0.3999357421793852, -0.5089069233001604, 0.08219783144486481, 0.5516822755925623, 0.25971222225102, 0.42283435936005964, 0.0022962245509861035, -0.7496361577830546, 0.20803160500326823, -0.22508194873449994, 0.3484169058988542, -0.4948384032464803, 0.9598068779261328, -0.7877038520582116, 0.638930416941061, 0.938319438193804, 0.09548876593556455, 0.044866648518696595, -0.5181950503244619, 0.28458537654781524, -0.06387960713277852, -0.48379402887378853, -0.44613196209850114, -0.7230251477397684, -0.6616257743940901, -0.5336556836834094, -0.40629531230104643, 0.5133273570190997, 0.8603437552492803, 0.9271580473003722, 0.35763379108744586, -0.4877037225229879, 0.259152837932487, 0.3432883501393662, 0.12029916162400434, 0.2699564779109609, 0.898143682327442, 0.007511399132787533, 0.887842629734436, -0.9181212210869436, 0.8078727354396387, 0.8300176492422775, 0.2518507533101728, 0.0899237988645436, -0.08521230860021767, 0.8938481474581195, -0.6414121526105219, 0.3325113113538265, -0.4018182451083232, -0.9640242819808105, 0.6793295869617906, -0.7969228507918396, 0.14010664326313038, 0.7887194715972097, 0.2948373842559491, -0.6951731078270884, -0.8611596625388882, 0.6056888041523185, -0.6707730695840368, -0.45798349954861584, 0.18230192857354033, -0.4113966159238738, 0.6673112951621383, -0.10338402679125758, -0.4022503796826946, -0.70126209123588, 0.29498764595399596, 0.580375278249593, -0.5156856518487503, -0.5796203941293507, -0.26690046111505006, 0.4256647940336935, -0.723726077326927, 0.42328301298329296, -0.6653256580453215, 0.002960413160313813, -0.3581846955979786, -0.03455798753984407, -0.158725453591261, 0.40188297988596244, 0.7924790456684918, 0.7490407682933533, -0.18730862983467267, 0.8839726407320561, -0.3968847937642461, -0.24278628459041451, 0.7556232585358968, 0.8134925974224081, 0.2450501538123926, -0.5561607159983861, -0.40423415792329553, -0.5795928936695618, -0.9585792057515583, 0.8534457851342685, 0.9353998630964322, 0.39808802621646966, 0.1994037373371369, 0.9068755794148338, -0.6132406005191942, -0.8830077225551547, -0.22692916742286973, -0.24241393676456946, -0.28320425349764555, -0.8639805352126617, -0.5644348441702915, 0.958679130079882, 0.25130747928178154, 0.7286105336186741, -0.91899234342804, -0.07035745399976623, -0.4078273700398063, -0.3759428525687356, -0.987588566506777, -0.1280904807758214, 0.6371379135154009, -0.7004330195201349, 0.06161999485564906, -0.6173688152844976, 0.7844376638016854, 0.31510138631478934, -0.8215359187683506, -0.19848684775964465, 0.3730705421870666, 0.24589927163955805, 0.9958797990579653, -0.13146142473131195, -0.6223087965193019, -0.612726946773819, -0.2542862558210153, -0.8103827914112349, -0.8499166333216821, -0.4856335734325796, -0.4173949763969198, -0.1637809544522688, -0.748203211326709, -0.5314063294624283, -0.5420061296797887, 0.008903215150891208, 0.9529920120351574, 0.12946902387692605, -0.3247641404092403, -0.9347872193246838, 0.8330647428195268, -0.8259933236538102, -0.5238009251086533, 0.32817605157945917, 0.9166336375901503, -0.850436972555161, 0.9090527595685474, -0.27700429352523925, 0.2611112873610455, -0.7075847027830708, -0.5918155585945573, -0.050668462272038806, -0.3228165972028325, 0.3287983570662891, -0.5343478744194925, -0.06285172323878019, 0.10185593093689516, 0.42894540215489463, -0.9578737643133124, -0.4979451659828873, 0.012265492978843717, -0.900123224435168, 0.4478891556822635, 0.5527327275385787, -0.8584819300328308, -0.8163026561573195, -0.7686691839227364, -0.2710785535331037, -0.6772210135213692, 0.7714228072331082, -0.9226591714781966, -0.8411357313434702, 0.9558588216409225, 0.690313640794953, 0.08822344667840887, 0.514857827824865, -0.6241103453723162, 0.026800766968052114, 0.7976988814369343, 0.5324252251218229, -0.8183216280591847, 0.9169862509410331, -0.3164165311874094, -0.5881341590698073, -0.385564729655687, 0.6008818862739689, -0.2971977178057792, -0.7762270288639854, -0.3059695125593156, 0.4003837320956203, -0.6977167541747016, -0.11951153488864152, 0.35789689249934153, 0.5007058245314775, 0.03132991569903165, -0.9037193821397238, -0.6531005813703175, -0.11731983744843988, -0.16898114341275994, -0.656148256248474, 0.004491523646415141, 0.127089072586265, 0.2647912369733352, -0.1262735780213129, -0.6879729226172009, -0.9654648084679909, -0.5654965602116395, -0.715151124518699, 0.1323553394541399, -0.301929910651378, -0.8997925498739405, 0.35191066381450864, 0.6349691524987515, 0.772917667161696, -0.9208577509481057, -0.789704361257711, 0.20854634974431652, 0.07805979078232106, 0.6582486312530118, -0.27249501214652083, 0.2491162600526433, 0.3366712145342039, -0.2184041063819262, 0.4320366590906417, -0.6562268653817116, -0.2056404218696004, -0.21500504652721308, -0.038775862371378755, 0.8130146563229492, 0.31127200824323875, -0.599784160116354, 0.7952957536081406, 0.6144536085624677, -0.8832477250667767, 0.48329849097345057, -0.1775434056638625, 0.3501026800438649, -0.21970796159776396, -0.28638920489163167, 0.912487264389501, -0.8308971674388463, -0.9035656701165138, 0.47283382394795215, -0.7969106760722711, -0.7924519287396967, 0.11870785921873117, 0.976854898823478, 0.2842405911465842, 0.7560452691301045, 0.271866179220007, -0.7932173016770365, -0.23166087330276985, 0.6897215045969622, -0.2118667334355211, 0.502933221512331, 0.10274737550953628, 0.5061572707955564, -0.07005753536481585, 0.1080744540587919, 0.5721268582781787, 0.7529188042222681, -0.49566386511911276, -0.6012833567232023, 0.8373575572271525, 0.19578568811028352, -0.22520523381381152, 0.37535006570575846, 0.8069092131312927, -0.22535165196979512, -0.10438214072133944, 0.9880770570834652, 0.34401699785188966, -0.6285730778722651, -0.7792394368429569, 0.8132829281052569, -0.5657957223212784, 0.06214239012655787, 0.4277697225508872, 0.6270895557193921, 0.4752920034484329, 0.8855225086789589, 0.13506455203631207, 0.8659087160773642, -0.07280310094001674, -0.3132121289198333, -0.9811495906792305, 0.13020427659269362, -0.08324876850283358, -0.25589298834749097, 0.5851028724392471, 0.010097601950403767, -0.30908433793499945, -0.8014391155072704, 0.3735801585038583, 0.20519714845220438, -0.3919986346266704, 0.19453035924169382, 0.7486528892076856, -0.06990556055164054, -0.3011044806394372, -0.015103880060408859, -0.7318060830380642, 0.9161173966542078, 0.7312957636999837, -0.7222788018969186, -0.3840814067901457, 0.29564183747993944, 0.9599874993163025, 0.23555952995617258, 0.6701076755608677, -0.5266904185861325, -0.5320138854337821, 0.11169166468902225, 0.4174731079133487, 0.3083884548313578, -0.656417348303393, -0.8868103086778174, -0.411496775815859, 0.16991679267340598, 0.701117192308842, 0.8236879050555983, -0.4659829893549148, -0.6786336404030187, -0.44508721765199355, 0.772695214043235, -0.3927805703656837, 0.1984533573626137, -0.4746025794924993, 0.42165465265127544, -0.13573487432789078, -0.7500996324010563, -0.30100120399336294, -0.6442066138028351, -0.641283061200304, -0.41389390699307893, -0.39961685189005336, -0.4292858895284981, 0.519639290573267, 0.7507291889555867, 0.029339396541447238, 0.954011486930721, -0.08715373154851891, 0.5158318387623602, 0.6946137046995913, -0.5943404486124457, 0.014742982062931675, -0.24813020404773756, 0.2249394680858412, -0.08653227151530585, -0.8473503792832109, -0.2379865408644073, 0.8898538772894828, 0.9332556472435138, 0.6363224272356345, -0.28088298900875763, 0.7370926888892522, -0.506045214937034, 0.9880509657109089, -0.06161669249713042, 0.22262067867234947, 0.3493480612476527, -0.21380775862457924, 0.8250464713513839, -0.639813437788401, 0.464943262210358, 0.379115362963071, 0.3805670761678157, 0.379961591721822, 0.1969521378945549, -0.911498595791296, -0.08143051592415151, 0.5475317326347855, -0.14033204871003124, 0.31139634910591263, -0.20282755386378848, -0.9268388724135086, -0.020181899779656787, -0.5231721832243015, 0.4422135264763254, -0.2510290404546067, 0.87470145146835, -0.7205690840522669, -0.36232474395154024, -0.7608263204946193, 0.726190641378401, 0.3470332442869346, 0.07999410756203629, 0.08977514640759043, 0.725687032090162, 0.39155566518890983, 0.6188315572310918, -0.2392003553086719, 0.30775589148036, 0.7348467669040191, 0.6334402108285127, 0.5247576450668634, -0.8244142515657074, 0.7892932460056998, -0.07195781908427357, -0.823439501050012, 0.0949307453562025, 0.9194578538007963, -0.837044500932562, -0.64529398637618, 0.9743557086074526, 0.8865661149370205, -0.9730485950493626, -0.0158560465098192, 0.517729481893503, -0.5505527808806419, 0.6053783754694728, 0.7948348554128566, -0.6828578756693067, -0.7895036882729187, -0.7361871554409243, 0.5906715836660235, 0.5665194321604252, -0.386059208467016, 0.3798668340052098, 0.3654428475633056, -0.504737441523724, 0.5557212388616155, 0.05706850758344717, 0.718111021554789, -0.17570155083032146, -0.187207402546272, 0.07417014079721995, 0.8600659141736953, 0.8987788496922258, -0.83846319679285, -0.8039367630751681, 0.5713757741564613, 0.015274481249722971, -0.4933291531439876, -0.8244369341101472, -0.6803425961061149, 0.5343598255226227, 0.22216902200948474, -0.7528149972601779, -0.41230264711479614, 0.13521442724089527, 0.25439963044207414, -0.4476550587055206, -0.11553488328807404, -0.1515504416050324, 0.14699662490079946, 0.429896282855027, 0.9275028799362321, 0.9567561901295547, 0.7762771497447205, -0.56448229730816, 0.8648322624774889, 0.7647187933665964, 0.00683380317966642, 0.4958275968145205, 0.817882453132051, -0.9111008998720846, -0.739647131880542, -0.27024209137821265, -0.5074522592281896, 0.6247555135098672, 0.722522046807822, 0.4613298784941149, 0.6004050427604641, 0.31922474034753767, 0.8203929108578127, -0.26482475282657836, -0.9759767402746955, -0.13257516907073108, -0.9735523102407706, -0.5854692388910234, -0.20082885658621752, 0.9121497109700727, 0.892639369875416, -0.525471018383667, 0.8785541380671564, 0.65416252427668, -0.34710956586225894, 0.8269601431659008, -0.704832332184171, -0.6997900488168347, -0.9460896971027164, 0.536727782779582, -0.42798404195798234, 0.3496384386688842, 0.4421847307681348, 0.13576140192830866, -0.666764772266363, 0.22689978921649878, -0.32827482422266985, 0.5806263183248026, -0.4683278610199182, 0.30171939361705324, 0.7390165009380512, 0.7004218657110457, -0.7102146562851699, -0.25956597383048474, 0.8115459171225501, -0.8033982993831152, -0.7429409228781287, -0.8045383591718382, -0.8371643680355194, -0.9961966058271365, -0.2927116043530016, 0.40912525171202807, -0.03714494423973291, 0.09223597504540271, -0.38151716969609906, -0.7677507228651905, 0.8713501897296518, 0.02433628346256489, 0.12206200162244607, -0.795220705347975, -0.7301667035495614, 0.9276074142649966, -0.24600783082097455, 0.37466115749735174, 0.6310604610726722, 0.2748147625706574, 0.9074543238739585, -0.8929876786118576, 0.3583009194873863, 0.8507914622293011, 0.5436700368409411, 0.7230796838030451, 0.0670042824915491, 0.30000533918320116, 0.9450503132809047, 0.6407885862572371, 0.2252599476251247, 0.9579413656564604, -0.7055058188026677, 0.6692881761852929, 0.07407345902278739, -0.12265972568662686, 0.08131605615953363, -0.006425937447623653, -0.21015189254978583, -0.3385760989660316, 0.7099758466373007, -0.19340332720894238, 0.8624490932458233, -0.16834478386852458, -0.15763068674138703, 0.0518865141414635, 0.06237747412711392, -0.5369740649012418, -0.7344826563188789, 0.07416971889684798, 0.5346255375485802, -0.7922076842135215, 0.9634322800801105, -0.9896206948259028, -0.46706735367191365, -0.9098820864236101, 0.9725944465008518, 0.6440140214298891, -0.14827457540138922, -0.7147717814711276, 0.14752733641478644, 0.10417981493695194, 0.9657683821992022, 0.14569012443031393, -0.4411435279823597, 0.598066580306118, 0.0958358893378839, -0.029289992526734254, 0.38297477430447424, -0.3405002782599136, 0.41481416080504685, -0.5008032567736285, -0.28938308624159803, 0.35373136215253753, -0.2318231942529314, -0.4877922675861954, 0.8014173822844946, -0.5691205419808401, 0.43755496586844655, 0.7185562556650973, -0.5184720409286854, 0.4331179752197456, -0.9046628221072763, 0.1479122897099494, 0.3128626512603727, -0.07859398825353225, 0.517344052951302, -0.8486633003362556, -0.7510895773448476, -0.11332570564844224, -0.4109762824809302, -0.6847806934391822, 0.4806704192813349, 0.49828483356063025, -0.06945451885481235, 0.5669096637205613, 0.9018223839538253, 0.41064742336907667, 0.7444237991182239, -0.8128713877199487, 0.451018420655511, -0.15477319974797954, -0.4176549905590925, 0.9371543063941297, -0.7448367800165352, 0.021054206658754238, 0.8223594209979643, -0.622058905469743, -0.9226859141107042, -0.4364898345663033, 0.5709011144533738, 0.8867613356226447, 0.562961559530244, -0.18057018609693998, 0.1995946918970566, -0.07911719672671991, -0.34050053603648434, -0.7450804889825762, -0.3653109893723756, -0.3664630796312103, -0.6964213176609777, 0.418186497143602, 0.32490719965774906, -0.24063310258751258, 0.5475364437016716, -0.13292687032162087, -0.4389545698435464, -0.22342474403746948, 0.23859103349934774, -0.1858044664127223, -0.2277644837233288, 0.5760140367415749, 0.09680544004696601, 0.4737285901140127, -0.9509568193897955, 0.9251618673028057, -0.5234741859867968, -0.14370614091596967, 0.411182432537488, 0.6381980301277472, 0.7461177324616497, 0.212071789284759, -0.7537843696510389, 0.8503395222210082, 0.9632836533063791, 0.7051188560956141, 0.44875639455290317, -0.9695599255396989, -0.8489927246573943, -0.6476939130678334, -0.7516174128457547, -0.7408314658132313, 0.1999771146682061, 0.641307593403708, -0.5745809774884265, -0.4228447885199691, 0.8056966674294217, 0.5094723121510685, 0.768081262277007, -0.15296351171915035, 0.2086840667462324, 0.1862617651622298, 0.8089873458412309, -0.623649452585743, -0.9110207016193317, -0.4774303385794183, -0.5856175331001647, 0.758617975624351, 0.7151997859943722, -0.35244904893840734, 0.07370861239402271, 0.05546469273139221, -0.9500052520086673, -0.19082766689880937, 0.17418826625246497, 0.3320031963485004, -0.9553595993831134, -0.39950308228703135, -0.41259994728841587, -0.7648046051885631, -0.20907130026123433, -0.9620214098279016, -0.6740996077559671, -0.34933705196652776, -0.8430928837086828, -0.33865590015840796, 0.7954734076923269, -0.43802582969347137, -0.9001204885863177, 0.328069122975315, 0.4212432356744933, 0.01340453574837519, -0.5198690303646536, -0.47836198726268586, 0.4389035921221225, 0.3100358133701697, 0.04629011806368033, -0.8997375630012725, -0.035236857783844, -0.9973735060960636, -0.22463850814322894, -0.9650013387577976, -0.06957427226384305, -0.30057122958226623, 0.9839207356175601, 0.10837888908075466, 0.9270442403799586, 0.12858249731418536, 0.47793552098551007, 0.48961223418180966, 0.9377910259114262, 0.1853584273764748, 0.27140677616485465, -0.15022011514578648, -0.7551311462247181, -0.39179938792489755, -0.23434654891869422, -0.5313877634255422, -0.7353481219416329, 0.6831732587075259, -0.922201179542238, 0.831962448580948, -0.22049295706038086, -0.2963627888029172, -0.36740138340187833, 0.39383489855244, 0.8437090560825244, 0.20435674978354812, 0.5568131131856098, -0.042522544602692225, -0.36841858823782525, 0.9743414579218876, 0.571615213730208, -0.7459333883810562, -0.7465011928104539, -0.5342172232088793, -0.4327787674704857, 0.40158659601855273, 0.03281364772458217, -0.3188476561390845, 0.46521175383575675, 0.7270224073682054, -0.3856786920276485, -0.0435422245967636, 0.6973067197000131, 0.48035306694414626, 0.3363887337854199, 0.6345898086095962, 0.03318601735885518, -0.11931652832264117, 0.4485187092576961, -0.6166283067399578, -0.5850315932449999, 0.3444187255897837, -0.9162197204562625, -0.9296317546523825, -0.2567583730557055, 0.5251669034623656, -0.8323709449896672, -0.6794661598795169, 0.9894492845774345, -0.39772636864504296, -0.8118779642026803, 0.09687006297457956, -0.41319098081414674, 0.4815059953271006, -0.5027131299945984, -0.3925815081919426, 0.7914641566503449, 0.08699816682872785, 0.6562335450403476, 0.2852535308422177, -0.44273935021154065, -0.8569510586277502, -0.6110404992713456, 0.509953425590383, -0.25418395478954703, 0.9275677516677996, 0.26431525795356214, 0.7671343822458954, 0.6532089882211549, 0.1228938801592725, -0.858444676444349, 0.8668280874105236, -0.610912597014565, 0.4376180574353561, 0.5752538514035053, 0.27937378156188597, -0.037369821751994214, 0.6977704218052843, 0.1112187607831967, -0.8987800947525326, 0.5528129086292979, 0.7193052648978759, 0.461001601560066, 0.009093405635069995, 0.11823063162964687, -0.7453453322567509, 0.9137712400486175, 0.9819836548585574, 0.3867283616974484, 0.3518493471577451, 0.5166107795073225, 0.7453074418937706, 0.12464674437495393, -0.42125518465992506, -0.8784051497860998, 0.529702532814345, -0.869509690183262, -0.3628043601390971, -0.1518432224432933, 0.5282445015283095, 0.6637850513143568, -0.08848253242135318, 0.07128644385057736, 0.3984029563127358, 0.13016714336911783, 0.33722884171792744, -0.8471843374966432, -0.8845900029402922, 0.7774537488681175, 0.01570163706578498, 0.3892679472752618, 0.4757984868835723, -0.004671720860342843, -0.3275543346496559, 0.913295199918351, 0.7483358930104962, -0.2506319308111311, 0.02924471424097086, 0.667460231071265, -0.45318248841291564, -0.8915682947968355, 0.0899598189930757, 0.2758656082579538, -0.031558530008667596, -0.1639670579817285, -0.0016704119372661363, 0.16835746055958767, -0.5686027184331126, 0.5504023477274356, 0.22868133018415415, -0.2604312584193471, 0.2578499881633043, 0.6595623755254592, 0.7343547819946854, 0.7619582683657262, 0.8859556731586213, -0.7346759183722766, -0.9120079892616377, -0.03271468326309601, 0.33171039223005305, -0.07370003766065092, 0.6250706614024055, 0.0543811220301309, 0.5789774379945414, -0.8280453610643133, -0.5269562767826046, -0.4611267125299887, -0.02837747922917999, -0.23035871527644813, -0.004548976190180243, -0.974168725449466, -0.12572453945525686, 0.7251413997635672, -0.9616572536611381, 0.8172232498873448, 0.0340697746317542, -0.22308559424005225, -0.5802346172248387, 0.7094127584590864, -0.4056724829981895, -0.11714519523150857, -0.05521102919797061, 0.4548329663400408, -0.9657802654326475, -0.6547330715552808, -0.34337202955558954, -0.2128689145051499, 0.5848905202538219, -0.5251068711678784, 0.7057352851529692, 0.5094623894071648, -0.12608394321791772, -0.8308108792535427, 0.9398791892821334, 0.16248661998015024, -0.5289473369419437, -0.6362613212797867, -0.3785916310563566, 0.09208459489751553, 0.9842446662365383, -0.40507760828409256, -0.1277810079886772, 0.6880929932293987, 0.4272186111919427, -0.43038000386072683, -0.7395156262547702, -0.15374034648415535, 0.6571367908677193, 0.7576778845565038, 0.6077603179990847, 0.0633319173320328, -0.25110582592649844, -0.26530584888757613, -0.3866180566172437, -0.6308550456479309, 0.4998550838464022, -0.5026066264873639, 0.9164278207271435, -0.7500432166416022, -0.23783585226959847, 0.8247831460846344, 0.7366755707129491, -0.3485888209598973, 0.1345086934240689, -0.3524604224868082, 0.9651725111434892, 0.319466778807465, -0.2930448464550406, 0.6640903932350957, 0.9916385249441391, -0.48772476857894564, -0.6691905743422542, 0.9503127625597843, -0.35357388526410416, 0.9813362390073781, -0.9426876420476751, 0.9164577560338443, -0.29653201227813075, -0.5361454539851436, -0.8431690555987399, 0.12363380448100925, -0.04893901320535976, 0.36496697330705796, 0.5659279665949752, 0.5351079616690766, 0.723517494429567, -0.07867688798037786, 0.13127318978819824, -0.9100144161812129, -0.7242714306559144, 0.12354399214122469, 0.3241371207664352, 0.91372367272524, -0.4182079198258153, -0.07022885349862262, 0.9801317651783179, 0.9775944923067903, -0.8433771069607106, -0.16215110678738376, -0.6306764891612149, -0.3104652614358199, 0.06472706590988797, 0.8733051766557254, 0.7709277031359216, -0.37526939209014465, -0.16057608073208374, 0.07387727441017433, -0.058324717538706494, 0.2118219968599866, 0.22090904754535057, 0.06227985880467135, -0.921359864305054, 0.443753331380903, 0.3779430774318904, -0.11927084244425457, -0.12247587436297858, -0.4763774314807321, 0.5185004304297227, -0.15528393271558816, 0.4247788227355862, 0.13630239536725597, -0.09216497140705471, 0.68166699733287, 0.48492652231395206, 0.8998309554625123, -0.9253269462333376, 0.25364381156613724, -0.6518861719159419, 0.701403201769917, 0.7210768269261423, -0.2686493718834575, 0.8896125849176506, 0.09544347105601458, -0.3667841420556903, 0.08664476199863347, -0.6971876790356069, 0.39741274284182637, -0.33517181606666724, -0.34721088322291527, 0.8806182453780889, 0.6085166904243859, -0.6966212139834733, -0.9274272034565558, -0.8621229068084191, -0.8142451651407747, 0.6147948465883086, 0.9961094558936114, 0.6182982483345929, 0.3120765924205009, 0.27432689655615605, -0.4874704916134627, 0.5760832356503511, 0.6778490845224676, -0.7719381864439119, -0.4759488883591698, -0.5318625761322426, 0.887697863104908, -0.09653106975866943, -0.8928430175857864, 0.8261587736665754, -0.027092930423978645, 0.9478986557446609, -0.1329636208774918, -0.4424324380834237, -0.06883917400267814, 0.13023071487912574, -0.6762494216843717, 0.9996036437549678, 0.2815742457688646, -0.7371132617549985, 0.9386563934419196, 0.9474052848978067, -0.08446088197181734, -0.22751720229122774, 0.5433976004979377, -0.7829471232771146, -0.25708880156571423, -0.2325410653630362, 0.16175794099320728, 0.9824899188480738, 0.09535536829104663, -0.1943902437056182, 0.14191640121943427, 0.41941967445013284, 0.7838232943317105, -0.29894879568511246, 0.382925227191405, -0.9760715464633292, -0.18069741801714878, 0.14554157996520845, -0.08950103318853908, 0.4777738871533692, 0.618078709161916, 0.31699172363906736, -0.612508474968009, -0.7008006562684153, 0.09318427687735498, -0.5442634671293647, -0.08708766553114566, 0.6527781545028897, 0.9271323845263806, -0.8010264998843548, -0.0628979766573099, -0.8186439155385867, -0.579487945525553, -0.03446140917794027, -0.04487645950679453, -0.519997709371927, -0.4697778411695981, 0.349176049619931, 0.261746287821371, -0.16303830214936066, 0.13134887458656164, -0.9780194589792721, 0.31217192316339637, -0.9290262821931119, -0.8100147537395228, -0.3517421091541528, 0.9139795780853794, -0.528100641094287, 0.19643306071712185, -0.399004924223876, -0.7259272271269677, 0.35821915256601744, -0.41741878228190066, -0.6381319829019472, 0.7023686179981541, 0.8789086180974095, 0.1250638699824469, -0.25731355413469914, -0.5012685847594256, -0.9265162510929643, 0.10469680960618155, 0.05165392412006797, -0.8165048376518569, 0.8278117444842341, -0.783380004923631, -0.7364418297048039, 0.16568669962699722, 0.5797499237140749, 0.999317517981134, -0.34656285342348125, -0.019033460598431695, 0.957916400069355, 0.7446566857911165, 0.7933358561442205, 0.9457346590872806, -0.2269689602016911, 0.8835867054971875, -0.806011114719275, -0.12769391463051094, -0.11832785949387259, -0.13929261237530177, -0.6311479517818106, -0.4143041460971788, 0.23576649678119344, -0.6307966226530821, -0.07220171617347426, 0.7705224887000643, 0.4980479277100134, 0.7084874010551236, -0.2398314381861304, -0.10983032946112892, 0.8625639765362245, -0.7164410240608732, 0.36821540074332315, 0.5120676225934688, 0.29125751801647515, -0.9965701566534428, -0.77134712692313, -0.7333556186997328, 0.7358844419281341, -0.27626082833000676, -0.39297257911633254, -0.49143835131666647, -0.7082377912329285, -0.3728976622213849, 0.8428267118397279, -0.2233026736058752, -0.8704026916030665, 0.9187128282554791, -0.9336858154516625, 0.15457381299568773, 0.8065831128330634, 0.15048508438075903, -0.9689729502617301, 0.6154882539989543, 0.5620445638276537, -0.7115104320983976, 0.5589520344604872, 0.21060210056812534, 0.3149244044964039, -0.8731815261311673, 0.9872497048705955, 0.7268705810346707, 0.4888611935162319, -0.27722579215513554, -0.10680281639786826, -0.9254831460867803, -0.08169671434496051, -0.8300835845186181, -0.003310389253457169, -0.04414682432853967, 0.8281363977482776, -0.6696023308695038, -0.980228510473272, -0.6846633661825001, 0.8632737208988042, 0.3665901267696936, 0.861527883513918, 0.8299305657290179, -0.1435236850051944, 0.5339905325647247, -0.5436752250849961, -0.2180443353500321, -0.1119154683897785, -0.1526010101196409, 0.5208353241873331, 0.9313054227092483, -0.16621306160185556, -0.5810259880173052, -0.6360597291432761, -0.8687532186041695, -0.1427206104494776, 0.5559854577152388, 0.024577246307250222, 0.48371853118336583, -0.3298103093225191, -0.43642050844499947, 0.6808973581782478, 0.686252063036819, -0.4348533633686118, 0.008848562048953346, -0.4315450761377948, 0.29255137324408165, 0.658631127027693, 0.3765852924362776, 0.014611907644433764, -0.41861063763299855, 0.7571477917251337, 0.8670867978958539, 0.8659940660936232, 0.6458718925772999, -0.3403624051326226, 0.9230552993214756, 0.2295815084062094, 0.687025628077965, -0.5971736418062707, -0.8552581257173013, 0.047635077732129494, 0.3048005074002149, -0.5623639467933919, -0.7551672466371941, -0.8644930689086006, -0.3953209504194801, -0.4528470490440275, 0.3037963455454409, -0.6940694136054122, -0.4051767323703406, 0.9085221059644486, 0.4395951899673862, 0.10151602330146603, 0.9272580668094594, 0.08748979811692292, 0.8622162320461229, 0.455427902294814, 0.5546658492668186, -0.7599780736379782, 0.4291300157935316, -0.7508989328397875, 0.5807483791376193, -0.6037313156878956, -0.02880895945828854, 0.1705960260277124, -0.4888189617303935, -0.746869000142643, -0.05936040259130437, 0.7833934731947907, -0.5901211233044985, 0.9121485662489959, -0.10542643330762491, 0.41974503013790376, 0.029429107675992494, 0.20777500175021868, 0.5019124515397289, -0.9422894041812118, 0.8972946104944011, 0.08207915027014523, -0.5564314755383168, 0.6809290685067908, 0.1531920607783881, -0.7174868736618956, -0.37881397172462083, 0.1646031332819804, -0.7938755546109255, -0.6439801857194805, 0.07919092882472145, -0.6333157010020416, -0.7864506506038649, 0.1510469673338306, -0.7148944293084212, 0.13369529056634843, 0.8174639240543318, -0.8485187469560644, 0.9772154098445682, -0.00936739044478152, 0.7650229081526503, 0.6985320368026726, 0.05894563556751953, -0.6607811523317948, 0.05594897002444732, -0.7159934688155078, 0.9458672287414942, 0.6369886770706525, -0.8697297319737309, 0.6215731426658901, 0.7245503832700062, 0.36860355695546576, 0.3777443064060215, -0.45864537902634517, -0.099670324171385, 0.904799602300534, 0.20327819228252797, -0.10264060436100375, -0.7174045072883397, 0.032477267233004925, -0.6358329893831123, 0.17061633339838345, -0.06931683860464832, 0.24699618729557526, -0.8867325426033668, 0.9983801145197821, 0.13896831601181447, -0.7766764193468969, -0.7909704614978967, 0.6656837451068724, -0.7242582679225196, 0.26853074496668294, 0.01994852562220739, 0.47686579336695933, -0.3084268448406655, 0.7605197184292958, -0.6397650050795065, 0.8495366819944155, -0.5761288391830883, -0.4541207367586759, -0.048951201573424585, 0.48176865117124046, 0.21028262309906132, 0.2482314254533402, 0.22555790300554657, 0.3450214830670151, -0.27365282007227476, 0.39759058701032757, -0.8671312753942502, 0.42285620886993214, -0.24373654748744777, -0.4847449323383557, 0.5220558471133319, 0.07036742330224754, 0.4381945230630522, -0.6466563213541059, 0.38207656788686695, 0.7414976061784107, -0.07245603859652139, 0.3377106817290956, -0.7984080269436153, 0.4600857886783505, 0.7864433771254256, -0.45217823412956193, 0.47024684291743357, -0.12783664679424533, 0.07986933479540759, 0.4328554438715293, -0.3506448919210219, 0.16629948825211005, -0.6605864931056011, 0.34449131769303065, 0.5093015320655483, 0.8058751149196322, 0.14341788832179692, 0.9147811076526506, -0.13641851408161187, 0.893973474951858, -0.11530354771460893, 0.6167304229654427, 0.20931216540845998, -0.6410255430223788, -0.7316151672377142, -0.6034822728761509, 0.7545545067181776, -0.9598407599275987, 0.6370854862129327, -0.9059561203660462, -0.7173728819290945, -0.7807899314523654, 0.8031862445620663, 0.18121195406461665, 0.9773400697045496, 0.5048184426612081, 0.6267855754648335, 0.6454320804555733, -0.15838004435169362, -0.8860230863377236, -0.7658886215987404, 0.6626145674779975, 0.012067871823985277, 0.1776283726787522, -0.651163543402032, -0.9338557370708427, -0.5667225927100874, -0.10965903403803412, -0.2761928692247131, -0.9522566700500346, -0.8926111715207465, -0.8485806809931973, -0.8731533990039648, -0.9731346033400365, 0.6679146357535668, 0.2663256469709041, 0.38162823499024734, -0.48017253853739583, -0.6907912115510104, -0.5304675448849729, -0.4409508453039812, 0.150280613874056, 0.7503566928419525, -0.4376836928155343, 0.20981876469842775, 0.6321710310301563, 0.42495136143102186, -0.77942828026064, -0.5946191954171971, 0.3443922294516313, -0.19695443869681206, 0.6714251632910684, -0.20794627557258316, -0.20827327217557312, 0.6145024901676337, -0.48096176726091233, 0.9717188103527314, -0.5096358517238999, -0.9444563885700508, -0.4604788978053185, 0.9355838982127698, 0.8750327114465972, -0.309834025227705, -0.210468888311941, -0.8969759103692676, 0.18440301181675056, 0.8091885567609103, -0.9349399254065582, -0.07595699836204983, -0.8578859619764443, -0.27518120552660763, -0.6575941164666292, -0.9032225577676631, -0.8875945986869256, -0.4148674020257048, -0.2492585217518184, 0.3185108451038967, -0.3301115797618648, 0.8172296401537733, -0.06119018349917771, 0.053408803403080185, -0.37063646141402096, -0.9683170813199247, 0.6360900940240821, 0.2422739891174881, -0.3480060928340356, 0.7491247227479851, 0.7702777481062753, -0.4499729882881327, -0.2867649892210764, -0.4018462143489294, -0.5061598123158959, 0.40793338572853943, -0.9258626645982182, 0.8635952687593431, 0.017028579377951214, 0.9856514667828973, -0.734028978340165, 0.7971587023196731, -0.7864655529313076, -0.5243900264768941, -0.47604220489950455, -0.042603844354870635, -0.22975148267895507, -0.7211823159581194, -0.7799351116191433, -0.8591222842675099, 0.8510508873360121, 0.5880815885784478, -0.036193527714576845, -0.7636049762200916, 0.7484675743922837, -0.33815175574715983, -0.7838980308953476, -0.4590004308496858, -0.5523158952112384, -0.8292425834208823, -0.185815336774108, -0.1941352270671295, 0.2984791380090228, 0.8251890877462973, -0.5964268101028385, 0.16396886806327293, 0.4048331425255949, 0.8140679234803647, -0.005262448302129297, 0.5445798868385991, 0.16691636004359744, -0.05206757697954045, 0.31753650270173184, -0.9734282286940086, -0.9961022592983566, -0.8821635897430835, -0.5344763239614714, -0.3536572937088882, -0.7486402990359262, 0.8272524403075816, -0.22140281712431542, 0.7122176748800078, 0.8007917767201176, -0.571425906560868, 0.583636410580304, 0.6785579172671243, -0.902100495425225, 0.3877950525130458, 0.41092281939115716, -0.31364044766181975, 0.6975884559450984, 0.753382065004512, 0.5098734515866503, 0.7010115813444078, 0.4037740833632466, 0.057271947969377957, -0.9088964213612531, -0.9838397688680764, 0.9373192077958632, 0.9863815191006888, -0.08562719680866171, 0.06941402513252415, -0.5696369231437444, -0.2795047733575897, -0.3687168674563872, 0.9585936785664546, 0.85240261073085, -0.7583275195787473, -0.24598607509161186, 0.644459366693972, -0.46979155403996464, -0.8154082356811969, 0.05194590378655617, 0.7796796147543039, -0.5650237550866177, 0.5298379245104021, -0.8619841679336635, -0.6475141022063056, 0.8941826601469569, 0.8938812505052434, 0.4217378852087974, -0.22752000018384644, 0.934040616533409, 0.9113790452861945, 0.9358593136168842, 0.18384192592217796, -0.7089921275928868, 0.561524134748056, 0.9121024773001991, -0.5498914654634968, -0.6763192967602198, -0.1714366789549897, 0.37074549862576167, -0.7461383324666615, 0.0067690270042712886, 0.1489590822144624]\n",
            "[-0.015447951662233894, 0.6420502228750358, 0.5567303837978361, -0.8776270588942141, 0.3566924094757409, 0.9130911499866972, 0.9566409505980515, 0.6818461748766209, 0.9433386332433895, 0.46962716899201173, 0.9702230251990405, 0.6288549758460922, -0.8597314836996488, -0.8812726702609044, 0.4404244896401599, 0.851005881965587, -0.9491609447302198, 0.007895597745647187, -0.43374690226305934, 0.9401591876207187, -0.9302262160074852, -0.10484579559685847, 0.3971973856780646, 0.5870052198033604, -0.17395404732468434, 0.5242917621479606, 0.00531875301682927, -0.40136195473710523, 0.3493658207095298, -0.12878590129601664, 0.5624790497881151, 0.2040663073994966, 0.19274181298142934, -0.17893361592293622, 0.3453522871432875, -0.10797176251580898, 0.9906228862114677, 0.09603197615465198, 0.5372144422971656, 0.32868339017147186, 0.13656310679652806, 0.4045540003949486, -0.6807413827878144, 0.045219743616111696, 0.5504346502064228, -0.03882280338798938, 0.19876048546164382, 0.45514467359222843, -0.9054892711702318, -0.20536425652617885, -0.4386357074643257, -0.39212022434123095, -0.27478339787317974, 0.005920786720396531, -0.7426756429777126, -0.7550286640579444, 0.48387098640011894, -0.37629538991195677, -0.1975731931633633, 0.5233015590219534, 0.7531330996282435, 0.1277809159315335, -0.15817424612964603, -0.018674747607088538, 0.42873928130678784, 0.9889733718774001, 0.12048436033116827, -0.41859356547982696, -0.19360312005525682, 0.28287016935510945, -0.8495804131180713, -0.28818298688829236, 0.6514656156173608, 0.4041438442268708, -0.736417617083748, 0.07745995392451599, 0.8354984719962506, 0.44261153253595675, -0.15557497092884698, -0.45281342739013297, 0.7032112694339181, 0.6137540509986885, 0.708466759560876, -0.7005670165157818, -0.3414845395389239, 0.9718242196383522, 0.4192943537231131, 0.7302096305110763, 0.026103990304829683, 0.4955334762417307, 0.7201444008154347, -0.5359322147037733, -0.9452375831807449, -0.8091267539702127, -0.44427448820546145, 0.7124072347799422, -0.6321307709192165, -0.9069060560989959, 0.4450998814779701, 0.71302080574123, 0.684711602960761, -0.1798464876581496, -0.5937764129892307, -0.3562039043289753, 0.6862999045608562, -0.8163795130129441, 0.04168481344994124, -0.7902647485029313, -0.7546431893806331, 0.15824175988168254, -0.21005822692106646, 0.6122882141307551, -0.1625477193187963, 0.07157528874050123, -0.3695152255523857, -0.32247079842078974, -0.39380901042219896, 0.8653488175767827, -0.37044894529585304, -0.1401460171758444, 0.0363122030553813, 0.8440810866510537, 0.4813999911067157, 0.585859203647672, 0.2601654407257339, 0.23211087188883317, -0.21779141693677428, -0.7962708375899463, 0.7996145950546325, 0.8165592429221353, 0.45586979874067257, 0.9975383060275136, -0.33638817731181714, -0.17714437929976246, 0.08368705457630221, 0.6947459466763448, -0.7937482614110323, 0.5456635257227429, -0.7623246037264881, 0.4760361454185009, -0.49314437294087243, -0.5808601282844204, -0.5785579732325412, -0.3754240859326703, -0.2257590771192577, -0.8284943296764442, -0.9894538862330744, -0.5463044205131402, -0.41963153065464365, 0.49980506435535643, 0.1724123431101776, -0.718126079476344, -0.37547696894241134, -0.0793014795550302, -0.32420218738652307, -0.5699863277346293, -0.08730642003365574, -0.7635201273616552, -0.4266976326559597, 0.780389093374825, 0.045896568986279584, -0.5447289399993511, -0.842687199032149, 0.8981900213657281, -0.3598954910119019, 0.7225466066172361, 0.13534523957743705, 0.1475758212059448, 0.2986870080522215, 0.7186622405933618, -0.4769165623138314, 0.696549683350947, -0.33837493067841273, 0.38166624089715406, -0.5952991651242112, -0.8110627362747174, 0.8669947884715481, -0.8707531672214619, 0.8756171043836263, -0.41913168570646575, -0.22532819053856867, 0.3328050229364512, -0.9969799384164024, -0.16449578590108005, -0.44212421683721215, 0.7508586951042067, 0.5690833091141911, -0.03987187774661449, -0.8383527152746284, 0.8835075436833972, -0.8386987223333897, -0.2524994159399996, -0.01238341525264608, -0.3364091504339093, -0.079653990416648, -0.889673979411312, 0.5161354917649839, -0.9190873345739841, -0.5272544798219472, -0.9913730684997879, 0.7032884364485117, 0.3192769942378262, 0.5694268656629848, -0.8261778958754571, -0.486005257302756, 0.8546851891119751, 0.47309597546976323, 0.7093517773635682, -0.5801467138274403, 0.4702155185098489, 0.9514334032295693, -0.5189728890298604, -0.21475261505994547, 0.056308115762343514, -0.7314257408066447, -0.20527134272321024, -0.9774236343685057, 0.1570741028256808, -0.7922290988943241, 0.8918865798720734, 0.42261276434011363, -0.4276000835796021, 0.9362966115681568, -0.7977215005080949, -0.7241938724566095, -0.34890315194228094, -0.8977642115507207, 0.06740215137499495, -0.5793390370011984, 0.39052285873600723, -0.41056749779683077, -0.7824409614317316, 0.9924466616878245, -0.9616479970236658, -0.6500416701218974, -0.49922211599270505, 0.26500321653915493, 0.13595449766744716, 0.37324394336467726, 0.2128662367354477, 0.18074792012803176, -0.4825975039047534, 0.14466065120709537, -0.8638736102358389, -0.3293334671340036, -0.7948736692236678, 0.32698361217432637, -0.6911235656610519, -0.4402813668595347, 0.6022595279553877, -0.704379868555961, -0.45224172821786124, 0.6796295559949921, 0.883235470044792, 0.7531793627238703, -0.7479986596615447, 0.21727535948667764, -0.8671464593671596, 0.21769262295461034, 0.9509523996817899, 0.18794740204785998, -0.28596454972611385, 0.2849407656132441, 0.3926011018613913, -0.5041685824158941, 0.43842415924875744, 0.4675042635125133, 0.6888855158780647, -0.6641179052992847, -0.09167534083568274, 0.92176589779644, -0.19105668077203575, 0.19124143849120134, -0.04055598561906382, -0.5033729079087801, 0.08300341666456101, -0.6624629727700035, -0.8168502807345019, 0.9309054681784295, 0.5806631449412152, 0.03298895615745523, 0.5643673094154358, 0.217810337259708, -0.5505094897039491, -0.8540104040028067, -0.7685910839677517, -0.13258215063277823, -0.30362900834150475, -0.05378881419668735, 0.3285816822631267, 0.467278467585011, -0.44993266674357213, -0.7456498996377583, -0.11925725838724666, -0.9860854343766701, -0.8699647038146276, -0.43921095609375627, 0.5556653354883543, 0.7227925617163602, 0.35914609954483967, 0.11261726351389534, -0.4797622508617503, 0.3042758622261368, 0.23978202454995157, -0.7623632849448387, 0.7178355053561629, 0.6339807542783966, 0.0022457626155709676, 0.7251538269314524, 0.16349282376386642, 0.8055524844295356, 0.3719563896639142, 0.17595508403191906, 0.5817897892721573, 0.5749637200589686, 0.17244891264242423, -0.884581762034683, 0.7329829631468274, -0.683301953668751, -0.9936276468451553, -0.12808884899073436, -0.24455806032176475, 0.0008482805529659299, -0.9533317579859768, -0.5022680316643986, 0.7451499512579103, -0.2570734353435695, -0.4586574672387336, -0.9661107355051279, 0.44305658371875367, 0.1928947699849588, -0.5760207325457072, 0.7657154231115131, 0.23633675162661172, -0.48560299413042896, 0.2446897166558133, -0.18908355082845785, 0.14677594022629692, -0.9294301587251244, 0.637629919958896, -0.16836226843535296, -0.8213334221497717, 0.10552469638011508, 0.060003901598524756, -0.5348323944544333, 0.7183414936444681, 0.911321802540846, 0.6637135992929943, 0.3148857132454579, 0.7138180889919776, -0.39981475416428114, -0.926847299614382, 0.7299559747539017, 0.4881906152508324, -0.6900039790586048, -0.9318541323414109, -0.8994371488735695, -0.22379819560007475, 0.7820766689150249, -0.07094560921740567, 0.960991073114428, -0.6638318335774713, 0.472956971258937, -0.2667931823409526, 0.8088238796703375, 0.4733456366521567, 0.8125903167197421, -0.5894616926904845, -0.34078412139785175, -0.2506041449863208, -0.8583973751185925, -0.09485443602219656, 0.38989857728563515, 0.6140580876203452, -0.9134540320615412, 0.5048821387834457, 0.9458182977913527, 0.6747117545684773, 0.5162901771763182, -0.09726635537123096, -0.22436142345836751, 0.1619588295544705, 0.860300032928754, 0.288536055059625, 0.27964279017924043, 0.9229282786386042, 0.9144126659458762, -0.8700405597924497, -0.30571086946443327, 0.5996983671700875, 0.5207189019848375, -0.6708791533849567, -0.8720687247723677, -0.02481316326974836, -0.9494195668882872, 0.9429641584021435, 0.5174093881078541, -0.8229542216569299, -0.7158078558816829, -0.5985001295757617, -0.8438046043691041, -0.7616182480615954, 0.6985885625236481, -0.2205783712582572, -0.5355825463707307, 0.2390774476917381, 0.16407208356082825, -0.5938613393946777, -0.553966573693141, -0.7861825777403526, -0.484379116145204, -0.6867438581637391, 0.10102008780110783, 0.28176361865180666, 0.5366187864541301, 0.9675097022405608, -0.7783313168966082, -0.49324087086506996, 0.884645495049782, -0.3127601166752316, -0.04579256306747581, -0.2053672731980607, 0.14369614840052813, 0.1526447586917421, 0.593501430956318, 0.212124969826186, -0.23150820474420364, -0.7095948139751662, -0.7905365324322018, 0.930720589429995, -0.12921430272774348, -0.7264543544856421, -0.6048485542338522, 0.13154076152053795, -0.09278777697374263, 0.6642835067486421, 0.5170540681463158, -0.28329829978500354, -0.7890275069721717, -0.007725041250493003, 0.27684113655793463, 0.036275191825078545, 0.6096934693574996, -0.8078736906598991, -0.11034735795896156, -0.7750233088121794, 0.25759859596369883, 0.2628085137916729, -0.8696079038245446, 0.28639886067939946, -0.6739593008304234, -0.3251001859297533, 0.8315124624219563, -0.3949064746254405, 0.35656440945950796, -0.5922302530813601, 0.942087399678079, -0.7966819033843464, 0.7631083292293972, -0.7804658058478857, -0.8468738948310652, 0.794492522907795, 0.7545448459981576, -0.5558382318141759, 0.795922827879582, 0.05140643036219439, -0.048781683033250145, 0.3220676487939802, -0.6622996614205838, -0.5363821616497677, -0.40349983825996194, -0.11915009094318108, 0.6042294272627824, -0.2674819109737925, -0.4151397232606133, -0.8176887620081472, -0.7470532631582048, 0.13513745915364073, 0.3967683406695073, -0.48295528663219, -0.2265453882874462, 0.7145879987716102, 0.41745327733390325, 0.8287246937968142, -0.7726201655812113, -0.8450055605577593, 0.3185580658023732, 0.3013789274407559, -0.3355615119632509, 0.34535014748659076, -0.8989508860938531, -0.28633869380242993, -0.6282514772993828, 0.6960099908825701, -0.6372696518193333, -0.7640155006448324, -0.10320559484713043, 0.6773184903941691, -0.9690482780718086, 0.3474251699729094, 0.5201950030477693, -0.6932339433302519, -0.7440004192092027, 0.48975120319766674, 0.22598211425614267, 0.8288593324734777, 0.9153217305721684, 0.03565332255292275, -0.4736884197465172, 0.2184692955878358, -0.9527030261837421, -0.4218357745538279, -0.8164844743771686, -0.34035885448487013, -0.9759355389621114, -0.49017217106504796, -0.4159672944554689, 0.26288775025875144, 0.4804950991946124, 0.4490092151886538, -0.7389909285083034, 0.4663815325619245, 0.47689265031100314, 0.9515411145703327, 0.662247139151948, -0.5130557353205218, 0.28017884761716383, 0.16136465784869136, 0.2285407152849559, -0.693389451305451, 0.030914825460316653, 0.9410903115494085, 0.20551465224517096, -0.49757442601457913, -0.6027723770664826, 0.07519424887799842, 0.6157480814212561, 0.19555061124291817, 0.8742312439636262, -0.28911797045386467, -0.459539816003562, -0.026233452696938553, -0.05795946721148271, -0.43540557493984444, -0.7211327463944048, 0.022893852465550202, -0.3017858494207857, 0.8051430839133005, 0.6626335202275866, 0.31338383316867024, -0.34476997500880757, -0.8287577326831836, 0.3190621496201107, -0.6448102360375374, -0.15987108209720335, -0.9834931914642961, 0.5997764559395549, 0.1740473792578494, 0.9401087506656871, 0.527799026512006, -0.5167849297224434, -0.5578055449509784, -0.16044027390578397, 0.5486907786113435, -0.8285124113256137, 0.0325542648090098, -0.3706004919541228, -0.44892635068972764, -0.3078646037196777, 0.2123112023564595, 0.5166231642094827, -0.4089535430472315, 0.04808602237494797, 0.8364152288041482, 0.2925800449835845, 0.31364848305302995, 0.902494470779919, 0.5271157929348076, 0.08164700232558264, -0.4844752585452514, 0.4862799278672232, -0.12983217281420556, 0.9577303620751247, -0.5452179281398879, -0.7627796043810469, -0.6245495760786013, 0.10919599888590881, 0.25195950113150656, 0.25086542137696366, -0.8939140576898159, -0.7892834614813407, -0.7094904380521461, -0.4960336182148233, 0.1502528624365953, -0.9136373865945688, 0.41401526069684746, -0.5779768778523686, 0.7863986586352463, -0.9291175592987009, 0.4220672595779915, 0.015129924792772309, 0.9034417919244941, 0.12758923639235809, 0.03925220736886148, -0.4864597156185897, -0.9825050208471422, -0.4633412868760973, -0.8255919617156282, 0.5012060048154514, -0.8928988350769194, 0.9833384278698221, 0.3742464125179208, 0.5750496942455448, 0.3411363831872922, -0.442227133538895, -0.5984913674508836, -0.8304853330326254, 0.26781108401951426, 0.44066271461924567, -0.08380268383901557, -0.7036737982610317, -0.9810246030162255, 0.3630421364135463, 0.535562478267946, -0.05580894398611225, -0.446835651775662, -0.24587349951984594, -0.8387307029306434, -0.3239966171060449, 0.7992432137698595, 0.3948088135920309, 0.40131844184042453, -0.846762633654361, 0.1618671412684216, 0.34216131676911954, -0.002105282382289575, 0.3150475743245993, 0.12531526375130442, 0.05247689254159127, 0.5623129537867018, 0.709975246697153, 0.5159685662978557, -0.9477040806590802, 0.6099577490284231, 0.20799463650832895, -0.7853185670138894, -0.1647545947066209, 0.5393506226559921, 0.11721985461638984, -0.08869771356783551, 0.5619755245824043, -0.6697404872080397, 0.2881351068339657, 0.8856606539093765, 0.553952975088041, -0.9924453225561805, 0.28781651319816026, 0.6274860752007503, -0.8166762358642539, -0.9269441958294491, -0.17750258609776837, 0.943434680982363, 0.6232059652473114, 0.2021388309561638, -0.3754982438221761, 0.994807493089906, 0.6607517256805806, -0.7280780539196527, 0.10972141578291006, 0.1298222501473909, -0.8273968923884436, -0.499731505923543, 0.8736436094122686, -0.6233680837603297, -0.37458780339770126, 0.7102327699530677, 0.18678732719320745, 0.32927158467544837, -0.5903513527170412, 0.8976168939435281, 0.7020012219617926, 0.5768294356601491, -0.032884164407363414, -0.9513313315350149, 0.38570832616794637, 0.754749979808524, 0.39697423697504175, -0.6848810168806136, -0.20101947706701684, 0.6451778949357543, 0.8220307811756897, 0.07458874277996896, 0.12397451361904288, 0.48938839778648235, 0.023801478777112806, -0.48102261577812566, -0.7720745956103339, 0.4827016079687563, -0.5351185256825517, -0.9653173197809006, 0.5809047214975216, 0.618288359219791, -0.23375168068969399, 0.6524668549979071, -0.610267377797612, -0.5429697546343524, 0.5831030449198771, -0.6227535490722496, 0.6088754071201858, -0.21311973432801778, 0.10135230442115928, -0.009917914404665273, -0.3069549496615036, -0.16493013712710924, -0.379050334578777, -0.6083064376361402, -0.26101196813289285, -0.8863935515737478, -0.15442305047089278, -0.3438644577125938, -0.6523723164759871, -0.9870939589931935, 0.06330449775621982, -0.8630797891799131, -0.22061870499569136, 0.3118684324175498, 0.9441564239112423, 0.40185645610447396, 0.4936935480491942, -0.8997593242828856, 0.9052892176928136, 0.5436199671241573, 0.14794308519796084, -0.6482449509846255, 0.8035371427635098, 0.3760576399783959, 0.6032039172481065, 0.6751967572278388, 0.44248621451720327, 0.507392022984078, 0.5065483222520579, -0.8370865333093833, 0.6266272635548493, -0.397157486277401, -0.08223478031502807, 0.2773740512943048, -0.17524704609189423, 0.9492984223417908, 0.46384466221840426, 0.48316993586711443, -0.7819054005818076, -0.2908775927685012, 0.45940629131342003, -0.08969260873470875, 0.2376025652168885, 0.18733233555395423, 0.555928280314026, 0.772075429438366, 0.06001172672833088, -0.8607957934227406, -0.33863183166025146, -0.6947322735913017, 0.17868174257892488, 0.2989467495348712, 0.6302524475939923, 0.28923702144455343, 0.2793210067098122, 0.1326413771342645, 0.5087044305650839, 0.4367833470756082, -0.9841068165583637, -0.44362580512346916, -0.18123833748170948, 0.8398157293035737, -0.012249909571833806, -0.90299767343304, -0.6017827646300724, 0.5678289437221844, 0.4120287555546289, 0.3264928878449924, -0.4694552337049569, -0.8519083692287421, -0.7872003250916422, 0.9313738442916693, -0.5487878586026336, -0.8966700094598747, 0.04150635358691446, -0.3105536693605535, -0.8649265084208446, -0.1615664425241392, -0.2179875453605975, 0.23006155992662647, -0.8066930368972363, 0.3656857432544307, 0.965530749245654, -0.0381248405361756, -0.5952325968734957, 0.672619444388231, -0.7033179998894981, 0.5096453893794766, -0.3616533306864711, 0.14362497166355026, -0.4662964829208991, -0.939573384257391, -0.7186989063448777, 0.3580573097197166, 0.35045567675272626, -0.17117709873489706, -0.25004785383837747, 0.33557222610961057, -0.9138613419715744, 0.3820004383855107, 0.24219000340530883, -0.1080197326503709, 0.3384910528383318, -0.8706987036939702, 0.7389283103953148, -0.5496100315513777, 0.4626018359614974, 0.5904458890170905, 0.7433282209026999, 0.7716362341073886, 0.2878809032762779, -0.7807146935172717, -0.898381180908455, -0.4851426607100293, 0.606147034771533, -0.5500020723692987, -0.5446739231883972, 0.15277197471835247, -0.5323916563231732, 0.6091747016668461, -0.5616897117357234, 0.38494534883344733, -0.9420902804796485, -0.3489507781821044, -0.7650777816937104, 0.7471436936660785, 0.8772866448082905, -0.0750608392940817, 0.17962846057927195, 0.4006780750912262, -0.375793597623431, 0.5933538768802058, -0.11969526034213218, 0.23298893902460405, 0.2933659842061016, 0.645288755300311, 0.8729383874029368, 0.4732631607058133, -0.9841300454365527, 0.950347214252115, -0.6096513118661746, 0.9901191777782465, 0.287245273200766, -0.09193067663287913, -0.4744406681355686, -0.005854683029867358, 0.6324003145247103, -0.42485321071072235, 0.7451398894993857, -0.41834133269722473, 0.020629233413173464, 0.8619187996811486, 0.5285968842327229, 0.3627124134939743, -0.3954603989332328, -0.15006796882090945, -0.09245941962836568, -0.57769466577777, 0.9614189315683728, 0.43112762987071007, 0.754162443573744, 0.8227779487561575, 0.6300888080903349, 0.26136933952256514, -0.441605588725712, -0.4334419260209075, 0.9910272929654984, -0.7166811412275516, 0.8352988488364557, 0.9220548519396898, 0.5807064540477864, 0.20244802055124955, 0.26626757863418193, -0.13896910769620585, 0.059475943609917925, 0.9586013329781395, 0.5887057196602352, -0.3465793846927703, 0.056751733764207835, 0.45116589179048705, -0.3576268048895952, -0.6253375682220816, 0.9285547978907573, 0.45219189816795713, -0.028642506231768783, -0.08057938218285576, 0.6948266048045095, 0.17730049070827802, -0.6055223433984283, -0.6211643478313102, -0.15497303848728872, -0.24673652742105623, -0.21019086887709926, -0.25204179631268264, -0.7129493114896155, -0.0001560812305081427, -0.6338553516867305, -0.39878804867015183, -0.40806815809397, 0.45872275938341955, -0.4615870592859417, -0.7047872518781393, -0.6469370623436355, 0.5427696612552293, -0.7569627750784369, -0.027486851111291344, -0.8036158395327988, -0.9999106352007088, -0.25582289967120553, -0.4537029307005538, 0.5455618204223311, -0.9185654432749568, 0.5468737526400964, 0.8909588484076969, 0.5289111662695831, 0.7756637780545108, 0.9873847847856638, -0.0687049037291887, -0.4441832094755973, 0.17677397455731558, 0.976297054451096, 0.6445646039009885, 0.3314418128969636, -0.1314425477439709, 0.8333589923358697, 0.9469399862421295, -0.6121993786292583, 0.2890702237305822, -0.7111281099127691, -0.3044467427440243, 0.10820832782232892, 0.4263578837994835, 0.9585986261152584, 0.8029514575427508, -0.9544067907398639, -0.4415533130140423, -0.6111092171398924, 0.47728261540815864, 0.5740820441380552, -0.7933568625123772, -0.01154459485727477, 0.3839100946067664, -0.29900036580571765, 0.5675241544753367, 0.964654152152773, 0.46370486370271613, -0.2528294792622532, 0.6313958057981974, 0.8359556718221945, -0.43621370120384384, -0.8454675491517354, -0.9456877240878281, -0.26787563754681853, -0.0028172533146373357, -0.7973991303637216, 0.08682238289382838, 0.3349629561384728, -0.5529953398204381, -0.09783018114539743, -0.5060263170206454, 0.1738860531920534, 0.9328493251872749, -0.4560709377266752, 0.029460317577258532, -0.019427785375005335, 0.994693072746933, -0.9835412089970388, -0.6501629220779623, 0.5752240444815448, -0.6987331414239542, 0.04008536874071367, -0.6326025711564309, -0.7982221696988387, 0.08047839165711945, 0.36542416545822487, 0.07948717445408682, -0.07126216859806345, 0.2836127910052373, -0.10780528272096479, 0.4899757194641965, -0.9035949659423599, 0.5303508745749697, 0.2390464492671629, -0.33293621392438166, -0.4449944925010285, -0.4646328665658721, 0.3626436771473973, 0.3201499431861641, 0.8116045190747181, -0.07721231367329517, 0.8423704483934578, 0.5986141422273701, 0.9620245151465967, -0.09959473541044606, 0.28260458815169565, 0.028826191780616295, -0.7572288817203938, 0.7769605925390946, -0.507650182375849, 0.9651543452951195, 0.11122850440062892, 0.5843122313044915, -0.1016160787877658, 0.6553021896328164, 0.6107566480601456, -0.311272476036468, -0.9418308209549702, 0.9064842222428577, 0.2505422835359832, -0.12207163555993072, -0.7680777111530916, -0.5235063582094972, -0.06652603319483652, -0.19179837483009954, -0.5040893565308713, 0.7844356356594806, -0.6807059091297942, -0.08039979512595874, -0.22615528965113985, 0.28708459858015245, -0.25615819710632803, -0.2697559385467587, 0.7158389052476888, 0.9251040172463492, -0.106846592348248, -0.03100009841429019, -0.7071909704088475, 0.08923282529302168, -0.5459990066943567, 0.33701067876649504, 0.7499239335306096, -0.09866158992329899, -0.35010925095472567, 0.7340055921509328, 0.5770046033857217, 0.7023180668662938, 0.07180249101270864, 0.5916417608794988, -0.5035796735827405, -0.7799835786737173, -0.6325543422882609, -0.6077140032004376, -0.22976953939198208, 0.011932070577535203, -0.6246874156041164, 0.31213390604786984, 0.36123580365724073, -0.9258886887391438, 0.6611591346133037, 0.33192839407508257, 0.22998206585487435, 0.03987743109084585, 0.5801268611071098, 0.5179659636396037, 0.3540257078512361, -0.559929814961933, -0.8908075816932512, -0.16620389069492214, 0.2948213111872804, -0.9269114296407457, 0.5300726131765257, 0.3672299859174921, 0.6580514378190949, -0.33988829504718754, -0.9889834559791362, -0.6558231850451617, -0.7128008999312994, -0.5362119720395446, 0.8133365254271057, 0.8311234466841924, 0.9080146299888439, -0.2449954108346959, 0.5819754015697232, 0.7795187872650071, -0.38946730964050835, -0.0760440115750387, 0.9889449231513885, 0.826561519245635, 0.052412321788918304, 0.3400841611369787, 0.5995407101495553, -0.2161020372400717, -0.5440772619208765, -0.23826283711277352, 0.5256649072319513, 0.5949532646490299, 0.8149520668304124, -0.013566995777681612, -0.6167940248548303, -0.06274721426704999, 0.6916873166991022, 0.498899006992757, -0.7928605548030245, -0.6489691622886493, 0.8276019784263018, -0.5832820191826389, 0.5954672247287796, 0.5376481637595387, 0.5740428298907876, 0.061250465095522566, 0.9728691123671589, 0.024397561335112172, -0.2707032215859875, 0.017264503655633145, 0.008126550048866088, -0.20337758891659274, -0.7460062512941903, -0.9223217170477229, -0.48908489057563, -0.5230526304010068, 0.6171251127642978, 0.538441738455798, -0.8478393020022601, -0.02207904220714041, -0.704813802509141, 0.9776806110435909, 0.4917014311136483, -0.12376920999956398, 0.7504810802762789, 0.5853902788420537, -0.11714496934983898, -0.584987251237131, -0.8772281382174079, 0.003330322858609458, -0.07395926676850628, 0.26125960430549955, 0.5080912814500509, -0.6939671202846278, 0.18755772353841604, 0.9700679503992609, -0.03733528412973475, -0.6943517124206373, -0.6617040967097245, -0.16724702777241163, 0.5755873521633603, -0.5800989074994596, -0.3767521451561837, 0.3105787776275699, 0.6007655595527692, -0.3784818192032813, -0.4514302628851661, -0.8378940157226462, -0.5076863145570938, 0.2395878524266688, -0.9843750846695849, 0.13939299665691407, 0.5229489886654357, -0.855221730772554, 0.3736462805976686, -0.8678770959545152, -0.9417363243077554, 0.9276489705252977, 0.5015726334819026, -0.28507134467657047, 0.7524245884202894, 0.9771425622681444, 0.8734695472244758, -0.0509193342922758, 0.24965700078312403, -0.8219538873526653, -0.5067588674443422, 0.8855885341083134, 0.12882309026197203, -0.6265967959599532, 0.5112505241578191, 0.1580912747442067, -0.913348910738595, -0.5203191881687201, -0.36194346745814565, 0.3795599766945159, -0.5134840734263542, 0.14353831259089933, -0.8766273438405339, -0.2715092405128845, -0.4776079612106874, -0.24837774448506922, -0.5367648959810074, -0.5770892256248248, -0.2189153877613068, 0.7479791685149058, -0.45411107391151484, -0.2110682003546962, 0.12107709400789313, -0.5802229015320355, -0.5813261141890076, -0.3310464592809499, -0.7462623450369754, -0.5622422854808027, -0.10919180528620442, -0.1725483960383869, -0.11599551742020187, -0.661554209048274, -0.6483458053310514, 0.03615023542087403, -0.7816193386086763, 0.418753866886173, 0.24808388437274576, 0.18174160581130394, 0.26479538698709537, -0.8371568801190918, -0.49139638976135913, -0.38756738886467645, 0.7638068266130349, 0.02350568809333442, 0.36708736684391385, -0.6029108822589193, 0.046601138290651045, 0.46022202157312475, 0.37661407226194465, -0.04499039402284977, 0.8457698342937525, -0.7834848793896294, -0.032188380979176534, 0.37566315609475875, 0.10025727831045095, 0.15745924013532298, -0.12761955980864537, -0.7173944756985493, -0.8231794431505046, 0.6776697871071722, -0.0808131297003496, -0.6926951784126947, 0.6023973569211676, -0.7449535176996975, 0.7399888808297039, -0.23077323303629527, -0.9123874865260035, 0.9694004548977928, -0.393624832971015, -0.8189913745272084, 0.22776216714463526, -0.8851634953197332, 0.7781040670185875, -0.7078990792577906, -0.8061433680317291, -0.32080415421693065, 0.89697313077325, 0.313438979258186, 0.6229717267015651, 0.4903311793186873, 0.26294440278154996, 0.661731524643707, -0.9973948543093463, -0.9056951209892823, 0.3757890743468124, 0.8055206093391356, -0.1050629560469345, -0.5637824154940849, -0.5934297232183381, -0.04069392227048563, -0.6952795790993036, 0.1689196129776509, 0.534714194305592, 0.03089995050275607, 0.48242806255690973, 0.8088289743649424, -0.21485460205592455, 0.08008683220423829, -0.7537757173911512, 0.3057143824632149, 0.6814447424746441, 0.6070378855218885, 0.9948692761274609, 0.018330694784324253, -0.9838383220701519, -0.24150509496902473, 0.4779657153786485, 0.9203720707576613, -0.04187351953110885, -0.28301584604426133, 0.8395704193160065, 0.8587873823243812, 0.2719073873399733, 0.13397787730260502, 0.8789488707709165, -0.6820751896092969, 0.18704499651354856, 0.16731889856211257, -0.8822310329816612, -0.07521343356516152, 0.3916603507718328, -0.033684860471854705, 0.552822782126039, 0.6985052694570602, 0.9710450817766239, -0.4395983253593714, -0.23906522757748738, 0.3693174826082526, -0.5169154183858815, -0.7020072120643228, -0.7195728073910777, 0.21965396165736872, -0.3702376080245475, -0.4257688606841292, -0.3335543318539891, 0.4737657304186562, 0.43187412307137585, 0.4536917173375976, -0.1834239342906947, -0.14148067715738644, 0.902498936925173, 0.838793982266425, -0.5255902274110411, 0.5170407102316539, 0.2833938918331189, 0.17173988858503608, -0.15099552824867524, -0.9402009156243685, -0.13399914834112292, 0.15299577107507178, -0.6378147471166502, 0.8670317625606281, 0.3362347109501356, -0.9354046337685014, 0.2033692875186328, 0.6394262040638858, -0.8445205166187233, 0.49919866822638714, 0.7003638589577521, -0.21509810815966057, 0.6525397102156125, -0.8263358885865253, 0.6052009711703834, 0.09643512820838596, 0.2764547601272547, 0.19018508597093753, -0.7523208822403225, -0.07640807872041577, 0.9573840359409611, 0.48059984927471344, -0.22888570919609363, 0.9159024262962681, 0.4445941027510636, -0.5946186749920801, 0.6262224507396852, -0.7025428879238236, 0.1605366247499398, -0.37555610925529925, 0.3692971075719764, 0.785185370180776, 0.20151742536972406, -0.270344566954164, -0.803228837417693, -0.183752846364859, -0.21391890677338465, -0.7084052013856257, 0.9239854174712987, -0.6612010685133043, -0.8449899874195939, 0.7906050804532718, 0.3098738108177308, -0.4715830579522331, -0.467786971848293, 0.8996037908487311, -0.849274868591857, -0.8169922330504631, 0.5238116777560045, -0.8127749033318921, 0.18822946091511605, -0.822095688514463, -0.6123401563283686, 0.8363170826047621, 0.7471025931689808, 0.36418491359016314, -0.8215630406393939, -0.9778763098523553, -0.4873628586036731, -0.29192507328487016, -0.7582862713462499, -0.76237775620674, 0.053234065184679435, -0.7567788777165874, -0.4269225423339018, -0.13481022637006834, 0.6353482954080698, 0.028484945451506594, -0.5350458530078936, 0.8385216678799432, -0.037695131095780265, 0.1491504057891706, -0.6960861793993969, -0.9578405864016046, 0.2687075398572083, 0.539226431199374, -0.6793776529017383, 0.5047060958504319, -0.7293615144855494, -0.8298801085183289, 0.7288239891530301, 0.5491358826596258, -0.7378276824718664, 0.5805512114066245, 0.9942598222201726, -0.1508726217237717, -0.9685073502124302, -0.7614726815456117, 0.4414312463978123, -0.14708724513623128, -0.2798999586312807, -0.8759096115589, -0.5810697800977165, -0.2951490668507444, 0.6862167160300436, -0.41576806161345625, -0.10178951882533238, 0.013788980092745984, -0.20023183420972912, -0.5721517106862601, -0.8083901869041881, -0.37791018914216323, -0.8146477765500277, -0.3734428968837029, 0.647840504940411, 0.06680300497838054, -0.45027265545738193, 0.6734651491555226, -0.27256696498479926, 0.13136429568784047, -0.6275070951090091, -0.11358420089052079, 0.453250324727714, 0.906516244068684, 0.8058595892607212, -0.6105782481572117, -0.8294478736969986, 0.9149365666733587, 0.5763858622453493, 0.20444000912372973, 0.7283427331648455, -0.7132425899366426, -0.1329604292980633, 0.5857842029823104, 0.3231830828394411, -0.299819997228443, -0.9913159444003918, -0.6762953761915467, -0.19096478598038025, -0.2171004770474838, -0.347718384680767, -0.6023300959172457, -0.606556202007408, -0.8398777615366597, 0.07899484428983783, -0.3080192968034354, -0.8973215315264962, -0.6701965876725231, -0.97894339849477, 0.9732030237623182, 0.04649317460615343, 0.13659575489692655, 0.922687813719741, -0.38503139915480666, 0.004973510530159375, -0.2940872475694123, -0.9278903257037172, -0.7895655640660912, -0.941913535924809, 0.6158072178631191, 0.6263696552282854, 0.639995984132014, -0.06374442294056926, -0.7271260296835931, 0.6463773414423255, -0.4133419328853123, -0.7821124136895625, 0.42474369816905533, 0.030512704388335754, 0.3344879354735515, -0.9074097519485194, 0.838881472625497, 0.0020230737812190203, 0.9435545043363149, 0.8304409008326974, 0.43491568458001195, -0.15377418736520254, 0.4783189542620825, 0.20967007528573545, -0.20667340365355424, -0.9967303441981716, -0.6409010097567893, -0.1690786916808107, 0.09855831789634006, 0.5073918228037728, 0.44796360997551155, 0.9248326940072422, 0.3636260612546345, 0.7286005485882312, 0.5685895215686352, 0.2726986310508899, 0.551555516799277, 0.09270794133868465, -0.2514459144621053, 0.44620120648581296, 0.38457681360532403, -0.8632113944265325, 0.6979980572438231, 0.5767783419157098, -0.3699617716703647, 0.491895444268341, 0.24079636266024962, -0.06631213428858618, -0.8661530791230745, 0.7247475689370286, 0.007602576229722491, 0.10864428648972635, -0.04365782225210668, -0.35403598668523406, -0.06006402641241437, -0.029240585915124795, -0.9145319271093622, -0.699264227831494, 0.6537675990127327, -0.35253834548713225, -0.9465899099608435, -0.8269927278134841, 0.8026279897003832, 0.035480218100937266, -0.636258070459762, -0.2201526078473195, -0.4125079227078372, -0.9109415793875133, -0.5741563003196601, 0.6952071082568523, -0.882164616576554, 0.8399502211819307, 0.28062147245764923, -0.2859740976977734, -0.04612727921328186, -0.8276010397613915, 0.199029603418293, -0.9203658796843477, -0.535348990305808, -0.4207621058201252, -0.8101944517392259, 0.10409072677023934, 0.3070997852805406, -0.5362924822161548, -0.7381881765710572, 0.5775394516302896, 0.5768146176855771, 0.8769736856682435, 0.9747871260352075, 0.12568175079592447, 0.09499149876579116, 0.8342479971855248, -0.6061394727336396, -0.660391959899596, 0.1875395892566194, -0.8831819957454983, -0.5386181402187977, -0.9758214563083012, -0.4913006093398302, -0.04791470229799355, 0.8907595946664817, 0.3730329658306537, -0.7943973776918805, -0.3719520325535133, -0.7534882024587848, 0.5725255282344084, 0.15355314033656398, -0.43518940104436865, -0.10039165477738421, 0.6974851862571352, 0.4857888865061353, -0.7719287796074574, -0.546620784114983, -0.9125688405764587, 0.4318066402646261, 0.45981778406643925, -0.16169687353435203, -0.09238695136072983, 0.19580264257537183, 0.8586742590743122, -0.9289892949367093, 0.8059933591451576, 0.1977384125825261, 0.5759503109589634, -0.19996981104151823, -0.426892935785649, 0.6535309680151138, 0.31262686333568057, -0.9681505017011267, -0.158045452812428, -0.9587614538217086, 0.2561241238975016, 0.5995451292219047, 0.6801725496172195, -0.3540868331112237, 0.5268307127178788, 0.9156810206918355, -0.3489092866827852, 0.9218700449070105, -0.6169488039756952, -0.7525585584307961, -0.1005832689752384, -0.04696361518580394, 0.7033411959334017, 0.35501199571891395, 0.9283613302130287, -0.7246502763423943, -0.6464075041779613, 0.7466898749128394, 0.5646755473751686, 0.5260357651217613, -0.942005150955284, 0.4883025175597344, -0.176425006831455, 0.4291491873772848, -0.8900429625544286, -0.28586617011457216, -0.49344573160134364, 0.03941862032967802, -0.43683664389824606, 0.6649863883620659, -0.6584728876096657, 0.8563445942975101, 0.4039319442137297, 0.43703894474964566, 0.6936999447978167, 0.24359069302577696, -0.3400883045949361, -0.6401838520100109, -0.10057830048272765, 0.1336944037901151, 0.46095005173387893, -0.9997875674583272, -0.16067959079260752, -0.9662852280818313, -0.19318047026248442, -0.01455583932242055, 0.04300300913912625, -0.7280448228222178, 0.3380963989266319, -0.20652319706525435, 0.18797908570933775, 0.7805039910843008, -0.4056561623854327, -0.9752962603925883, 0.5412273304679944, 0.257714701038275, -0.1701974363569141, -0.11497517943062974, 0.0158956143993072, 0.9546789243872653, -0.03107850565989434, -0.4263636913081774, -0.9957070478588164, -0.10913602893299834, -0.20641011199091674, -0.20809246340697585, 0.8931072715158783, 0.9861065385560559, -0.14073678695527225, -0.6285476108918049, 0.3208499532387925, 0.6979740943955308, -0.44547104898705503, 0.8131576980104245, -0.44317699712306524, -0.8387039156275908, 0.6428419099096256, -0.8689754113428931, 0.3054381070850918, -0.3441568097816603, -0.5708665801001882, 0.12798229035758824, 0.5551691324147432, -0.25905472535617724, 0.14177828483560684, -0.3557633732795371, -0.9404119737086145, 0.7555715362939468, -0.9824276427364944, -0.08709298746237759, -0.04503091666882586, 0.38563716366171064, 0.5347164686336208, 0.49787406176082816, -0.8704036241039215, -0.4287925218466171, -0.05145493524095257, -0.15331915476839098, 0.3225943019754065, -0.8718288770378357, 0.7769642231247373, 0.880162675892769, -0.6150307820095939, 0.9660688993258744, -0.6873419981792579, -0.11497830174695478, -0.4971290437067206, 0.35206704896793806, -0.20473765479232675, -0.6401506657361073, 0.36956439835900534, -0.02391200734602994, -0.03233186791885023, -0.4193919032010436, -0.025587376002276008, 0.04015593460994471, 0.3274365540313333, -0.2785731750357767, -0.18123549308074716, 0.47837428117300473, -0.2899473603976215, 0.8129551393767604, -0.3755873237790808, -0.5512151160135532, -0.8193055025034752, 0.24393351001653674, -0.34023509196024837, -0.9836211082193635, -0.7539126186482239, -0.04744854382382191, 0.1593989459781171, -0.08476964359523498, -0.1347697606790561, -0.3702833434267072, 0.33917130924772665, -0.8239521703246844, -0.0487507383608905, 0.6445747521997887, 0.5140383834577087, -0.48765523334208294, -0.2196334345781541, -0.6299761040368512, -0.3432924020311352, 0.5662744823023804, -0.5291047463552581, 0.42326786646238324, -0.8293239921660838, -0.14797665102045476, 0.5508412172083319, -0.03367404946273678, 0.2298218475952487, 0.48439508909918727, 0.09423094632364948, -0.19818492347471905, 0.23145659574837252, -0.5638340969883355, 0.9027034326514067, -0.9711186086880721, -0.19947063269260634, 0.384462110926757, 0.5137083319378852, 0.3807931172021972, -0.4209076350306804, 0.7728072650005864, -0.23481907819328662, -0.7108245950363354, -0.7288304147818787, -0.47688464136976894, -0.8371793804090468, -0.14283619931288838, 0.3344544718917779, 0.2553039160535586, 0.3953751389851734, 0.6217708550215795, 0.5745504435736775, 0.6104804903975249, 0.9239702405785051, 0.8385599128683838, -0.43375211504234956, 0.4638392553328956, 0.3696773002526137, 0.8908946026466853, 0.7147651926665, -0.08083658358812706, 0.7751453544311253, -0.7121953634070812, 0.8647227823191008, 0.3267399038633192, -0.2116846609766474, 0.10706321879062419, -0.07584485411811559, 0.8220645694848019, -0.7548098019179048, -0.9959234441850326, 0.3752715557500277, -0.4148174599403467, -0.9876371293794874, 0.816054288472255, -0.8318656564702602, 0.4906153640824684, -0.14588113105496125, -0.9837887826879537, -0.23092263877908747, 0.6916925630594839, 0.5234950016056921, 0.07836003076077236, 0.3907177096301797, 0.06523109925609627, -0.1931422278035808, 0.9971409787718748, 0.20270344485057423, 0.9578747418454989, -0.03416283574956602, 0.8037229943255229, 0.22280268546018123, -0.4960634780245463, 0.5077882086940713, -0.33630770723561576, -0.33021727110974397, 0.8002555634791542, 0.9477069547661876, -0.7765252544976662, 0.9942152027350906, 0.8446477519959334, 0.7587605027107869, -0.16221955674452238, 0.9062250480066942, -0.950497864715041, -0.6295478013602629, -0.7515388676124122, -0.3362489246980298, 0.3448544642438005, -0.7054344613417134, 0.1687684189407075, 0.623634239955384, -0.06520777676549794, -0.7365662008936142, -0.03788140739329249, 0.9510560867492357, 0.19850068493952322, 0.4967918082729481, -0.0953630336245761, -0.5841370787205411, 0.6614160007592578, 0.7792735344279123, -0.8702477751412288, 0.6732818952286792, 0.7453203791646943, -0.8630531952489271, 0.8977514573324521, -0.7627097369794187, -0.5034752096112027, 0.8218131105078434, -0.8076154084333296, -0.020893077700159024, -0.7270001725291042, 0.9878235358189151, -0.9449926520100078, 0.44914313314434096, 0.49681740557504495, 0.538187384244549, 0.67641434948409, -0.12001759918126531, 0.8676061413993281, -0.852473177401391, -0.506082112647551, -0.9419379103271557, 0.4710641875710906, -0.9423779580073319, -0.7646385283165007, 0.30546626703683777, -0.6231462319359777, -0.4907773677531504, -0.8144458622744333, 0.7631049921461404, 0.6959281414996945, -0.12378671634145544, 0.07153331299844767, 0.5958792077143402, 0.15523023259864277, -0.4968683593455725, 0.8141170009504479, -0.6995182236361097, 0.24316963132065683, -0.28207558807988353, -0.9521892067105162, 0.9883959111326155, 0.1859421973466009, -0.7427946402796566, -0.11155986408435292, 0.11803115841809486, -0.7976532473716997, 0.7377286001492009, -0.15181962621919554, -0.6324097398473523, 0.06763696685072595, -0.05144320770096389, 0.6319007680870015, -0.22995205448260703, -0.8805243660365929, -0.8658326770080553, 0.78764946721813, 0.17303498163747433, -0.8668458060772919, -0.35942373930649896, -0.6991422889362622, -0.49953122831050667, 0.11944952828429956, 0.26678391580413896, -0.6256514026362612, -0.085162885026145, -0.9639244740835216, 0.3807351895172104, 0.9237866729600832, -0.4618823176321556, 0.9090161919414914, -0.37041977246824676, -0.21437847053889292, 0.219531015960035, -0.5334530773600785, -0.10264824156356989, -0.9246846072249462, 0.6691610991624979, 0.10619709783031639, -0.11274619150557919, -0.7502774971462483, -0.7683067802377606, -0.2007667059699625, -0.48171291808868455, 0.337100080137136, 0.3440497297153866, -0.898072713556707, 0.8782375571001779, 0.05427322451769667, -0.6392845839276691, -0.7355164587127951, -0.29985922381138863, -0.806992607592548, -0.5038042043282551, -0.7236851221744489, 0.5360873543105256, 0.6727055342534578, -0.9566993059566007, -0.7009625392275185, 0.42098856366787585, 0.3848782892589393, 0.25596229124773573, 0.9900792817478623, 0.12566818139221247, 0.5759784478018874, -0.5724886032870715, 0.13074415507769976, 0.8395095910741326, -0.08612426234795412, -0.2385683110892669, -0.5384612512310631, -0.4645514902107739, -0.2617596549015382, 0.21922678585420496, 0.782653365812003, -0.9564973420961418, -0.5122364606202989, 0.04014248024228584, 0.34747627117347646, 0.12069106847454925, 0.18365451529160115, 0.8307313574840092, 0.4981922104825587, 0.8120361617369842, -0.46693825056934424, -0.5602768425734683, -0.1446677786736874, -0.4099893400061767, -0.10588267154837205, -0.5877069572922144, 0.9094202414853156, -0.09465957324594432, -0.611063590778403, 0.06517030926695533, -0.9952114648183394, -0.949755969746483, 0.6202812773413362, -0.7775991454360636, 0.5997947745702552, 0.2489832726780501, -0.7511091367648168, -0.7764645755252502, 0.0709725906778853, 0.36577569492831263, -0.4443656581551685, 0.22156593115254353, -0.6009336314602143, 0.5755399260990599, -0.8982592151355944, 0.23261660163796205, -0.039634008927400766, -0.023727671039507126, 0.6316716567730665, -0.14082777235661492, 0.26601708604552377, -0.4760653719177035, -0.035957496697064384, 0.5042040035066728, 0.3180890559883358, 0.5120241205067602, -0.07960253509415627, -0.06089216670624853, -0.859146357022333, 0.5606866554668455, -0.7400799980692971, -0.06622961172804165, -0.8765879366739666, -0.6884087026002783, 0.0981433115301471, 0.7065972100065476, 0.5289471764346878, 0.5554614464027041, -0.6685532674428334, 0.23418903203340746, 0.04038709010466146, 0.36679630037019284, 0.6994837324888676, 0.7385099314080226, 0.7360731115218342, 0.603032331966862, -0.9934117070144586, -0.7229686350408797, -0.8631860578522523, -0.7577368136450786, 0.03033342335593603, -0.4351076181765463, 0.01060844069222222, -0.7099497149233709, -0.7973238720076199, 0.5186294960581597, 0.6125726264111739, -0.7303625856000164, -0.14453614844226625, -0.5315463466696249, -0.5935319362810427, 0.1713419655757431, 0.6715206009176611, -0.32901997507192116, -0.6980390165965358, 0.6954606158525736, -0.3228449538404825, 0.29764190628622766, 0.5262474726596731, -0.6566177853489772, 0.6676263195925323, -0.7275154832421824, 0.7897749562716043, 0.2699842243111741, 0.003614682681783332, 0.5302344031901758, -0.6085251239403371, -0.3273928211109145, -0.5906780965905938, -0.09724065017349015, -0.6060254912396716, 0.5481430862323957, 0.1267291058372111, -0.9166733319074332, 0.9244450444410013, -0.9859761668086044, -0.19692647075807246, -0.9434334405171785, -0.7748952322203453, -0.25158693888340844, -0.19039310509526408, 0.964491125400659, -0.26993461536052044, -0.10098403492427677, 0.3639356438915389, -0.6547663217079547, 0.37434702844876333, -0.9039414640251751, -0.1118574564376531, -0.7079019486859854, -0.5347885144918332, 0.7525671121677453, 0.10428496514945129, 0.03886384895886308, -0.8540791808432997, 0.11177811706906393, 0.3667876042686984, 0.7868349117605524, -0.7588833877968986, -0.6743941294962261, -0.03098464551927349, 0.4981439504709364, 0.21472623153263592, -0.2174642313482109, -0.8288683393085301, 0.5623447127598387, 0.7099493066224141, 0.4204807386422724, 0.9669569109911254, -0.3262100923659983, 0.38736910483850884, -0.5627427035171191, -0.361327258828805, -0.8070452583709029, -0.4064643173450968, -0.9033356032540221, 0.6887737402672669, 0.994114423284232, -0.11492371153934444, 0.11626235031209164, -0.7801151775511752, -0.11423940249475328, 0.7521268129576304, -0.0771422466469267, -0.4356255245277292, 0.03208325408047297, 0.3697806351601671, -0.747029612986452, 0.4577297688684123, -0.21649838322259018, 0.22159085900221198, 0.0946803973972481, 0.3312794773710095, -0.4278408553246551, -0.32477597491511134, 0.34400361307983496, 0.7723844145225311, 0.46887905809992714, -0.7103174412724678, -0.5912996086443021, -0.4589301388645508, -0.275494249274324, -0.8572248095289894, -0.059420473576049515, 0.5671362933969408, 0.8206976917559505, 0.12080603641907173, -0.8567911333121241, 0.777672105735624, 0.7576579992911878, -0.34240444803294245, -0.17456284632812324, -0.706721009691057, -0.48836318652268473, -0.9603234825061069, 0.27439326643501594, -0.8494945311588802, 0.7005497705636625, -0.04119678141335714, -0.5636040517697904, -0.3972900768168668, -0.08005187801042069, -0.879767508156855, 0.05583396424015019, -0.7954466513613283, -0.28098343024023187, 0.9091479892567649, -0.6002516479351101, -0.9763670011568786, 0.5743786938443902, -0.8190233803430036, -0.45398509252748065, 0.37859924669654, -0.6577830989999474, -0.16167614044511125, 0.2730263015522296, -0.9427858445139614, -0.7669080039798564, 0.34089897014350545, 0.28709005478339544, 0.8018506307795077, -0.4091504981506098, 0.17308025608318633, -0.09237807908493245, 0.10596312244759276, -0.2129275411537639, 0.25821219232413295, 0.6986901005726294, -0.10758727291719827, -0.4973369828954517, 0.8295873025669527, 0.03038496644721711, -0.35948728192570245, 0.30794162474754283, -0.36337291470361466, -0.5911210393473216, -0.4518443935109375, -0.6889928463321422, -0.3862732576151129, -0.6128415361093815, 0.8678574539689483, 0.47417600735193144, -0.45329024132137796, 0.9019224409361362, -0.8412657505834773, 0.5194031943286879, 0.8409551300742513, 0.8584772037705781, -0.17543128658770768, 0.6262152774477519, -0.3520125117665758, -0.28646960177461134, 0.9348504948221756, -0.5844569427977309, 0.30588079808255597, 0.967791556298623, 0.4346663142175555, -0.169053721539143, -0.879420573776869, 0.04103332125163561, 0.7939208895848244, -0.6374448951406151, 0.8813620109709028, -0.5580977332756192, -0.0707628604497057, 0.02857834364464229, -0.3708433149699659, 0.5711351821891073, 0.5286756971397493, 0.04648014046881177, 0.4528200648854319, 0.9815154605093204, 0.04221808120269133, 0.7413907114803284, -0.6104643711415383, 0.2364631392266825, -0.7196221861601853, -0.5924941124136263, -0.5717453154823566, 0.015116674160592147, -0.6170479460335134, 0.8754997073895792, 0.30238265523897967, 0.17135445960858142, -0.17955168723474313, -0.7074892429726638, -0.2884541535274221, 0.66402081639095, 0.605585142791468, -0.8286436647098792, -0.43707269495418055, -0.5706362847451465, -0.05779532248654551, -0.8991471967313918, -0.5960470300970802, 0.4803117658523681, -0.7260220015198873, -0.1047807468633617, -0.5671677154813588, -0.8244118163743157, -0.6295936352883591, 0.13482689765883538, -0.7440802905928292, 0.7316319147677575, -0.016088318997127482, -0.6726931777575005, 0.24956596322444446, 0.9424393939910858, -0.34112738198281534, -0.06139043596730387, 0.6534226890980954, 0.8023970653083439, -0.9321438871737608, -0.295795512843825, -0.8882544054718668, -0.5615723251011868, -0.8913898887778362, -0.2382643506383335, -0.9640343847113984, 0.2039390702441657, -0.5391945489416488, 0.167719156082472, 0.01582080743625447, -0.21814289745012672, 0.9525090088460144, 0.7609322533727851, -0.7581176927235802, -0.25068104641721756, 0.5255531002923406, 0.9987764125908714, -0.9699091877469652, 0.669410336716382, 0.7111512437094141, 0.1949332952273386, 0.5119544575899391, 0.24869237015490864, -0.11877796501006688, 0.7897588152113368, 0.018318550255218113, -0.3688955823399238, 0.020653586949342717, 0.31399709411893006, -0.1175805688961904, 0.8824321623388147, -0.7853429396192029, 0.8359160186397301, -0.5408595667086216, -0.7574050722530223, 0.10544883935600535, 0.9969474438001675, 0.7536538832392656, 0.35987697507052685, 0.9505244924096743, -0.437016535266606, -0.09046017145518426, -0.2235468546040551, -0.47436315533653417, 0.7119283898342679, -0.9748828050267251, 0.15040209348227918, -0.1716941622976198, -0.9444454860312661, -0.3184292392948187, -0.3983131610896822, 0.7176291932581926, 0.014263848446843364, 0.7153085149836904, 0.947092785311076, 0.654270017846674, -0.5722418046564293, 0.1103742304717985, 0.16263736264181405, -0.4666689095182366, 0.8521346490133987, -0.031294758843630754, 0.16129186385966587, -0.5107939628119764, 0.22630461800814983, -0.06317883658550372, 0.7918993757107893, 0.17504957180899527, -0.3052716616288951, 0.48244564579973104, 0.1567183289683456, 0.7742957961627992, -0.520045757257805, -0.762933752235498, -0.9109136116620731, 0.7966795883714786, 0.2167394413183943, 0.6784212398862677, -0.027632322807357168, -0.23317021044578268, 0.5668687989399055, -0.9239094875599361, 0.8568039657368802, 0.6023890500145228, 0.18707881501746648, 0.1623237382519671, -0.7355510463550261, 0.5712121191553259, 0.673298587270057, 0.5349498950594831, 0.06606947359702553, -0.5871657152809557, -0.7177496831418972, -0.7592193857267402, -0.8583307835034601, -0.3216944046413406, 0.9200962522536802, 0.7599683751658426, 0.4282132251376696, -0.7169453180113996, -0.5739643127238507, -0.1390475673883791, 0.7886716895445711, -0.5572976118181572, 0.3830884740692415, 0.7797246959539326, 0.7869606703361793, 0.7622865352245214, -0.686795162874988, 0.2161624290262414, -0.49813141713640086, -0.5494831012342654, -0.16457425112100244, 0.3675493851138374, 0.1466909125635607, -0.7048416087705092, -0.10608334918419371, 0.9434020255042905, 0.4000150583232316, 0.8907056785658645, -0.6892680569604528, 0.19782382983650248, 0.5647250473194223, -0.3223908890323619, 0.9106687492042453, -0.882766495854399, -0.9307418776311076, 0.5522392551691879, -0.1304655353857036, 0.07059093709650122, -0.28754760639128074, 0.17934513069539637, 0.7706292356124262, 0.5659305656427673, 0.5616489904876143, -0.7812393858925237, 0.7963829313446213, 0.3226365648462286, -0.45942067997817304, 0.9069395337779149, 0.623339887395788, 0.8238549300642446, 0.3435978108883566, 0.2880760352198293, -0.4076361363056302, 0.1450814168495269, -0.4811354132340353, 0.369391089581806, -0.7746273534124968, -0.16025524165376015, 0.39790609831264323, 0.6288013365177783, -0.7725355110154677, -0.5586349568144087, 0.4391494578967101, -0.6918460562733786, -0.13758741432841526, -0.6607574607002158, 0.4460378451830702, 0.6871437538377452, 0.23022385191995198, 0.9480128954206442, -0.7112990332895941, -0.4404683702342702, -0.9352336553368483, -0.4287936347829284, -0.9160192005999861, -0.42871697714070844, -0.9073592198876466, 0.33060487477102374, -0.31587884261571153, -0.630130348608378, -0.18926706836667972, 0.9444771172726869, 0.5663127020652854, 0.4492782840983911, 0.5210878890831365, -0.8473728777265601, 0.05459585984052007, -0.4790630300517378, -0.8993569519703537, -0.7461525246138425, -0.3572868286762294, 0.1859591526544122, 0.979805521026033, -0.047091154730140916, -0.07737504101117154, -0.27868344267473955, -0.5181164669828926, -0.8840185018209448, 0.4633911970504705, 0.2696056146462884, 0.007965018307485527, 0.9806997416335499, -0.6791144952983466, 0.6353861750540801, 0.5868202550303103, -0.2693069521812326, 0.06406297614036438, -0.21073380566116073, -0.3631139907862002, -0.8118916270035055, 0.07417568136684327, 0.6305495098367826, 0.21713987610018326, 0.09262123309295589, 0.4346750509309083, 0.7731048306641355, 0.2161245708920183, -0.945182958991547, 0.6137915822286568, -0.7635056798049442, -0.3970017297129007, -0.8850413487329847, 0.6232784456371423, -0.16175949773171672, -0.3063858225649736, 0.8637870344201366, 0.08190613234614053, 0.24209999145217376, 0.3797683714469109, -0.3148455546899869, 0.8488754344975886, -0.5092763111535876, 0.6943478554476006, 0.24546012349171042, 0.3319801825205906, -0.24247035998995758, 0.2914116306307639, -0.7834640600538665, 0.7712708099835552, -0.42141195799155007, -0.9261889382013553, -0.49298020313067403, -0.8511674773492377, 0.13987057413245552, -0.18503535382267144, 0.342436720625352, 0.6852898251921444, -0.34046470332621914, 0.7490212225414994, -0.40294279673444744, -0.861775007408526, -0.02725033245544517, 0.11372872511699672, -0.9451006138220133, -0.3853804117919619, -0.02038568969576593, 0.6341575198079406, -0.268363159212766, 0.4599930882239063, -0.772360143107961, -0.28685791667239546, 0.3086185128255572, 0.9370822441050515, 0.8193824973282597, -0.20030885472289017, -0.5681095241425915, 0.8631845052596565, 0.5165147942503991, 0.055626118652030376, -0.8806728916111282, -0.9710765705537518, 0.835730175322769, -0.21111123372599394, -0.16809622900365917, 0.9752935337548401, -0.9730911104310667, 0.6005520458873554, 0.5922802263776006, 0.7527707089305848, -0.17321673209418798, 0.8430930743632377, 0.7200499354520846, 0.23036079060222536, 0.01028464680432184, 0.4900328306340742, 0.6047132991284947, 0.17672338994813952, 0.5610837664011097, -0.8347114257726225, -0.14132278825716882, -0.25937451992196636, 0.2972088924298881, -0.22200367783979358, 0.3700225273644686, -0.829685354454061, 0.816022480129891, -0.7965661349562827, 0.8240802078989193, 0.29740754608820175, -0.9011611240093931, -0.9193439905529803, 0.3088676978196663, 0.9742905926927465, -0.5400428501563186, -0.6723458754555169, 0.9518224014511694, 0.7710677407461384, -0.20962794404459717, 0.014275029220489621, 0.49938875219489987, -0.7519257223989064, 0.9481182481572616, -0.5352230331062775, -0.4333292253865966, 0.21918294170091346, -0.10717488361404603, 0.0037157852211628928, 0.9177746462253298, 0.1800661621647841, -0.4412766999435003, 0.1720811083804028, 0.3890314053951529, -0.5330207129772171, 0.6047014191860052, 0.1527264029062554, 0.8828833080588829, 0.39533999439754774, 0.5127978946691543, 0.1918526543995649, -0.5287129952106573, 0.2697302784590734, -0.4693034170067216, -0.4087575277209694, -0.6449898460717858, -0.9844859111262094, 0.9000982932380965, -0.5320716485981289, 0.04323596486535042, -0.45532605376851465, -0.3578715134728456, -0.9972517252577853, 0.9187756710698216, -0.48404060117155523, -0.4572801223083891, -0.0683294793222311, -0.8376391556976122, -0.8151550859441761, -0.9234431815857314, 0.024146868322034054, 0.6021001474294432, 0.19427287544449845, -0.5198922524277425, 0.8509825052653528, -0.8585745346067055, -0.2646669222637643, 0.6961464236567712, -0.4492021883896753, -0.030594442456196802, -0.5811423074369864, -0.36436511453007325, -0.313471994277128, -0.7716094356169418, -0.9703876184664271, -0.09126550263124233, 0.537577339746663, -0.9315878430785622, -0.6256306486054275, 0.30043703381825426, 0.8207586615510924, 0.4471841118015878, -0.7835098785254688, -0.6004365315659455, -0.8821303996103873, -0.44471104736523515, -0.8133222675675387, 0.39391062182742775, -0.017793735074232764, -0.9439083248796138, -0.3708934748017456, -0.6747904832604321, -0.8731535208897618, -0.4887342554402987, 0.5369494708486953, 0.9221592339182523, -0.7343318218488262, -0.8051103751542632, -0.9421141701608045, -0.2774744236550404, -0.47814962387645155, 0.8856161250125669, 0.6973360864816649, 0.6324442693107482, 0.03899666727872497, -0.25008520573489434, -0.8884711527912428, 0.03290110057209539, 0.5020740550131331, 0.06546364872865529, -0.39212031510139456, -0.28222454264766683, -0.10818754790747742, -0.3587528545930905, -0.7757852095366045, 0.5283895563856298, -0.9921087822178947, 0.8870516362687926, 0.8167787117680982, -0.005681264229610017, 0.9183474943819179, -0.3079839980235921, -0.6009790071687218, 0.34011647232772924, -0.19568958859289642, 0.9912355159484991, -0.9663291285634323, 0.37913959651357865, 0.9176065653701573, -0.8126267871878252, -0.9450362122464424, -0.6719586784968501, 0.2454233338142635, -0.36308440475600534, 0.6821038405060336, 0.9485019019951235, -0.6772987890305953, -0.2734865102761157, 0.40893716149867965, 0.5811848216197788, 0.20307579484189198, 0.5009940634091341, 0.9214156968478928, 0.4462877878705396, 0.5197217170338735, -0.7104904962890337, 0.8197659508626989, -0.6049949992891268, 0.8486184547500573, -0.10317854724913023, -0.9043428039901427, 0.783607356286613, 0.35720576898651113, 0.5192527270042924, -0.9036946540830348, 0.3841629177738519, -0.19159647035004435, -0.6362182183061296, -0.25803134301818753, -0.7785227374906256, 0.33920496363168806, -0.976075023129064, -0.37330129651794874, -0.33040610545917626, -0.5855692252530724, -0.29531912330981, 0.4371720540578936, -0.8522793598644272, 0.41013949701930996, -0.18498977220162582, 0.78087923638709, 0.3998452114988331, 0.4946189162997827, 0.7382996383692473, -0.43855109496892575, -0.8279118644335652, -0.09272518804329177, 0.7532764145438788, 0.21871466882746793, -0.948171943279593, 0.2691878001400507, -0.9837683645626001, 0.8590045655079626, -0.38498959514966935, -0.514840623550064, 0.17372942471916497, -0.7541259967020992, -0.7860410815668437, -0.4733800973682598, 0.09989399646646246, 0.5948103646497789, -0.41735158867787114, 0.5703264438181324, -0.6859018146954747, 0.9048434734150324, -0.5011919357433512, -0.2215438747428795, 0.09259193363398022, 0.7058183749000431, -0.07026360615562988, 0.6536795040201535, -0.12687725245745995, 0.46837766964562233, -0.8768698003848727, 0.007969730166231148, -0.7544511090410972, 0.09427392742298157, 0.08258951794761771, -0.7098279917910284, -0.7933194865251925, 0.738000759069787, 0.3577888567626679, 0.5404638689177881, -0.06356164346103643, 0.34489561491764587, 0.09412649557087627, -0.8704528203367143, -0.4688554565128289, -0.2012362855638956, -0.7085218716087911, -0.9354411449360314, 0.6277874767460321, -0.43168977788073803, -0.31615907671635557, -0.3486446580811646, 0.18492311503487713, -0.8100653221288534, 0.9220501380912347, -0.9034963456212528, -0.599550010673606, 0.5022144350079667, -0.9529864611084156, -0.5219131302354507, 0.8484139845784564, -0.6397496687845592, -0.8204841110350005, -0.25551121992971315, -0.8140827271141591, 0.7703542198536939, 0.3368620342297368, -0.27548903854862816, 0.7628014868246584, -0.015356714110587921, -0.9362058966649398, 0.8101888054347604, -0.46059114910998034, 0.3493039471491508, -0.8535663495570975, -0.5332829630873217, 0.5642192220094326, 0.2614948258895111, 0.3279737394321891, -0.590256576449194, 0.35708913293124844, -0.9770568805195314, -0.3868170957638095, -0.9105647055473376, 0.15465582539893785, -0.14938968302012423, -0.7688381699384397, -0.24519239167248852, 0.947445742113812, -0.44179704027224154, 0.8075548133315089, -0.16505919973722616, 0.11702092486938165, 0.08006714501270817, 0.5042556169142025, 0.9400249472576747, -0.3214196742023483, -0.8787613305834183, -0.21561683146746558, -0.23702936972580924, 0.20184869943164285, 0.4483330597362436, -0.2946554224729916, 0.3285300416987238, -0.8869676504854545, -0.25886426752375225, -0.4994508771907302, -0.18762969861495948, -0.832570279693817, 0.8632424908814655, -0.4613021257551677, -0.7375948570503135, 0.38843504346701496, -0.2008799739418321, -0.07608476214789417, 0.9774042564085499, -0.860194943232623, 0.5625259663426949, -0.5079079611297737, 0.9203349106921661, -0.1714358891488772, -0.5592509979217695, -0.6798914508159826, -0.9202963222210878, 0.454350184327589, -0.4718370989575793, -0.5148015927625877, 0.6479511971069782, 0.9770344266962769, 0.9563699447648111, 0.19715527763608254, -0.26676443781838977, 0.1394095363914063, -0.47722618699856834, 0.5384023694594655, 0.7121074277117507, -0.2895235141252357, 0.47894822726170516, 0.347822564369757, -0.5245925800057878, 0.2846406711980569, -0.48251317060899, -0.20634425347855423, -0.9947763852924527, 0.5141456052779065, 0.9150142150002556, 0.6728513679120063, 0.647980283151659, 0.3219020371182595, -0.19047607084569518, 0.1270346499157291, 0.3766392594746264, -0.07075717540533355, -0.13075736257249515, 0.7073946837899401, -0.9355554106026878, -0.5096706480141413, -0.09939248990443761, -0.12037233507330947, 0.20357158090303384, 0.39442827868650854, 0.34230700742264286, 0.993642435346149, -0.8509275225765816, -0.2459902281895523, 0.2929178638009855, 0.5854066477247724, -0.8508021791369043, 0.2090101669595441, -0.21174441360366192, -0.8718017412783476, 0.40920339336798617, -0.5547835126285223, -0.20032038987313117, 0.8732488942861223, 0.485320151178082, -0.1967492752394684, -0.6881946766958766, 0.6762841565133777, 0.04485572207232158, 0.17250183747320302, 0.33934053906251194, 0.2580391988045403, -0.24752162924006527, -0.40570331161328843, 0.8869454491823761, -0.5826716491553381, -0.05973153459964964, 0.8326559575425236, 0.5743807385949873, 0.15120150076739103, 0.5471102377693322, 0.30060937257931575, -0.7370584595777501, -0.32532665398982097, 0.4278592553646341, 0.5034706094907089, 0.9751594524452798, 0.5593316947745854, -0.5901280921286638, -0.6129119032599442, -0.2962830313451652, 0.18957889465768907, 0.22006092491478335, -0.9073971613083107, -0.9181189370623726, 0.7005392010620346, -0.6053897944888875, -0.4081361537868391, -0.4903462302600099, -0.4418896109265904, 0.11532225479713132, -0.8123965760723484, 0.972553791766249, -0.6580143683007265, 0.6855777819684101, 0.4186737843483883, -0.45485157988219393, 0.9578543376334909, -0.4478803807843379, 0.4973321773034547, -0.411192646353679, -0.4282890854673296, -0.11894797913811295, -0.5323433583379384, -0.1701265510657124, 0.7951745463731348, 0.09015332674589471, -0.7681778060528188, -0.6198591038429919, 0.611734946416634, -0.88723586449036, -0.0022142493952450604, -0.48552402052532484, -0.3459033315527591, -0.8910491813166443, 0.1241823509737785, 0.7598531781579387, 0.9439220611457662, 0.3338851620615009, -0.4706537485175353, -0.6320322833169159, -0.6185444507938234, -0.541527443451975, 0.6974916387273158, 0.5044832458876813, -0.8372089365799571, -0.8217775785310375, -0.8455593544000277, 0.018306432167964015, 0.70778134226907, -0.6259870019656366, -0.5504646911061768, -0.37276173242408905, -0.9517247941788245, 0.2685050000970528, -0.7613384869507824, 0.8538700227359177, 0.04031672075807902, -0.4779970867032617, -0.06200752450984304, -0.08228985153159796, -0.7770845685276917, 0.16914651781401036, -0.5820027009496844, -0.398304763889213, -0.09791812065742778, -0.03949620781927665, -0.5608269464321725, 0.03329574095034871, -0.41257320575266654, -0.8269285126848478, 0.8230089792721731, 0.6020295465725691, 0.15861800007536453, -0.6633100192437109, -0.993243984906693, 0.4866186429996524, 0.9336368780514577, 0.8628953252603195, 0.2712443562550697, 0.7868042706501657, -0.6852809614388422, -0.39932035151546863, -0.859357811824194, 0.572266358193066, 0.6136219307840813, -0.9275906773274636, 0.26433280959636907, 0.24377132977087346, -0.34951987233731585, 0.5632506220644204, -0.30441410074965614, 0.16643465910133526, 0.7188506806878243, 0.0964228061374699, -0.644285612031082, 0.6747506498519231, -0.953758139915784, -0.0032394250648297707, -0.9510126650086204, -0.5544695620225388, -0.9719767509450923, 0.17308344703099832, 0.7557653480269686, 0.1380697230320016, -0.16319117041426234, -0.3448138191056844, 0.5903656917838838, -0.1116264510348115, 0.8296616269455155, -0.7682754271340821, 0.044148245386611995, -0.9516425854434674, 0.3796391907437542, 0.17955566497983622, -0.12866328978382535, -0.42230187324902557, 0.6292561777075758, 0.9584877050991885, -0.8120578945547321, -0.403733745986665, -0.7669936582384738, 0.30475391622030146, -0.8859341838683308, -0.031188287521311198, -0.5852162502107512, 0.012350019548977409, -0.925119654187085, 0.3892595384757742, -0.28961025711317223, 0.673637239770378, -0.34377793613892105, 0.4234059229128939, 0.4778516376787545, 0.4859205761404881, -0.5576788330697786, 0.1242788135635986, 0.37056391493981033, -0.9189643872377729, -0.9695802976436623, 0.5585648436479669, -0.07158996021894315, 0.31132266597835834, 0.38175683024640383, -0.28702850013658865, 0.5314291037854171, -0.7974860277334967, 0.5376253613416169, 0.46462181811542624, -0.9956791968375547, -0.91620321446126, 0.5468641059134851, 0.11607804467253158, 0.8747954995183393, 0.9989862283772983, 0.4450423206344012, -0.39075184973993915, -0.3577606256541379, 0.23650306707452073, -0.7492181979367405, -0.9048846264223018, 0.7655529798687177, 0.3934931355175546, -0.39570190638550584, -0.36856534068595903, -0.7602066582388718, -0.6303115293590753, -0.1793973163874385, 0.4096330230301237, -0.32985992480883675, -0.2060528619899542, -0.5425598541948553, -0.8175159170774864, 0.3666097820113252, 0.642817491031439, 0.3661270933393237, 0.38762336847782675, -0.033175283582612236, -0.5310699014914366, 0.34204178242495686, 0.8217431182323129, 0.33547451913129867, 0.4051239679676426, -0.9507021973600052, -0.27855704978228135, -0.5358402452130071, 0.7650541003761118, -0.20473557818528532, -0.44652999383392755, -0.6655097741827574, 0.2502164336891948, -0.5382429046911916, -0.9105554361278207, 0.7033113741129533, -0.9600303830008929, 0.3847503631757694, 0.8145551230711654, 0.1843403072649854, -0.8699773422574208, -0.28930159005534617, 0.9198940428478137, 0.52442428240536, -0.6393969084396804, 0.7251940240899435, 0.29065441677662585, 0.685752166113883, -0.9210401634247212, 0.304229308844425, 0.06720968675723071, -0.8294184300934673, 0.4094184620576342, -0.6479184723551878, 0.40522725544953886, 0.2089366366426193, -0.3134627386773978, 0.6296375282983198, 0.08944004911566017, -0.4550462380254663, 0.2003179265248467, -0.8042370022847287, 0.9799206597044443, -0.6271767198467357, 0.4329396885661927, -0.23852039356035104, -0.9529830938187351, -0.2282496395611655, -0.8391844471846479, 0.22852746133517177, -0.4827977813424069, 0.5823062230955094, 0.015709026238442503, -0.6622044849986302, -0.5482206408843784, -0.6038803764117118, -0.9209737308646524, -0.26023722821464834, 0.37940675221380826, 0.46616653275323183, -0.7998021480026536, 0.9680665551408016, 0.44625138023194855, -0.5706666200713264, 0.8847646519071652, -0.9489472724348285, -0.9043248246097244, 0.8921173306919175, 0.7721573322625426, -0.9626924361857356, 0.5637453159789259, 0.9171413833123216, 0.46393835725388266, -0.493074137680398, -0.8148772495646806, -0.07418604326827105, -0.3269600083350772, 0.5138673409365284, -0.8531361959278139, -0.819427388274387, -0.18709072952285077, -0.5540430163387882, 0.8243471441789718, -0.760249051768358, 0.010843776238233183, 0.07927389902977566, -0.13649804496736961, 0.6628211071054586, -0.5655592382076249, -0.030857148053918593, 0.4518765656054169, 0.35789176349915497, 0.30800802928830406, 0.8301622855741757, -0.8660231155877771, -0.6532347963479574, -0.9327691605305937, 0.10854392326698603, -0.45543975347956334, -0.3326245080182928, -0.6408181599963496, 0.9006997079452281, -0.4240820971226833, -0.27509804614100175, -0.9764229566377878, -0.051885422874259124, -0.1635203997806507, 0.4266331978250091, -0.4477421460306783, 0.35737875045088097, -0.6396439998002466, -0.7884343917125358, 0.2759647108943568, 0.40751153280661945, 0.46825264640123354, 0.3601714896516177, 0.1533136747734931, -0.5471040771440216, -0.4993256270613522, -0.8643663743434657, 0.038504812842879765, -0.1135090747379397, 0.3719283352542657, 0.7562973688364363, -0.6798563713804902, -0.31242259269229455, 0.6458472345771942, -0.7120657383600242, -0.9542521108258786, -0.46099699958177487, 0.14235379747994892, 0.5842501087409546, -0.19127950132733273, 0.5957173696776168, -0.015670580043565918, -0.3080895890193944, -0.9700452716725978, 0.45596831385938774, 0.426202525180164, 0.6790683773948352, 0.455622807666932, -0.8262995813890739, 0.2180182554220993, 0.1909263302652109, -0.9992067063197745, 0.41357850404827734, -0.8175017398451216, -0.2914867264480163, 0.26474507797513924, -0.5923695102343107, -0.2428977933795189, 0.8413076536774187, 0.9055937266317047, 0.34359384772778623, 0.16259851305408723, -0.43259513766093893, -0.011725128328468948, -0.07814401556801687, 0.8777829743435002, -0.4850666109528976, 0.6759947032384004, 0.684349666765296, 0.9558726074980508, 0.8703345859043083, -0.8538919449003011, -0.5217526520865594, 0.12874315776304202, -0.7078293182416577, -0.1843671798277995, -0.11903214771358006, -0.4770568032671487, -0.6828113683076011, 0.9962079650143962, -0.9304633674701874, 0.2968108628111268, 0.19082167242693582, -0.07185555894458262, -0.3334719018115704, 0.5967250486234441, -0.04383876571804124, 0.8627848401544367, 0.6017971536583728, 0.5676933891671068, -0.6393846691311711, 0.5897827575964849, -0.9342671571403709, 0.3874566058518347, 0.5031516436257022, 0.2422363558796512, 0.5640819364089811, -0.10031540733820288, -0.8660425855128868, -0.5685658196567425, 0.8603288116996572, -0.7867779812318885, 0.04231234229958547, -0.36200739975822094, -0.21652061409857204, -0.8015610546006784, 0.6608106083949077, 0.14804444000806427, 0.13121593132951093, -0.26161467574542385, 0.5702515570326476, 0.3179998832800315, 0.1351057134196907, -0.2689113037891635, 0.9921461509028311, 0.40894768523349856, -0.30394965632924786, -0.8817103150974286, -0.9640710070065923, 0.21663504137144862, 0.18823516146723906, 0.39714150395144654, -0.49403412464850316, 0.02191241418841816, 0.2690606200627348, -0.6127132388925545, -0.4215804276755064, -0.3752138944228305, -0.40911386754413837, -0.5537269904645885, 0.5015016956873155, 0.37171050246286885, -0.9590563470781508, 0.9365320685061955, -0.6919613345825444, -0.9382194544030935, -0.1842821433843984, 0.38677408111885825, 0.32805761493437324, 0.9105746009979128, -0.483922133152453, -0.20357346116900144, -0.8506775047971, 0.6671273988687181, 0.5439873199117513, 0.5510655665572006, -0.053233850515810976, -0.010654454895435483, -0.32562791666653945, -0.67990381689306, 0.08886635786886221, -0.5377362938610248, -0.7200361006795457, 0.05561245739826304, 0.7929090361310007, 0.47290405171674, -0.902227589716152, -0.9606055889305483, -0.47556825848039574, 0.5817591967096716, -0.4249982640283425, -0.8325440444673247, 0.9052299918439166, 0.8490081349735368, -0.4976415011533768, 0.18759646269918795, -0.4384169085424552, 0.5149584392572693, 0.3558201389382578, -0.47158620835703413, 0.9270892651718614, 0.6160565278724106, -0.23654704057341225, 0.17948406010270235, -0.09430865413233747, 0.02916383262995792, -0.914767519543042, 0.23948298246501398, 0.0029886160867023115, 0.9901151508184132, 0.18883393559155426, 0.9518507387661541, -0.8005366684243578, -0.24033040892314994, -0.9474592946180209, -0.05144551672368802, 0.8703564132880626, 0.4527562793792279, 0.6893682884728918, -0.6290944477923379, -0.7805799438938616, 0.00768809906498924, -0.28746260657576905, -0.5697115153996644, -0.7187620686650609, -0.5259763243554547, 0.35281933517978836, 0.535362735717458, 0.4324716801453392, 0.10720118646017474, -0.8753854753367816, 0.7151851446676865, -0.5753069474486683, 0.03940442945775757, -0.5864004626182444, -0.8837527111649504, 0.39644913931442893, -0.7575963140921596, -0.5388982301959611, 0.0001147481974039799, -0.66969889355874, -0.43310570382098157, -0.7928200393330289, 0.9088936434233688, 0.5233849909299542, -0.3463026174387067, 0.19809134232457426, 0.02313413099999817, 0.033637160429214275, -0.861317004116642, -0.4084222114669014, 0.3874318507974097, -0.5351768842893421, 0.9322321494322785, 0.7286139615647818, -0.8194954093810749, -0.9011105180368078, 0.6967318942353988, 0.7594745730597987, -0.3228961174759728, -0.40519353956411686, 0.07506700947131462, -0.35261938513633173, 0.1189150214716741, -0.4666037518375812, 0.5537288522849675, 0.22391431690317054, -0.37252388029220906, -0.8458815015672998, -0.06546057617623102, -0.9102778362969481, -0.5459694139928155, 0.7428690802656812, 0.6644203165251386, -0.834324245679855, -0.6497842334143415, -0.5864481862062485, 0.35314835284064383, 0.030045216312591494, 0.18347117479591613, 0.9790983297649536, 0.5576580043448933, -0.8150669321518265, -0.470777660746833, -0.2148649245785823, 0.5853277130461716, -0.2892194882041521, 0.7223734824076742, -0.9492243207213085, 0.014250170982841759, 0.5624354669861009, -0.6268506459293259, -0.8080867226662343, -0.16347198671031937, -0.0551175393401675, -0.04631957825155952, -0.8342201189800897, -0.3931849625028103, 0.554751028620518, -0.5909669419030281, 0.11616058161424858, 0.3835121478152601, 0.4447032970370819, 0.8174098396356284, 0.41993659440503794, -0.568312012634298, -0.9376518435441821, -0.8488286163554053, 0.61363577386223, -0.924048123800941, -0.005971155351782498, 0.5003942457705259, -0.869920646833032, 0.0739296269888221, 0.3918634260863545, 0.8271776580950911, -0.14466077767993357, -0.07486128145178261, -0.16147983902059204, 0.6090349876352759, -0.7182358272587013, 0.3377482834301455, -0.6383864214081478, 0.6915849945690935, 0.5916109902378666, 0.3126441282082195, -0.8215873637375319, 0.40893976270999044, -0.8121418399246139, 0.11353598468971215, -0.1118370279051979, -0.7823142519490385, -0.1919385018244748, 0.5655346476489145, -0.3654628638842803, -0.7389954004126458, 0.8411204463537343, 0.13780947045914482, -0.32787806703550104, 0.7116819753110895, 0.07222494762799747, -0.3344834541176551, -0.09260691987301994, -0.2649652223986394, -0.5271105730198964, 0.7617689911154266, -0.9064862087931613, 0.9081917933903567, -0.635809289201889, -0.9077584276485571, -0.7154324618285766, -0.9142589504268059, 0.5672448194489619, -0.2538736290041528, 0.9894816449256232, -0.31475080114919396, -0.26024505398071063, 0.8400785207973769, 0.23982905762643947, 0.32295749097772153, 0.4345773903164658, -0.3752006930769969, 0.9092156070471507, -0.7389822201471141, 0.2543594569212142, -0.46934685388975184, 0.40928872596172194, -0.4923712462404106, -0.3606664783915208, 0.1646706466872161, -0.9277545316676847, 0.5946919824239749, 0.31115122721534894, 0.5475863822997995, 0.4160681528849217, -0.6521941946220637, -0.4145717163180396, 0.6101495283257419, -0.02083962188061972, -0.9687197226840127, 0.43521777257774286, -0.21164934307357686, 0.8604879904848084, -0.12305739114160485, 0.7107660469892727, -0.573518878689746, -0.6932840444357051, -0.13657060002979593, -0.3854322106076906, 0.9991324090806641, -0.19904575595197938, -0.33940068970658177, 0.9647499113480495, -0.4057869229702884, 0.9031116876213259, -0.5602750565913739, -0.3871734275605472, -0.8242387194521497, -0.7236833993802094, -0.3760909346467771, -0.9004541655592082, -0.0379978187653387, -0.43556294698531706, -0.6464210346476644, 0.21457473670262184, -0.3850520247975988, -0.8265792342000917, 0.8074386469116728, 0.17655296777635132, -0.530797496370764, -0.07211404058918469, 0.843050395947881, -0.5802093481859751, 0.7069454076811732, 0.2630872402639366, 0.9567017877843405, 0.5423084568422594, -0.4966389632347652, 0.9147160253014259, -0.3958698250183956, 0.6218547139357953, -0.3708363523251945, -0.12487902948491314, 0.6610474330372778, -0.9612804166579931, -0.0861151100206412, 0.8745728231723708, 0.9470456178004638, -0.045337527748813944, 0.9631135252753948, -0.37311987948888814, 0.9143653870961395, 0.04495594864328489, 0.2905151219070681, 0.9679464595297322, -0.5866431330602284, -0.7613249704904583, 0.38721364643972955, -0.2759719424210094, -0.2278481956659424, 0.37339154860343804, 0.8763152567940329, 0.45079732516461646, 0.936378484018541, -0.5838272228094097, -0.6629884694791801, -0.6913438672955561, 0.3999357421793852, -0.5089069233001604, 0.08219783144486481, 0.5516822755925623, 0.25971222225102, 0.42283435936005964, 0.0022962245509861035, -0.7496361577830546, 0.20803160500326823, -0.22508194873449994, 0.3484169058988542, -0.4948384032464803, 0.9598068779261328, -0.7877038520582116, 0.638930416941061, 0.938319438193804, 0.09548876593556455, 0.044866648518696595, -0.5181950503244619, 0.28458537654781524, -0.06387960713277852, -0.48379402887378853, -0.44613196209850114, -0.7230251477397684, -0.6616257743940901, -0.5336556836834094, -0.40629531230104643, 0.5133273570190997, 0.8603437552492803, 0.9271580473003722, 0.35763379108744586, -0.4877037225229879, 0.259152837932487, 0.3432883501393662, 0.12029916162400434, 0.2699564779109609, 0.898143682327442, 0.007511399132787533, 0.887842629734436, -0.9181212210869436, 0.8078727354396387, 0.8300176492422775, 0.2518507533101728, 0.0899237988645436, -0.08521230860021767, 0.8938481474581195, -0.6414121526105219, 0.3325113113538265, -0.4018182451083232, -0.9640242819808105, 0.6793295869617906, -0.7969228507918396, 0.14010664326313038, 0.7887194715972097, 0.2948373842559491, -0.6951731078270884, -0.8611596625388882, 0.6056888041523185, -0.6707730695840368, -0.45798349954861584, 0.18230192857354033, -0.4113966159238738, 0.6673112951621383, -0.10338402679125758, -0.4022503796826946, -0.70126209123588, 0.29498764595399596, 0.580375278249593, -0.5156856518487503, -0.5796203941293507, -0.26690046111505006, 0.4256647940336935, -0.723726077326927, 0.42328301298329296, -0.6653256580453215, 0.002960413160313813, -0.3581846955979786, -0.03455798753984407, -0.158725453591261, 0.40188297988596244, 0.7924790456684918, 0.7490407682933533, -0.18730862983467267, 0.8839726407320561, -0.3968847937642461, -0.24278628459041451, 0.7556232585358968, 0.8134925974224081, 0.2450501538123926, -0.5561607159983861, -0.40423415792329553, -0.5795928936695618, -0.9585792057515583, 0.8534457851342685, 0.9353998630964322, 0.39808802621646966, 0.1994037373371369, 0.9068755794148338, -0.6132406005191942, -0.8830077225551547, -0.22692916742286973, -0.24241393676456946, -0.28320425349764555, -0.8639805352126617, -0.5644348441702915, 0.958679130079882, 0.25130747928178154, 0.7286105336186741, -0.91899234342804, -0.07035745399976623, -0.4078273700398063, -0.3759428525687356, -0.987588566506777, -0.1280904807758214, 0.6371379135154009, -0.7004330195201349, 0.06161999485564906, -0.6173688152844976, 0.7844376638016854, 0.31510138631478934, -0.8215359187683506, -0.19848684775964465, 0.3730705421870666, 0.24589927163955805, 0.9958797990579653, -0.13146142473131195, -0.6223087965193019, -0.612726946773819, -0.2542862558210153, -0.8103827914112349, -0.8499166333216821, -0.4856335734325796, -0.4173949763969198, -0.1637809544522688, -0.748203211326709, -0.5314063294624283, -0.5420061296797887, 0.008903215150891208, 0.9529920120351574, 0.12946902387692605, -0.3247641404092403, -0.9347872193246838, 0.8330647428195268, -0.8259933236538102, -0.5238009251086533, 0.32817605157945917, 0.9166336375901503, -0.850436972555161, 0.9090527595685474, -0.27700429352523925, 0.2611112873610455, -0.7075847027830708, -0.5918155585945573, -0.050668462272038806, -0.3228165972028325, 0.3287983570662891, -0.5343478744194925, -0.06285172323878019, 0.10185593093689516, 0.42894540215489463, -0.9578737643133124, -0.4979451659828873, 0.012265492978843717, -0.900123224435168, 0.4478891556822635, 0.5527327275385787, -0.8584819300328308, -0.8163026561573195, -0.7686691839227364, -0.2710785535331037, -0.6772210135213692, 0.7714228072331082, -0.9226591714781966, -0.8411357313434702, 0.9558588216409225, 0.690313640794953, 0.08822344667840887, 0.514857827824865, -0.6241103453723162, 0.026800766968052114, 0.7976988814369343, 0.5324252251218229, -0.8183216280591847, 0.9169862509410331, -0.3164165311874094, -0.5881341590698073, -0.385564729655687, 0.6008818862739689, -0.2971977178057792, -0.7762270288639854, -0.3059695125593156, 0.4003837320956203, -0.6977167541747016, -0.11951153488864152, 0.35789689249934153, 0.5007058245314775, 0.03132991569903165, -0.9037193821397238, -0.6531005813703175, -0.11731983744843988, -0.16898114341275994, -0.656148256248474, 0.004491523646415141, 0.127089072586265, 0.2647912369733352, -0.1262735780213129, -0.6879729226172009, -0.9654648084679909, -0.5654965602116395, -0.715151124518699, 0.1323553394541399, -0.301929910651378, -0.8997925498739405, 0.35191066381450864, 0.6349691524987515, 0.772917667161696, -0.9208577509481057, -0.789704361257711, 0.20854634974431652, 0.07805979078232106, 0.6582486312530118, -0.27249501214652083, 0.2491162600526433, 0.3366712145342039, -0.2184041063819262, 0.4320366590906417, -0.6562268653817116, -0.2056404218696004, -0.21500504652721308, -0.038775862371378755, 0.8130146563229492, 0.31127200824323875, -0.599784160116354, 0.7952957536081406, 0.6144536085624677, -0.8832477250667767, 0.48329849097345057, -0.1775434056638625, 0.3501026800438649, -0.21970796159776396, -0.28638920489163167, 0.912487264389501, -0.8308971674388463, -0.9035656701165138, 0.47283382394795215, -0.7969106760722711, -0.7924519287396967, 0.11870785921873117, 0.976854898823478, 0.2842405911465842, 0.7560452691301045, 0.271866179220007, -0.7932173016770365, -0.23166087330276985, 0.6897215045969622, -0.2118667334355211, 0.502933221512331, 0.10274737550953628, 0.5061572707955564, -0.07005753536481585, 0.1080744540587919, 0.5721268582781787, 0.7529188042222681, -0.49566386511911276, -0.6012833567232023, 0.8373575572271525, 0.19578568811028352, -0.22520523381381152, 0.37535006570575846, 0.8069092131312927, -0.22535165196979512, -0.10438214072133944, 0.9880770570834652, 0.34401699785188966, -0.6285730778722651, -0.7792394368429569, 0.8132829281052569, -0.5657957223212784, 0.06214239012655787, 0.4277697225508872, 0.6270895557193921, 0.4752920034484329, 0.8855225086789589, 0.13506455203631207, 0.8659087160773642, -0.07280310094001674, -0.3132121289198333, -0.9811495906792305, 0.13020427659269362, -0.08324876850283358, -0.25589298834749097, 0.5851028724392471, 0.010097601950403767, -0.30908433793499945, -0.8014391155072704, 0.3735801585038583, 0.20519714845220438, -0.3919986346266704, 0.19453035924169382, 0.7486528892076856, -0.06990556055164054, -0.3011044806394372, -0.015103880060408859, -0.7318060830380642, 0.9161173966542078, 0.7312957636999837, -0.7222788018969186, -0.3840814067901457, 0.29564183747993944, 0.9599874993163025, 0.23555952995617258, 0.6701076755608677, -0.5266904185861325, -0.5320138854337821, 0.11169166468902225, 0.4174731079133487, 0.3083884548313578, -0.656417348303393, -0.8868103086778174, -0.411496775815859, 0.16991679267340598, 0.701117192308842, 0.8236879050555983, -0.4659829893549148, -0.6786336404030187, -0.44508721765199355, 0.772695214043235, -0.3927805703656837, 0.1984533573626137, -0.4746025794924993, 0.42165465265127544, -0.13573487432789078, -0.7500996324010563, -0.30100120399336294, -0.6442066138028351, -0.641283061200304, -0.41389390699307893, -0.39961685189005336, -0.4292858895284981, 0.519639290573267, 0.7507291889555867, 0.029339396541447238, 0.954011486930721, -0.08715373154851891, 0.5158318387623602, 0.6946137046995913, -0.5943404486124457, 0.014742982062931675, -0.24813020404773756, 0.2249394680858412, -0.08653227151530585, -0.8473503792832109, -0.2379865408644073, 0.8898538772894828, 0.9332556472435138, 0.6363224272356345, -0.28088298900875763, 0.7370926888892522, -0.506045214937034, 0.9880509657109089, -0.06161669249713042, 0.22262067867234947, 0.3493480612476527, -0.21380775862457924, 0.8250464713513839, -0.639813437788401, 0.464943262210358, 0.379115362963071, 0.3805670761678157, 0.379961591721822, 0.1969521378945549, -0.911498595791296, -0.08143051592415151, 0.5475317326347855, -0.14033204871003124, 0.31139634910591263, -0.20282755386378848, -0.9268388724135086, -0.020181899779656787, -0.5231721832243015, 0.4422135264763254, -0.2510290404546067, 0.87470145146835, -0.7205690840522669, -0.36232474395154024, -0.7608263204946193, 0.726190641378401, 0.3470332442869346, 0.07999410756203629, 0.08977514640759043, 0.725687032090162, 0.39155566518890983, 0.6188315572310918, -0.2392003553086719, 0.30775589148036, 0.7348467669040191, 0.6334402108285127, 0.5247576450668634, -0.8244142515657074, 0.7892932460056998, -0.07195781908427357, -0.823439501050012, 0.0949307453562025, 0.9194578538007963, -0.837044500932562, -0.64529398637618, 0.9743557086074526, 0.8865661149370205, -0.9730485950493626, -0.0158560465098192, 0.517729481893503, -0.5505527808806419, 0.6053783754694728, 0.7948348554128566, -0.6828578756693067, -0.7895036882729187, -0.7361871554409243, 0.5906715836660235, 0.5665194321604252, -0.386059208467016, 0.3798668340052098, 0.3654428475633056, -0.504737441523724, 0.5557212388616155, 0.05706850758344717, 0.718111021554789, -0.17570155083032146, -0.187207402546272, 0.07417014079721995, 0.8600659141736953, 0.8987788496922258, -0.83846319679285, -0.8039367630751681, 0.5713757741564613, 0.015274481249722971, -0.4933291531439876, -0.8244369341101472, -0.6803425961061149, 0.5343598255226227, 0.22216902200948474, -0.7528149972601779, -0.41230264711479614, 0.13521442724089527, 0.25439963044207414, -0.4476550587055206, -0.11553488328807404, -0.1515504416050324, 0.14699662490079946, 0.429896282855027, 0.9275028799362321, 0.9567561901295547, 0.7762771497447205, -0.56448229730816, 0.8648322624774889, 0.7647187933665964, 0.00683380317966642, 0.4958275968145205, 0.817882453132051, -0.9111008998720846, -0.739647131880542, -0.27024209137821265, -0.5074522592281896, 0.6247555135098672, 0.722522046807822, 0.4613298784941149, 0.6004050427604641, 0.31922474034753767, 0.8203929108578127, -0.26482475282657836, -0.9759767402746955, -0.13257516907073108, -0.9735523102407706, -0.5854692388910234, -0.20082885658621752, 0.9121497109700727, 0.892639369875416, -0.525471018383667, 0.8785541380671564, 0.65416252427668, -0.34710956586225894, 0.8269601431659008, -0.704832332184171, -0.6997900488168347, -0.9460896971027164, 0.536727782779582, -0.42798404195798234, 0.3496384386688842, 0.4421847307681348, 0.13576140192830866, -0.666764772266363, 0.22689978921649878, -0.32827482422266985, 0.5806263183248026, -0.4683278610199182, 0.30171939361705324, 0.7390165009380512, 0.7004218657110457, -0.7102146562851699, -0.25956597383048474, 0.8115459171225501, -0.8033982993831152, -0.7429409228781287, -0.8045383591718382, -0.8371643680355194, -0.9961966058271365, -0.2927116043530016, 0.40912525171202807, -0.03714494423973291, 0.09223597504540271, -0.38151716969609906, -0.7677507228651905, 0.8713501897296518, 0.02433628346256489, 0.12206200162244607, -0.795220705347975, -0.7301667035495614, 0.9276074142649966, -0.24600783082097455, 0.37466115749735174, 0.6310604610726722, 0.2748147625706574, 0.9074543238739585, -0.8929876786118576, 0.3583009194873863, 0.8507914622293011, 0.5436700368409411, 0.7230796838030451, 0.0670042824915491, 0.30000533918320116, 0.9450503132809047, 0.6407885862572371, 0.2252599476251247, 0.9579413656564604, -0.7055058188026677, 0.6692881761852929, 0.07407345902278739, -0.12265972568662686, 0.08131605615953363, -0.006425937447623653, -0.21015189254978583, -0.3385760989660316, 0.7099758466373007, -0.19340332720894238, 0.8624490932458233, -0.16834478386852458, -0.15763068674138703, 0.0518865141414635, 0.06237747412711392, -0.5369740649012418, -0.7344826563188789, 0.07416971889684798, 0.5346255375485802, -0.7922076842135215, 0.9634322800801105, -0.9896206948259028, -0.46706735367191365, -0.9098820864236101, 0.9725944465008518, 0.6440140214298891, -0.14827457540138922, -0.7147717814711276, 0.14752733641478644, 0.10417981493695194, 0.9657683821992022, 0.14569012443031393, -0.4411435279823597, 0.598066580306118, 0.0958358893378839, -0.029289992526734254, 0.38297477430447424, -0.3405002782599136, 0.41481416080504685, -0.5008032567736285, -0.28938308624159803, 0.35373136215253753, -0.2318231942529314, -0.4877922675861954, 0.8014173822844946, -0.5691205419808401, 0.43755496586844655, 0.7185562556650973, -0.5184720409286854, 0.4331179752197456, -0.9046628221072763, 0.1479122897099494, 0.3128626512603727, -0.07859398825353225, 0.517344052951302, -0.8486633003362556, -0.7510895773448476, -0.11332570564844224, -0.4109762824809302, -0.6847806934391822, 0.4806704192813349, 0.49828483356063025, -0.06945451885481235, 0.5669096637205613, 0.9018223839538253, 0.41064742336907667, 0.7444237991182239, -0.8128713877199487, 0.451018420655511, -0.15477319974797954, -0.4176549905590925, 0.9371543063941297, -0.7448367800165352, 0.021054206658754238, 0.8223594209979643, -0.622058905469743, -0.9226859141107042, -0.4364898345663033, 0.5709011144533738, 0.8867613356226447, 0.562961559530244, -0.18057018609693998, 0.1995946918970566, -0.07911719672671991, -0.34050053603648434, -0.7450804889825762, -0.3653109893723756, -0.3664630796312103, -0.6964213176609777, 0.418186497143602, 0.32490719965774906, -0.24063310258751258, 0.5475364437016716, -0.13292687032162087, -0.4389545698435464, -0.22342474403746948, 0.23859103349934774, -0.1858044664127223, -0.2277644837233288, 0.5760140367415749, 0.09680544004696601, 0.4737285901140127, -0.9509568193897955, 0.9251618673028057, -0.5234741859867968, -0.14370614091596967, 0.411182432537488, 0.6381980301277472, 0.7461177324616497, 0.212071789284759, -0.7537843696510389, 0.8503395222210082, 0.9632836533063791, 0.7051188560956141, 0.44875639455290317, -0.9695599255396989, -0.8489927246573943, -0.6476939130678334, -0.7516174128457547, -0.7408314658132313, 0.1999771146682061, 0.641307593403708, -0.5745809774884265, -0.4228447885199691, 0.8056966674294217, 0.5094723121510685, 0.768081262277007, -0.15296351171915035, 0.2086840667462324, 0.1862617651622298, 0.8089873458412309, -0.623649452585743, -0.9110207016193317, -0.4774303385794183, -0.5856175331001647, 0.758617975624351, 0.7151997859943722, -0.35244904893840734, 0.07370861239402271, 0.05546469273139221, -0.9500052520086673, -0.19082766689880937, 0.17418826625246497, 0.3320031963485004, -0.9553595993831134, -0.39950308228703135, -0.41259994728841587, -0.7648046051885631, -0.20907130026123433, -0.9620214098279016, -0.6740996077559671, -0.34933705196652776, -0.8430928837086828, -0.33865590015840796, 0.7954734076923269, -0.43802582969347137, -0.9001204885863177, 0.328069122975315, 0.4212432356744933, 0.01340453574837519, -0.5198690303646536, -0.47836198726268586, 0.4389035921221225, 0.3100358133701697, 0.04629011806368033, -0.8997375630012725, -0.035236857783844, -0.9973735060960636, -0.22463850814322894, -0.9650013387577976, -0.06957427226384305, -0.30057122958226623, 0.9839207356175601, 0.10837888908075466, 0.9270442403799586, 0.12858249731418536, 0.47793552098551007, 0.48961223418180966, 0.9377910259114262, 0.1853584273764748, 0.27140677616485465, -0.15022011514578648, -0.7551311462247181, -0.39179938792489755, -0.23434654891869422, -0.5313877634255422, -0.7353481219416329, 0.6831732587075259, -0.922201179542238, 0.831962448580948, -0.22049295706038086, -0.2963627888029172, -0.36740138340187833, 0.39383489855244, 0.8437090560825244, 0.20435674978354812, 0.5568131131856098, -0.042522544602692225, -0.36841858823782525, 0.9743414579218876, 0.571615213730208, -0.7459333883810562, -0.7465011928104539, -0.5342172232088793, -0.4327787674704857, 0.40158659601855273, 0.03281364772458217, -0.3188476561390845, 0.46521175383575675, 0.7270224073682054, -0.3856786920276485, -0.0435422245967636, 0.6973067197000131, 0.48035306694414626, 0.3363887337854199, 0.6345898086095962, 0.03318601735885518, -0.11931652832264117, 0.4485187092576961, -0.6166283067399578, -0.5850315932449999, 0.3444187255897837, -0.9162197204562625, -0.9296317546523825, -0.2567583730557055, 0.5251669034623656, -0.8323709449896672, -0.6794661598795169, 0.9894492845774345, -0.39772636864504296, -0.8118779642026803, 0.09687006297457956, -0.41319098081414674, 0.4815059953271006, -0.5027131299945984, -0.3925815081919426, 0.7914641566503449, 0.08699816682872785, 0.6562335450403476, 0.2852535308422177, -0.44273935021154065, -0.8569510586277502, -0.6110404992713456, 0.509953425590383, -0.25418395478954703, 0.9275677516677996, 0.26431525795356214, 0.7671343822458954, 0.6532089882211549, 0.1228938801592725, -0.858444676444349, 0.8668280874105236, -0.610912597014565, 0.4376180574353561, 0.5752538514035053, 0.27937378156188597, -0.037369821751994214, 0.6977704218052843, 0.1112187607831967, -0.8987800947525326, 0.5528129086292979, 0.7193052648978759, 0.461001601560066, 0.009093405635069995, 0.11823063162964687, -0.7453453322567509, 0.9137712400486175, 0.9819836548585574, 0.3867283616974484, 0.3518493471577451, 0.5166107795073225, 0.7453074418937706, 0.12464674437495393, -0.42125518465992506, -0.8784051497860998, 0.529702532814345, -0.869509690183262, -0.3628043601390971, -0.1518432224432933, 0.5282445015283095, 0.6637850513143568, -0.08848253242135318, 0.07128644385057736, 0.3984029563127358, 0.13016714336911783, 0.33722884171792744, -0.8471843374966432, -0.8845900029402922, 0.7774537488681175, 0.01570163706578498, 0.3892679472752618, 0.4757984868835723, -0.004671720860342843, -0.3275543346496559, 0.913295199918351, 0.7483358930104962, -0.2506319308111311, 0.02924471424097086, 0.667460231071265, -0.45318248841291564, -0.8915682947968355, 0.0899598189930757, 0.2758656082579538, -0.031558530008667596, -0.1639670579817285, -0.0016704119372661363, 0.16835746055958767, -0.5686027184331126, 0.5504023477274356, 0.22868133018415415, -0.2604312584193471, 0.2578499881633043, 0.6595623755254592, 0.7343547819946854, 0.7619582683657262, 0.8859556731586213, -0.7346759183722766, -0.9120079892616377, -0.03271468326309601, 0.33171039223005305, -0.07370003766065092, 0.6250706614024055, 0.0543811220301309, 0.5789774379945414, -0.8280453610643133, -0.5269562767826046, -0.4611267125299887, -0.02837747922917999, -0.23035871527644813, -0.004548976190180243, -0.974168725449466, -0.12572453945525686, 0.7251413997635672, -0.9616572536611381, 0.8172232498873448, 0.0340697746317542, -0.22308559424005225, -0.5802346172248387, 0.7094127584590864, -0.4056724829981895, -0.11714519523150857, -0.05521102919797061, 0.4548329663400408, -0.9657802654326475, -0.6547330715552808, -0.34337202955558954, -0.2128689145051499, 0.5848905202538219, -0.5251068711678784, 0.7057352851529692, 0.5094623894071648, -0.12608394321791772, -0.8308108792535427, 0.9398791892821334, 0.16248661998015024, -0.5289473369419437, -0.6362613212797867, -0.3785916310563566, 0.09208459489751553, 0.9842446662365383, -0.40507760828409256, -0.1277810079886772, 0.6880929932293987, 0.4272186111919427, -0.43038000386072683, -0.7395156262547702, -0.15374034648415535, 0.6571367908677193, 0.7576778845565038, 0.6077603179990847, 0.0633319173320328, -0.25110582592649844, -0.26530584888757613, -0.3866180566172437, -0.6308550456479309, 0.4998550838464022, -0.5026066264873639, 0.9164278207271435, -0.7500432166416022, -0.23783585226959847, 0.8247831460846344, 0.7366755707129491, -0.3485888209598973, 0.1345086934240689, -0.3524604224868082, 0.9651725111434892, 0.319466778807465, -0.2930448464550406, 0.6640903932350957, 0.9916385249441391, -0.48772476857894564, -0.6691905743422542, 0.9503127625597843, -0.35357388526410416, 0.9813362390073781, -0.9426876420476751, 0.9164577560338443, -0.29653201227813075, -0.5361454539851436, -0.8431690555987399, 0.12363380448100925, -0.04893901320535976, 0.36496697330705796, 0.5659279665949752, 0.5351079616690766, 0.723517494429567, -0.07867688798037786, 0.13127318978819824, -0.9100144161812129, -0.7242714306559144, 0.12354399214122469, 0.3241371207664352, 0.91372367272524, -0.4182079198258153, -0.07022885349862262, 0.9801317651783179, 0.9775944923067903, -0.8433771069607106, -0.16215110678738376, -0.6306764891612149, -0.3104652614358199, 0.06472706590988797, 0.8733051766557254, 0.7709277031359216, -0.37526939209014465, -0.16057608073208374, 0.07387727441017433, -0.058324717538706494, 0.2118219968599866, 0.22090904754535057, 0.06227985880467135, -0.921359864305054, 0.443753331380903, 0.3779430774318904, -0.11927084244425457, -0.12247587436297858, -0.4763774314807321, 0.5185004304297227, -0.15528393271558816, 0.4247788227355862, 0.13630239536725597, -0.09216497140705471, 0.68166699733287, 0.48492652231395206, 0.8998309554625123, -0.9253269462333376, 0.25364381156613724, -0.6518861719159419, 0.701403201769917, 0.7210768269261423, -0.2686493718834575, 0.8896125849176506, 0.09544347105601458, -0.3667841420556903, 0.08664476199863347, -0.6971876790356069, 0.39741274284182637, -0.33517181606666724, -0.34721088322291527, 0.8806182453780889, 0.6085166904243859, -0.6966212139834733, -0.9274272034565558, -0.8621229068084191, -0.8142451651407747, 0.6147948465883086, 0.9961094558936114, 0.6182982483345929, 0.3120765924205009, 0.27432689655615605, -0.4874704916134627, 0.5760832356503511, 0.6778490845224676, -0.7719381864439119, -0.4759488883591698, -0.5318625761322426, 0.887697863104908, -0.09653106975866943, -0.8928430175857864, 0.8261587736665754, -0.027092930423978645, 0.9478986557446609, -0.1329636208774918, -0.4424324380834237, -0.06883917400267814, 0.13023071487912574, -0.6762494216843717, 0.9996036437549678, 0.2815742457688646, -0.7371132617549985, 0.9386563934419196, 0.9474052848978067, -0.08446088197181734, -0.22751720229122774, 0.5433976004979377, -0.7829471232771146, -0.25708880156571423, -0.2325410653630362, 0.16175794099320728, 0.9824899188480738, 0.09535536829104663, -0.1943902437056182, 0.14191640121943427, 0.41941967445013284, 0.7838232943317105, -0.29894879568511246, 0.382925227191405, -0.9760715464633292, -0.18069741801714878, 0.14554157996520845, -0.08950103318853908, 0.4777738871533692, 0.618078709161916, 0.31699172363906736, -0.612508474968009, -0.7008006562684153, 0.09318427687735498, -0.5442634671293647, -0.08708766553114566, 0.6527781545028897, 0.9271323845263806, -0.8010264998843548, -0.0628979766573099, -0.8186439155385867, -0.579487945525553, -0.03446140917794027, -0.04487645950679453, -0.519997709371927, -0.4697778411695981, 0.349176049619931, 0.261746287821371, -0.16303830214936066, 0.13134887458656164, -0.9780194589792721, 0.31217192316339637, -0.9290262821931119, -0.8100147537395228, -0.3517421091541528, 0.9139795780853794, -0.528100641094287, 0.19643306071712185, -0.399004924223876, -0.7259272271269677, 0.35821915256601744, -0.41741878228190066, -0.6381319829019472, 0.7023686179981541, 0.8789086180974095, 0.1250638699824469, -0.25731355413469914, -0.5012685847594256, -0.9265162510929643, 0.10469680960618155, 0.05165392412006797, -0.8165048376518569, 0.8278117444842341, -0.783380004923631, -0.7364418297048039, 0.16568669962699722, 0.5797499237140749, 0.999317517981134, -0.34656285342348125, -0.019033460598431695, 0.957916400069355, 0.7446566857911165, 0.7933358561442205, 0.9457346590872806, -0.2269689602016911, 0.8835867054971875, -0.806011114719275, -0.12769391463051094, -0.11832785949387259, -0.13929261237530177, -0.6311479517818106, -0.4143041460971788, 0.23576649678119344, -0.6307966226530821, -0.07220171617347426, 0.7705224887000643, 0.4980479277100134, 0.7084874010551236, -0.2398314381861304, -0.10983032946112892, 0.8625639765362245, -0.7164410240608732, 0.36821540074332315, 0.5120676225934688, 0.29125751801647515, -0.9965701566534428, -0.77134712692313, -0.7333556186997328, 0.7358844419281341, -0.27626082833000676, -0.39297257911633254, -0.49143835131666647, -0.7082377912329285, -0.3728976622213849, 0.8428267118397279, -0.2233026736058752, -0.8704026916030665, 0.9187128282554791, -0.9336858154516625, 0.15457381299568773, 0.8065831128330634, 0.15048508438075903, -0.9689729502617301, 0.6154882539989543, 0.5620445638276537, -0.7115104320983976, 0.5589520344604872, 0.21060210056812534, 0.3149244044964039, -0.8731815261311673, 0.9872497048705955, 0.7268705810346707, 0.4888611935162319, -0.27722579215513554, -0.10680281639786826, -0.9254831460867803, -0.08169671434496051, -0.8300835845186181, -0.003310389253457169, -0.04414682432853967, 0.8281363977482776, -0.6696023308695038, -0.980228510473272, -0.6846633661825001, 0.8632737208988042, 0.3665901267696936, 0.861527883513918, 0.8299305657290179, -0.1435236850051944, 0.5339905325647247, -0.5436752250849961, -0.2180443353500321, -0.1119154683897785, -0.1526010101196409, 0.5208353241873331, 0.9313054227092483, -0.16621306160185556, -0.5810259880173052, -0.6360597291432761, -0.8687532186041695, -0.1427206104494776, 0.5559854577152388, 0.024577246307250222, 0.48371853118336583, -0.3298103093225191, -0.43642050844499947, 0.6808973581782478, 0.686252063036819, -0.4348533633686118, 0.008848562048953346, -0.4315450761377948, 0.29255137324408165, 0.658631127027693, 0.3765852924362776, 0.014611907644433764, -0.41861063763299855, 0.7571477917251337, 0.8670867978958539, 0.8659940660936232, 0.6458718925772999, -0.3403624051326226, 0.9230552993214756, 0.2295815084062094, 0.687025628077965, -0.5971736418062707, -0.8552581257173013, 0.047635077732129494, 0.3048005074002149, -0.5623639467933919, -0.7551672466371941, -0.8644930689086006, -0.3953209504194801, -0.4528470490440275, 0.3037963455454409, -0.6940694136054122, -0.4051767323703406, 0.9085221059644486, 0.4395951899673862, 0.10151602330146603, 0.9272580668094594, 0.08748979811692292, 0.8622162320461229, 0.455427902294814, 0.5546658492668186, -0.7599780736379782, 0.4291300157935316, -0.7508989328397875, 0.5807483791376193, -0.6037313156878956, -0.02880895945828854, 0.1705960260277124, -0.4888189617303935, -0.746869000142643, -0.05936040259130437, 0.7833934731947907, -0.5901211233044985, 0.9121485662489959, -0.10542643330762491, 0.41974503013790376, 0.029429107675992494, 0.20777500175021868, 0.5019124515397289, -0.9422894041812118, 0.8972946104944011, 0.08207915027014523, -0.5564314755383168, 0.6809290685067908, 0.1531920607783881, -0.7174868736618956, -0.37881397172462083, 0.1646031332819804, -0.7938755546109255, -0.6439801857194805, 0.07919092882472145, -0.6333157010020416, -0.7864506506038649, 0.1510469673338306, -0.7148944293084212, 0.13369529056634843, 0.8174639240543318, -0.8485187469560644, 0.9772154098445682, -0.00936739044478152, 0.7650229081526503, 0.6985320368026726, 0.05894563556751953, -0.6607811523317948, 0.05594897002444732, -0.7159934688155078, 0.9458672287414942, 0.6369886770706525, -0.8697297319737309, 0.6215731426658901, 0.7245503832700062, 0.36860355695546576, 0.3777443064060215, -0.45864537902634517, -0.099670324171385, 0.904799602300534, 0.20327819228252797, -0.10264060436100375, -0.7174045072883397, 0.032477267233004925, -0.6358329893831123, 0.17061633339838345, -0.06931683860464832, 0.24699618729557526, -0.8867325426033668, 0.9983801145197821, 0.13896831601181447, -0.7766764193468969, -0.7909704614978967, 0.6656837451068724, -0.7242582679225196, 0.26853074496668294, 0.01994852562220739, 0.47686579336695933, -0.3084268448406655, 0.7605197184292958, -0.6397650050795065, 0.8495366819944155, -0.5761288391830883, -0.4541207367586759, -0.048951201573424585, 0.48176865117124046, 0.21028262309906132, 0.2482314254533402, 0.22555790300554657, 0.3450214830670151, -0.27365282007227476, 0.39759058701032757, -0.8671312753942502, 0.42285620886993214, -0.24373654748744777, -0.4847449323383557, 0.5220558471133319, 0.07036742330224754, 0.4381945230630522, -0.6466563213541059, 0.38207656788686695, 0.7414976061784107, -0.07245603859652139, 0.3377106817290956, -0.7984080269436153, 0.4600857886783505, 0.7864433771254256, -0.45217823412956193, 0.47024684291743357, -0.12783664679424533, 0.07986933479540759, 0.4328554438715293, -0.3506448919210219, 0.16629948825211005, -0.6605864931056011, 0.34449131769303065, 0.5093015320655483, 0.8058751149196322, 0.14341788832179692, 0.9147811076526506, -0.13641851408161187, 0.893973474951858, -0.11530354771460893, 0.6167304229654427, 0.20931216540845998, -0.6410255430223788, -0.7316151672377142, -0.6034822728761509, 0.7545545067181776, -0.9598407599275987, 0.6370854862129327, -0.9059561203660462, -0.7173728819290945, -0.7807899314523654, 0.8031862445620663, 0.18121195406461665, 0.9773400697045496, 0.5048184426612081, 0.6267855754648335, 0.6454320804555733, -0.15838004435169362, -0.8860230863377236, -0.7658886215987404, 0.6626145674779975, 0.012067871823985277, 0.1776283726787522, -0.651163543402032, -0.9338557370708427, -0.5667225927100874, -0.10965903403803412, -0.2761928692247131, -0.9522566700500346, -0.8926111715207465, -0.8485806809931973, -0.8731533990039648, -0.9731346033400365, 0.6679146357535668, 0.2663256469709041, 0.38162823499024734, -0.48017253853739583, -0.6907912115510104, -0.5304675448849729, -0.4409508453039812, 0.150280613874056, 0.7503566928419525, -0.4376836928155343, 0.20981876469842775, 0.6321710310301563, 0.42495136143102186, -0.77942828026064, -0.5946191954171971, 0.3443922294516313, -0.19695443869681206, 0.6714251632910684, -0.20794627557258316, -0.20827327217557312, 0.6145024901676337, -0.48096176726091233, 0.9717188103527314, -0.5096358517238999, -0.9444563885700508, -0.4604788978053185, 0.9355838982127698, 0.8750327114465972, -0.309834025227705, -0.210468888311941, -0.8969759103692676, 0.18440301181675056, 0.8091885567609103, -0.9349399254065582, -0.07595699836204983, -0.8578859619764443, -0.27518120552660763, -0.6575941164666292, -0.9032225577676631, -0.8875945986869256, -0.4148674020257048, -0.2492585217518184, 0.3185108451038967, -0.3301115797618648, 0.8172296401537733, -0.06119018349917771, 0.053408803403080185, -0.37063646141402096, -0.9683170813199247, 0.6360900940240821, 0.2422739891174881, -0.3480060928340356, 0.7491247227479851, 0.7702777481062753, -0.4499729882881327, -0.2867649892210764, -0.4018462143489294, -0.5061598123158959, 0.40793338572853943, -0.9258626645982182, 0.8635952687593431, 0.017028579377951214, 0.9856514667828973, -0.734028978340165, 0.7971587023196731, -0.7864655529313076, -0.5243900264768941, -0.47604220489950455, -0.042603844354870635, -0.22975148267895507, -0.7211823159581194, -0.7799351116191433, -0.8591222842675099, 0.8510508873360121, 0.5880815885784478, -0.036193527714576845, -0.7636049762200916, 0.7484675743922837, -0.33815175574715983, -0.7838980308953476, -0.4590004308496858, -0.5523158952112384, -0.8292425834208823, -0.185815336774108, -0.1941352270671295, 0.2984791380090228, 0.8251890877462973, -0.5964268101028385, 0.16396886806327293, 0.4048331425255949, 0.8140679234803647, -0.005262448302129297, 0.5445798868385991, 0.16691636004359744, -0.05206757697954045, 0.31753650270173184, -0.9734282286940086, -0.9961022592983566, -0.8821635897430835, -0.5344763239614714, -0.3536572937088882, -0.7486402990359262, 0.8272524403075816, -0.22140281712431542, 0.7122176748800078, 0.8007917767201176, -0.571425906560868, 0.583636410580304, 0.6785579172671243, -0.902100495425225, 0.3877950525130458, 0.41092281939115716, -0.31364044766181975, 0.6975884559450984, 0.753382065004512, 0.5098734515866503, 0.7010115813444078, 0.4037740833632466, 0.057271947969377957, -0.9088964213612531, -0.9838397688680764, 0.9373192077958632, 0.9863815191006888, -0.08562719680866171, 0.06941402513252415, -0.5696369231437444, -0.2795047733575897, -0.3687168674563872, 0.9585936785664546, 0.85240261073085, -0.7583275195787473, -0.24598607509161186, 0.644459366693972, -0.46979155403996464, -0.8154082356811969, 0.05194590378655617, 0.7796796147543039, -0.5650237550866177, 0.5298379245104021, -0.8619841679336635, -0.6475141022063056, 0.8941826601469569, 0.8938812505052434, 0.4217378852087974, -0.22752000018384644, 0.934040616533409, 0.9113790452861945, 0.9358593136168842, 0.18384192592217796, -0.7089921275928868, 0.561524134748056, 0.9121024773001991, -0.5498914654634968, -0.6763192967602198, -0.1714366789549897, 0.37074549862576167, -0.7461383324666615, 0.0067690270042712886, 0.1489590822144624]\n",
            "[-0.01544795  0.64205022  0.55673038 ... -0.74613833  0.00676903\n",
            "  0.14895908]\n",
            "tensor([[-0.0154,  0.6421,  0.5567,  ...,  0.2185, -0.9527, -0.4218],\n",
            "        [-0.8165, -0.3404, -0.9759,  ..., -0.1221, -0.7681, -0.5235],\n",
            "        [-0.0665, -0.1918, -0.5041,  ..., -0.5742,  0.6952, -0.8822],\n",
            "        ...,\n",
            "        [ 0.9144,  0.0450,  0.2905,  ..., -0.4683,  0.3017,  0.7390],\n",
            "        [ 0.7004, -0.7102, -0.2596,  ...,  0.6778, -0.7719, -0.4759],\n",
            "        [-0.5319,  0.8877, -0.0965,  ...,  0.9114,  0.9359,  0.1838]],\n",
            "       dtype=torch.float64)\n",
            "tensor([-0.7090,  0.5615,  0.9121, -0.5499, -0.6763, -0.1714,  0.3707, -0.7461,\n",
            "         0.0068,  0.1490], dtype=torch.float64)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/alishihab/ci-cw/ci_cw_final.ipynb Cell 29\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m memetic_train_and_validate(model, toolbox, pop, train_loader, val_loader, criterion, optimizer, mean, std, epochs\u001b[39m=\u001b[39;49mnum_epochs)\n",
            "\u001b[1;32m/Users/alishihab/ci-cw/ci_cw_final.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#inputs, train_load_labels = inputs.to(device), train_load_labels.to(device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m new_weights, new_biases, pop \u001b[39m=\u001b[39m pso_optimize(model, toolbox, pop, inputs, train_load_labels, i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m model\u001b[39m.\u001b[39mfc2\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParameter(new_weights)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model\u001b[39m.\u001b[39mfc2\u001b[39m.\u001b[39mbiases \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParameter(new_biases)\n",
            "\u001b[1;32m/Users/alishihab/ci-cw/ci_cw_final.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# eval current fitness\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m part \u001b[39min\u001b[39;00m pop:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     part\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m toolbox\u001b[39m.\u001b[39;49mevaluate(model, part, inputs, labels, potential\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m#actually only one fitness value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(pop[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Begin the evolution\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#for g in range(iterations):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# find the global best - lamarckian search party lead - gradient descent\u001b[39;00m\n",
            "\u001b[1;32m/Users/alishihab/ci-cw/ci_cw_final.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(new_weights)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(new_biases)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m new_weights \u001b[39m=\u001b[39m new_weights\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m new_biases \u001b[39m=\u001b[39m new_biases\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alishihab/ci-cw/ci_cw_final.ipynb#X40sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39mfc2\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParameter(new_weights)\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
          ]
        }
      ],
      "source": [
        "fc2_weights = model.fc2.weight.data\n",
        "weights_dim = fc2_weights.shape\n",
        "weights_len = len(fc2_weights.reshape(-1))\n",
        "fc2_bias = model.fc2.bias.data\n",
        "bias_dim = fc2_bias.shape\n",
        "bias_len = len(fc2_bias.reshape(-1))\n",
        "populationSize  = 100\n",
        "dimension = weights_len + bias_len\n",
        "\n",
        "# Freeze all layers except the last\n",
        "freeze_all_but_last(model)\n",
        "\n",
        "# DEAP inits\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Particle\", list, fitness=creator.FitnessMin, speed=list,\n",
        "                smin=None, smax=None, best=None, potential=None)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"particle\", generate_particle, dimension)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.particle)\n",
        "toolbox.register(\"evaluate\", evaluate_particle)\n",
        "toolbox.register(\"learn\", behaviour_learning)\n",
        "\n",
        "# Model hyperparameters\n",
        "dropout_prob = 0.30\n",
        "num_epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "# call function\n",
        "# Main loop for k-fold cross-validation\n",
        "for fold, (train_fold_indices, val_fold_indices) in enumerate(skf.split(train_idx, stratified_train_labels)):\n",
        "    print(f'Fold {fold + 1}/{num_folds}')\n",
        "    mean, std = calc_mean_std(Subset(stratified_train_set, train_fold_indices))\n",
        "\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_fold_indices)\n",
        "    val_sampler = torch.utils.data.SubsetRandomSampler(val_fold_indices)\n",
        "\n",
        "    # create an initial population of individuals\n",
        "    pop = toolbox.population(n=populationSize)\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"avg\", np.mean)\n",
        "    stats.register(\"std\", np.std)\n",
        "    stats.register(\"min\", np.min)\n",
        "    stats.register(\"max\", np.max)\n",
        "\n",
        "\n",
        "    logbook = tools.Logbook()\n",
        "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
        "\n",
        "    fits = []\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=stratified_train_set,\n",
        "        batch_size=batch_size,\n",
        "        sampler=train_sampler,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        dataset=stratified_train_set,\n",
        "        batch_size=batch_size,\n",
        "        sampler=val_sampler,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    memetic_train_and_validate(model, toolbox, pop, train_loader, val_loader, criterion, optimizer, mean, std, epochs=num_epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hoQTbsACfg_f",
        "g3nmEEA8MgoD",
        "8jGcc7gkI4Mt",
        "xSuZ5c9VNjU8",
        "vJ9wVgxINsGO",
        "G3ZaKDEXTxj8"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
